{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Kawai <p>The Cute agentic framework.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>ReAct Pattern: Implements Reasoning and Acting paradigm with strict one-tool-per-step execution</li> <li>Tool Calling: OpenAI-compatible function calling via OpenAI-compatible endpoint</li> <li>Optional Planning: Multi-step planning with configurable re-planning intervals</li> <li>Rich Logging: Color-coded console output with syntax highlighting</li> <li>Weave Integration: Automatic experiment tracking and observability</li> <li>REST API Deployment: Deploy agents as REST APIs with streaming support</li> <li>Caching Strategies: Aggressive prompt and tool caching strategies to save your token budget</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/soumik12345/kawai\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import weave\nfrom kawai import KawaiReactAgent, WebSearchTool, KawaiLoggingCallback, OpenAIModel\n\n# Initialize Weave for tracking\nweave.init(project_name=\"my-project\")\n\n# Create agent\nagent = KawaiReactAgent(\n    model=OpenAIModel(\n        model_id=\"google/gemini-3-flash-preview\",\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key_env_var=\"OPENROUTER_API_KEY\",\n    ),\n    tools=[WebSearchTool()],\n    max_steps=10,\n    callbacks=[KawaiLoggingCallback()]\n)\n\n# Run task\nresult = agent.run(\"What's the latest news on AI?\")\nprint(result[\"final_answer\"])\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Memory-augmented web-search agent</li> </ul>"},{"location":"#built-in-tools","title":"Built-in Tools","text":"<ul> <li>WebSearchTool: Google search via Serper API</li> <li>FinalAnswerTool: Task completion (auto-added)</li> <li>UserInputTool: Interactive user prompts</li> </ul>"},{"location":"#creating-custom-tools","title":"Creating Custom Tools","text":"<pre><code>from kawai.tools import KawaiTool, KawaiToolParameter\nimport weave\n\nclass CalculatorTool(KawaiTool):\n    tool_name: str = \"calculator\"\n    description: str = \"Performs arithmetic operations\"\n    parameters: list[KawaiToolParameter] = [\n        KawaiToolParameter(\n            param_name=\"expression\",\n            description=\"Math expression to evaluate\",\n            tool_type=\"string\"\n        )\n    ]\n\n    @weave.op\n    def forward(self, expression: str) -&gt; dict:\n        return {\"result\": eval(expression)}\n</code></pre>"},{"location":"#planning-mode","title":"Planning Mode","text":"<p>Enable multi-step planning for complex tasks:</p> <pre><code>agent = KawaiReactAgent(\n    model=OpenAIModel(\n        model_id=\"google/gemini-3-flash-preview\",\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key_env_var=\"OPENROUTER_API_KEY\",\n    ),\n    tools=[WebSearchTool()],\n    planning_interval=3,  # Re-plan every 3 steps\n    max_steps=15\n)\n</code></pre>"},{"location":"#custom-callbacks","title":"Custom Callbacks","text":"<p>Monitor agent execution with custom callbacks:</p> <pre><code>from kawai.callback import KawaiCallback\n\nclass MyCallback(KawaiCallback):\n    def at_reasoning(self, reasoning: str):\n        print(f\"Thinking: {reasoning}\")\n\n    def at_tool_call(self, tool_name: str, tool_arguments: dict):\n        print(f\"Calling {tool_name}\")\n\nagent = KawaiReactAgent(\n    model=OpenAIModel(\n        model_id=\"google/gemini-3-flash-preview\",\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key_env_var=\"OPENROUTER_API_KEY\",\n    ),\n    tools=[WebSearchTool()],\n    callbacks=[MyCallback()]\n)\n</code></pre>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<p>Inspired by HuggingFace smolagents. ReAct prompts adapted from their toolcalling agent implementation.</p>"},{"location":"deployment/","title":"Deploying Agents as REST APIs","text":"<p>Kawai allows you to deploy your agents as REST API servers, making them accessible from other applications or integrable into existing infrastructure.</p>"},{"location":"deployment/#quick-start","title":"Quick Start","text":"<p>Deploy an agent with a single method call:</p> <pre><code>import weave\nfrom kawai import KawaiReactAgent, WebSearchTool, OpenAIModel\n\nweave.init(project_name=\"kawai-server\")\n\nmodel = OpenAIModel(\n    model_id=\"google/gemini-3-flash-preview\",\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key_env_var=\"OPENROUTER_API_KEY\",\n)\n\nagent = KawaiReactAgent(\n    model=model,\n    tools=[WebSearchTool()],\n    max_steps=10,\n)\n\n# Start the REST API server\nagent.serve(port=8000)\n</code></pre> <p>The server provides these endpoints:</p> Endpoint Method Description <code>/chat</code> POST Non-streaming chat - send a message, get the response <code>/stream</code> GET Server-Sent Events for real-time streaming <code>/health</code> GET Health check with status and session count <code>/sessions/{id}</code> GET Get session information <code>/sessions/{id}</code> DELETE Delete a session"},{"location":"deployment/#api-reference","title":"API Reference","text":""},{"location":"deployment/#post-chat","title":"POST /chat","text":"<p>Send a message to the agent and receive the complete response.</p> <p>Request Body: <pre><code>{\n    \"message\": \"What is the capital of France?\",\n    \"session_id\": \"optional-session-id\",\n    \"max_steps\": 5,\n    \"force_provide_answer\": true\n}\n</code></pre></p> Field Type Required Description <code>message</code> string Yes The message/prompt for the agent <code>session_id</code> string No Session ID for conversation continuity <code>max_steps</code> integer No Override default max_steps <code>force_provide_answer</code> boolean No Force answer if max_steps exhausted (default: true) <p>Response: <pre><code>{\n    \"answer\": \"The capital of France is Paris.\",\n    \"session_id\": \"abc123\",\n    \"steps\": 2,\n    \"completed\": true,\n    \"plan\": null\n}\n</code></pre></p> <p>Example with curl: <pre><code>curl -X POST http://localhost:8000/chat \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\"message\": \"What is machine learning?\"}'\n</code></pre></p> <p>Example with Python: <pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/chat\",\n    json={\"message\": \"What is machine learning?\"}\n)\nresult = response.json()\nprint(result[\"answer\"])\n</code></pre></p>"},{"location":"deployment/#get-stream","title":"GET /stream","text":"<p>Connect to receive real-time events during agent execution via Server-Sent Events (SSE).</p> <p>Query Parameters:</p> Parameter Type Required Description <code>message</code> string Yes The message/prompt for the agent <code>session_id</code> string No Session ID for conversation continuity <code>max_steps</code> integer No Override default max_steps <code>force_provide_answer</code> boolean No Force answer if max_steps exhausted (default: true) <p>Event Types:</p> Type Description Data Fields <code>run_start</code> Agent started processing <code>prompt</code>, <code>model</code> <code>step_start</code> New step began <code>step_index</code> <code>reasoning</code> Agent's thinking <code>reasoning</code> <code>tool_call</code> Tool being called <code>tool_name</code>, <code>tool_arguments</code> <code>tool_result</code> Tool returned result <code>tool_name</code>, <code>tool_result</code> <code>planning</code> Plan generated/updated <code>plan</code>, <code>updated_plan</code> <code>warning</code> Warning occurred <code>message</code> <code>run_end</code> Agent finished <code>answer</code> <p>Example with curl: <pre><code>curl -N \"http://localhost:8000/stream?message=Search%20for%20AI%20news\"\n</code></pre></p> <p>Example with Python: <pre><code>import requests\nimport json\n\nresponse = requests.get(\n    \"http://localhost:8000/stream\",\n    params={\"message\": \"Search for AI news\"},\n    stream=True\n)\n\nfor line in response.iter_lines():\n    if line:\n        data = line.decode().removeprefix(\"data: \")\n        event = json.loads(data)\n        print(f\"{event['type']}: {event['data']}\")\n</code></pre></p> <p>Example with JavaScript (browser): <pre><code>const eventSource = new EventSource(\n    \"http://localhost:8000/stream?message=\" + encodeURIComponent(\"Hello!\")\n);\n\neventSource.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    console.log(data.type, data.data);\n\n    if (data.type === \"run_end\") {\n        eventSource.close();\n    }\n};\n</code></pre></p>"},{"location":"deployment/#get-health","title":"GET /health","text":"<p>Check server health status.</p> <p>Response: <pre><code>{\n    \"status\": \"healthy\",\n    \"sessions\": 5,\n    \"uptime\": 3600.5,\n    \"version\": \"0.1.0\"\n}\n</code></pre></p>"},{"location":"deployment/#get-sessionssession_id","title":"GET /sessions/{session_id}","text":"<p>Get information about a specific session.</p> <p>Response: <pre><code>{\n    \"session_id\": \"abc123\",\n    \"created_at\": \"2024-01-15T10:30:00\",\n    \"last_accessed\": \"2024-01-15T10:35:00\",\n    \"message_count\": 6\n}\n</code></pre></p>"},{"location":"deployment/#delete-sessionssession_id","title":"DELETE /sessions/{session_id}","text":"<p>Delete a session to clear its conversation history.</p> <p>Response: <pre><code>{\n    \"session_id\": \"abc123\",\n    \"deleted\": true\n}\n</code></pre></p>"},{"location":"deployment/#session-management","title":"Session Management","text":"<p>Sessions maintain conversation memory across requests, enabling multi-turn conversations:</p> <pre><code>import requests\n\nSESSION_ID = \"user-123\"\n\n# First message\nr1 = requests.post(\n    \"http://localhost:8000/chat\",\n    json={\"message\": \"What is Python?\", \"session_id\": SESSION_ID}\n)\nprint(r1.json()[\"answer\"])\n\n# Follow-up (agent remembers context)\nr2 = requests.post(\n    \"http://localhost:8000/chat\",\n    json={\"message\": \"What are its main features?\", \"session_id\": SESSION_ID}\n)\nprint(r2.json()[\"answer\"])\n</code></pre> <p>Sessions automatically expire after the configured timeout (default: 1 hour).</p>"},{"location":"deployment/#configuration-options","title":"Configuration Options","text":"<p>The <code>serve()</code> method accepts these parameters:</p> <pre><code>agent.serve(\n    host=\"0.0.0.0\",          # Bind to all interfaces\n    port=8000,               # Server port\n    session_timeout=3600,    # Session expiry in seconds (1 hour)\n    enable_cors=True,        # Enable CORS for browsers\n    allowed_origins=[\"*\"],   # CORS allowed origins\n    log_level=\"info\",        # Uvicorn log level\n)\n</code></pre>"},{"location":"deployment/#production-deployment","title":"Production Deployment","text":""},{"location":"deployment/#using-a-process-manager","title":"Using a Process Manager","text":"<p>For production, use a process manager like systemd or supervisord:</p> <pre><code># /etc/systemd/system/kawai-agent.service\n[Unit]\nDescription=Kawai Agent API\nAfter=network.target\n\n[Service]\nUser=www-data\nWorkingDirectory=/opt/kawai\nEnvironment=\"OPENROUTER_API_KEY=your-key\"\nEnvironment=\"SERPER_API_KEY=your-key\"\nExecStart=/opt/kawai/.venv/bin/python -m examples.serve_example\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":"<pre><code>FROM python:3.12-slim\n\nWORKDIR /app\n\n# Install uv\nRUN pip install uv\n\n# Copy project files\nCOPY pyproject.toml uv.lock ./\nCOPY kawai ./kawai\nCOPY examples ./examples\n\n# Install dependencies\nRUN uv sync\n\n# Environment variables\nENV OPENROUTER_API_KEY=\"\"\nENV SERPER_API_KEY=\"\"\n\nEXPOSE 8000\n\nCMD [\"uv\", \"run\", \"examples/serve_example.py\"]\n</code></pre> <pre><code>docker build -t kawai-agent .\ndocker run -p 8000:8000 \\\n    -e OPENROUTER_API_KEY=your-key \\\n    -e SERPER_API_KEY=your-key \\\n    kawai-agent\n</code></pre>"},{"location":"deployment/#reverse-proxy-with-nginx","title":"Reverse Proxy with nginx","text":"<p>For production, put the server behind nginx:</p> <pre><code>upstream kawai {\n    server 127.0.0.1:8000;\n}\n\nserver {\n    listen 80;\n    server_name api.example.com;\n\n    location / {\n        proxy_pass http://kawai;\n        proxy_http_version 1.1;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n\n        # For SSE streaming\n        proxy_set_header Connection '';\n        proxy_buffering off;\n        proxy_cache off;\n    }\n}\n</code></pre>"},{"location":"deployment/#security-considerations","title":"Security Considerations","text":"<ol> <li> <p>API Keys: Never expose API keys in client-side code. Use environment variables.</p> </li> <li> <p>Rate Limiting: Consider adding rate limiting middleware for production:    <pre><code>from slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\n</code></pre></p> </li> <li> <p>Authentication: Add authentication for production deployments:    <pre><code>from fastapi import Depends, HTTPException\nfrom fastapi.security import APIKeyHeader\n\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\nasync def verify_api_key(api_key: str = Depends(api_key_header)):\n    if api_key != os.getenv(\"API_KEY\"):\n        raise HTTPException(status_code=403)\n</code></pre></p> </li> <li> <p>HTTPS: Always use HTTPS in production. Configure SSL/TLS in your reverse proxy.</p> </li> <li> <p>CORS: Restrict <code>allowed_origins</code> to your specific domains in production.</p> </li> </ol>"},{"location":"api_reference/agents/","title":"Agents","text":""},{"location":"api_reference/agents/#kawai.agents.react","title":"<code>kawai.agents.react</code>","text":""},{"location":"api_reference/agents/#kawai.agents.react.KawaiReactAgent","title":"<code>KawaiReactAgent</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A ReAct agent that uses tool calling.</p> <p>This agent implements the ReAct paradigm where the agent iteratively: 1. Reasons about what to do next 2. Acts by calling a tool 3. Observes the tool's output 4. Repeats until the task is complete</p> <p>The agent maintains conversation history in OpenAI Chat Completions format and uses function calling to execute tools. All executions are automatically tracked by Weave for observability.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>str</code> <p>OpenRouter model identifier (e.g., \"openai/gpt-4\", \"anthropic/claude-3-sonnet\", \"google/gemini-3-flash-preview\").</p> <code>tools</code> <code>list[KawaiTool]</code> <p>List of tools available to the agent. FinalAnswerTool is automatically added if not present.</p> <code>system_prompt</code> <code>str</code> <p>System prompt that defines the agent's behavior. Defaults to <code>SYSTEM_PROMPT</code> which enforces strict ReAct pattern.</p> <code>instructions</code> <code>str | None</code> <p>Optional additional instructions appended to the system prompt. Use this to provide task-specific guidance without replacing the base ReAct behavior. Defaults to None.</p> <code>max_steps</code> <code>int</code> <p>Maximum number of ReAct steps before stopping. Defaults to 5.</p> <code>planning_interval</code> <code>int | None</code> <p>If set, agent generates/updates plans at this interval. None disables planning (default).</p> <code>tool_dict</code> <code>dict[str, KawaiTool]</code> <p>Internal mapping of tool names to tool objects. Automatically populated from tools list.</p> <code>callbacks</code> <code>list[KawaiCallback]</code> <p>List of callbacks for monitoring execution. Empty list by default.</p> <p>Example</p> <pre><code>import weave\nfrom kawai import KawaiReactAgent, WebSearchTool, KawaiLoggingCallback\n\n# Initialize Weave for tracking\nweave.init(project_name=\"my-project\")\n\n# Create agent with web search capability\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    max_steps=10,\n    planning_interval=3,  # Re-plan every 3 steps\n    callbacks=[KawaiLoggingCallback()],\n    instructions=\"Focus on finding recent sources from 2024.\"\n)\n\n# Run the agent\nresult = agent.run(\"Who won the 2024 Nobel Prize in Physics?\")\nprint(result[\"final_answer\"])\n</code></pre> Note <ul> <li>Requires <code>OPENROUTER_API_KEY</code> environment variable</li> <li><code>FinalAnswerTool</code> is automatically added to complete tasks</li> <li>Agent loops until <code>FinalAnswerTool</code> is called or max_steps reached</li> <li>If max_steps is reached without completion, agent is forced to provide   a final answer in one extra step</li> <li>With planning enabled, agent creates and updates execution plans</li> </ul>"},{"location":"api_reference/agents/#kawai.agents.react.KawaiReactAgent.execute_tool_from_response_call","title":"<code>execute_tool_from_response_call()</code>","text":"<p>Execute one ReAct step: get LLM response, call tools, update memory.</p> <p>This method implements a single step of the ReAct loop: 1. Calls the LLM with current memory and tool schemas 2. Parses the LLM response for reasoning and tool calls 3. Executes any tool calls and appends results to memory 4. Handles errors and edge cases (no tool call, unknown tool, etc.)</p> <p>The method uses OpenAI Chat Completions format for all messages and properly links tool calls with their responses via tool_call_id. It operates on and modifies <code>self.model.memory</code> directly.</p> <p>Returns:</p> Type Description <code>tuple[bool, str | None]</code> <p>tuple[bool, str | None]: A tuple containing: - Boolean indicating if final_answer was called (task complete) - The tool_call_id of the final_answer call, or None</p> Note <ul> <li>Operates on and modifies <code>self.model.memory</code> directly</li> <li>Triggers callbacks: at_reasoning, at_tool_call, at_tool_result, at_warning</li> <li>Tool errors are caught and returned as {\"error\": \"...\"} in memory</li> <li>If model doesn't make a tool call, prompts it to continue</li> </ul>"},{"location":"api_reference/agents/#kawai.agents.react.KawaiReactAgent.get_final_answer","title":"<code>get_final_answer(step_index)</code>","text":"<p>If we exhausted max_steps without calling final_answer, force one extra step</p>"},{"location":"api_reference/agents/#kawai.agents.react.KawaiReactAgent.run","title":"<code>run(prompt, force_provide_answer=False)</code>","text":"<p>Execute the agent on a task and return the result.</p> <p>This is the main entry point for running the agent. It implements the full ReAct loop with optional planning:</p> <ol> <li>Initialize memory with system prompt and task</li> <li>(Optional) Generate initial plan if planning_interval is set</li> <li>Loop up to max_steps:</li> <li>(Optional) Update plan at planning_interval</li> <li>Get reasoning and tool call from LLM</li> <li>Execute tool and observe result</li> <li>If final_answer called, extract and return answer</li> <li>If max_steps reached without final_answer, force one extra step to get    a final answer based on all information gathered</li> <li>Return comprehensive result dictionary</li> </ol> <p>The method is tracked by Weave as an operation for full observability.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The task description or question for the agent to solve.</p> required <code>force_provide_answer</code> <code>bool</code> <p>Force to provide an answer even though <code>max_steps</code> have been exhausted.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing: - final_answer (Any): The final answer from FinalAnswerTool. If max_steps     is reached without completion, the agent is forced to provide a final     answer in one extra step - steps (int): Number of ReAct steps executed (may be max_steps + 1 if     forced final answer was triggered) - memory (list[dict[str, Any]]): Full conversation history in OpenAI     format, including all reasoning, tool calls, and results - completed (bool): Whether the task completed successfully (final_answer     was called, either naturally or forced) - plan (str | None): The final plan if planning was enabled, or None</p> <p>Example</p> <pre><code>agent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    max_steps=5\n)\n\nresult = agent.run(\"What is the population of Tokyo?\")\n\nprint(f\"Answer: {result['final_answer']}\")\nprint(f\"Took {result['steps']} steps\")\nprint(f\"Completed: {result['completed']}\")\n</code></pre> Note <ul> <li>Triggers callbacks: at_run_start, at_step_start, at_run_end, at_warning</li> <li>Also triggers planning and tool execution callbacks</li> <li>If max_steps is exhausted, triggers at_warning before forcing final answer</li> <li>Tracked by Weave - view traces at wandb.ai/weave</li> </ul>"},{"location":"api_reference/agents/#kawai.agents.react.KawaiReactAgent.serve","title":"<code>serve(host='0.0.0.0', port=8000, session_timeout=3600, enable_cors=True, allowed_origins=None, log_level='info')</code>","text":"<p>Start a FastAPI server to serve the agent via REST API.</p> <p>This method starts a web server that exposes the agent through HTTP endpoints, allowing you to query the agent from other applications or integrate it into existing infrastructure.</p> Available endpoints <ul> <li><code>POST /chat</code>: Non-streaming chat endpoint. Send a message and get the final response.</li> <li><code>GET /stream</code>: Server-Sent Events streaming endpoint for real-time updates.</li> <li><code>GET /health</code>: Health check endpoint with server status and session count.</li> <li><code>GET /sessions/{session_id}</code>: Get information about a specific session.</li> <li><code>DELETE /sessions/{session_id}</code>: Delete a session.</li> </ul> <p>Sessions maintain conversation memory across requests, enabling multi-turn conversations with the agent.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Host address to bind the server to. Defaults to \"0.0.0.0\" (all interfaces).</p> <code>'0.0.0.0'</code> <code>port</code> <code>int</code> <p>Port number to bind the server to. Defaults to 8000.</p> <code>8000</code> <code>session_timeout</code> <code>int</code> <p>Session expiration time in seconds. Sessions that haven't been accessed within this time are automatically cleaned up. Defaults to 3600 (1 hour).</p> <code>3600</code> <code>enable_cors</code> <code>bool</code> <p>Whether to enable CORS middleware for cross-origin requests from browsers. Defaults to True.</p> <code>True</code> <code>allowed_origins</code> <code>list[str] | None</code> <p>List of allowed origins for CORS. If None and CORS is enabled, allows all origins (\"*\").</p> <code>None</code> <code>log_level</code> <code>str</code> <p>Uvicorn log level. One of \"critical\", \"error\", \"warning\", \"info\", \"debug\", \"trace\". Defaults to \"info\".</p> <code>'info'</code> <p>Basic Usage</p> <pre><code>import weave\nfrom kawai import KawaiReactAgent, WebSearchTool, OpenAIModel\n\nweave.init(project_name=\"kawai-server\")\n\nmodel = OpenAIModel(\n    model_id=\"google/gemini-3-flash-preview\",\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key_env_var=\"OPENROUTER_API_KEY\",\n)\n\nagent = KawaiReactAgent(\n    model=model,\n    tools=[WebSearchTool()],\n    max_steps=10,\n)\n\n# Start the server (blocking)\nagent.serve(port=8000)\n</code></pre> <p>Client Usage - Non-streaming</p> <pre><code>import requests\n\n# Send a chat request\nresponse = requests.post(\n    \"http://localhost:8000/chat\",\n    json={\n        \"message\": \"What is the capital of France?\",\n        \"session_id\": \"user-123\"  # Optional: maintains conversation\n    }\n)\n\nresult = response.json()\nprint(result[\"answer\"])  # The agent's response\nprint(result[\"session_id\"])  # Use this for follow-up messages\n</code></pre> <p>Client Usage - Streaming</p> <pre><code>import requests\nimport json\n\n# Connect to streaming endpoint\nresponse = requests.get(\n    \"http://localhost:8000/stream\",\n    params={\n        \"message\": \"Search for latest AI news\",\n        \"session_id\": \"user-123\"\n    },\n    stream=True\n)\n\n# Process events as they arrive\nfor line in response.iter_lines():\n    if line:\n        data = line.decode().removeprefix(\"data: \")\n        event = json.loads(data)\n        print(f\"{event['type']}: {event['data']}\")\n</code></pre> Note <ul> <li>This method blocks until the server is shut down (Ctrl+C)</li> <li>The server uses uvicorn as the ASGI server</li> <li>Sessions are stored in memory and lost on server restart</li> <li>For production use, consider using a reverse proxy like nginx</li> </ul>"},{"location":"api_reference/callbacks/","title":"Callbacks","text":""},{"location":"api_reference/callbacks/#kawai.callback","title":"<code>kawai.callback</code>","text":""},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback","title":"<code>KawaiCallback</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for implementing callbacks to monitor agent execution.</p> <p>Callbacks provide hooks into the agent's execution lifecycle, allowing you to observe, log, or react to various events during agent runs. All callback methods are optional - override only the ones you need.</p> <p>Common use cases: - Logging agent behavior to console or files - Collecting metrics and performance data - Integrating with monitoring systems - Custom visualization of agent reasoning - Debugging and development</p> <p>Example</p> <pre><code>class MyCustomCallback(KawaiCallback):\n    def at_reasoning(self, reasoning: str):\n        print(f\"Agent is thinking: {reasoning}\")\n\n    def at_tool_call(self, tool_name: str, tool_arguments: dict[str, Any]):\n        print(f\"Calling {tool_name} with args: {tool_arguments}\")\n\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    callbacks=[MyCustomCallback()]\n)\n</code></pre> Note <ul> <li>All methods have default no-op implementations</li> <li>Callbacks are called synchronously during agent execution</li> <li>Multiple callbacks can be attached to a single agent</li> <li>Exceptions in callbacks will propagate and stop execution</li> </ul>"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_run_start","title":"<code>at_run_start(prompt, model)</code>","text":"<p>Called when the agent run begins.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The task or question given to the agent.</p> required <code>model</code> <code>str</code> <p>The model identifier being used (e.g., \"openai/gpt-4\").</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_run_end","title":"<code>at_run_end(answer, cumulative_input_tokens, cumulative_output_tokens, cumulative_total_tokens)</code>","text":"<p>Called when the agent run completes.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>str</code> <p>The final answer from the agent, or None if max_steps was reached without completion.</p> required <code>cumulative_input_tokens</code> <code>int</code> <p>Total input tokens used during the run.</p> required <code>cumulative_output_tokens</code> <code>int</code> <p>Total output tokens used during the run.</p> required <code>cumulative_total_tokens</code> <code>int</code> <p>Total tokens (input + output) used during the run.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_step_start","title":"<code>at_step_start(step_index)</code>","text":"<p>Called at the beginning of each ReAct step.</p> <p>Parameters:</p> Name Type Description Default <code>step_index</code> <code>int</code> <p>The zero-based index of the current step.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_planning_end","title":"<code>at_planning_end(plan, updated_plan)</code>","text":"<p>Called after the agent generates or updates a plan.</p> <p>Only triggered when planning_interval is set on the agent.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>str</code> <p>The generated or updated plan text.</p> required <code>updated_plan</code> <code>bool</code> <p>True if this is an updated plan, False if initial plan.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_reasoning","title":"<code>at_reasoning(reasoning)</code>","text":"<p>Called when the agent produces reasoning/thinking content.</p> <p>This is the agent's natural language explanation of what it's doing before it makes a tool call.</p> <p>Parameters:</p> Name Type Description Default <code>reasoning</code> <code>str</code> <p>The reasoning text produced by the agent.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_tool_call","title":"<code>at_tool_call(tool_name, tool_arguments)</code>","text":"<p>Called when the agent is about to execute a tool.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool being called.</p> required <code>tool_arguments</code> <code>dict[str, Any]</code> <p>The arguments passed to the tool.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_tool_result","title":"<code>at_tool_result(tool_name, tool_result)</code>","text":"<p>Called after a tool has been executed with its result.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool that was executed.</p> required <code>tool_result</code> <code>str</code> <p>The result from the tool, as a JSON string or plain string.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_tool_cache_hit","title":"<code>at_tool_cache_hit(tool_name)</code>","text":"<p>Called when a tool execution result is retrieved from cache.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool whose cached result was used.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_warning","title":"<code>at_warning(message)</code>","text":"<p>Called when the agent encounters a warning condition.</p> <p>Common warnings include the model not making a tool call when expected.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Description of the warning condition.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiCallback.at_step_end","title":"<code>at_step_end(step_index, cumulative_input_tokens, cumulative_output_tokens, cumulative_total_tokens)</code>","text":"<p>Called at the end of each ReAct step with cumulative token usage.</p> <p>Parameters:</p> Name Type Description Default <code>step_index</code> <code>int</code> <p>The zero-based index of the current step.</p> required <code>cumulative_input_tokens</code> <code>int</code> <p>Total input tokens used so far.</p> required <code>cumulative_output_tokens</code> <code>int</code> <p>Total output tokens used so far.</p> required <code>cumulative_total_tokens</code> <code>int</code> <p>Total tokens (input + output) used so far.</p> required"},{"location":"api_reference/callbacks/#kawai.callback.KawaiLoggingCallback","title":"<code>KawaiLoggingCallback</code>","text":"<p>               Bases: <code>KawaiCallback</code></p> <p>Rich console-based logging callback for visualizing agent execution.</p> <p>This callback provides formatted, colored console output for all agent events using the Rich library. It displays reasoning, tool calls, results, and warnings in visually distinct panels and styles.</p> <p>Output features: - Color-coded panels for different event types - Syntax highlighting for JSON tool arguments and results - Markdown rendering for reasoning and answers - Optional truncation of long outputs - Step separators with visual rules</p> <p>Attributes:</p> Name Type Description <code>truncate</code> <code>bool</code> <p>Whether to truncate long tool results. Defaults to False.</p> <code>max_result_length</code> <code>int</code> <p>Maximum length for tool results when truncate=True. Defaults to 1000 characters.</p> <p>Example</p> <pre><code>from kawai import KawaiReactAgent, WebSearchTool, KawaiLoggingCallback\n\n# Basic usage\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    callbacks=[KawaiLoggingCallback()]\n)\n\n# With truncation for long outputs\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    callbacks=[KawaiLoggingCallback(truncate=True, max_result_length=500)]\n)\n</code></pre> Visual Output <ul> <li>Run start/end: Yellow bordered panels</li> <li>Steps: Yellow horizontal rules</li> <li>Reasoning: Cyan panels with markdown</li> <li>Tool calls: Green panels with JSON syntax highlighting</li> <li>Tool results: Magenta panels (JSON or markdown)</li> <li>Warnings: Red panels</li> </ul> Note <ul> <li>Requires the <code>rich</code> library (included in dependencies)</li> <li>Outputs to stdout using Rich Console</li> <li>All content is formatted for terminal display</li> </ul>"},{"location":"api_reference/memory/","title":"Memory","text":""},{"location":"api_reference/memory/#kawai.memory.base","title":"<code>kawai.memory.base</code>","text":""},{"location":"api_reference/memory/#kawai.memory.base.BaseMemory","title":"<code>BaseMemory</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for conversation memory storage.</p> <p>Provides simple in-memory conversation history storage using OpenAI-compatible message format. This class can be subclassed to implement custom memory backends with persistent storage, semantic search, or other advanced features.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of conversation messages in OpenAI format. Each message is a dictionary with at least 'role' and 'content' keys. Additional keys like 'tool_calls' and 'tool_call_id' may be present for function calling.</p> <p>Example</p> <pre><code>memory = BaseMemory()\nmemory.add(\"Hello!\", role=\"user\")\nmemory.add(\"Hi there!\", role=\"assistant\")\nmessages = memory.get_messages()\nlen(messages)\n</code></pre>"},{"location":"api_reference/memory/#kawai.memory.base.BaseMemory.add","title":"<code>add(content, role, **kwargs)</code>","text":"<p>Add a message to conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content text.</p> required <code>role</code> <code>str</code> <p>The message role (e.g., \"system\", \"user\", \"assistant\", \"tool\").</p> required <code>**kwargs</code> <code>Any</code> <p>Additional message fields (e.g., tool_calls, tool_call_id).</p> <code>{}</code>"},{"location":"api_reference/memory/#kawai.memory.base.BaseMemory.get_messages","title":"<code>get_messages()</code>","text":"<p>Retrieve all conversation messages.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of message dictionaries in OpenAI format.</p>"},{"location":"api_reference/memory/#kawai.memory.base.BaseMemory.clear","title":"<code>clear()</code>","text":"<p>Clear all conversation history.</p>"},{"location":"api_reference/memory/#kawai.memory.base.BaseMemory.search","title":"<code>search(query, **kwargs)</code>","text":"<p>Search conversation history.</p> <p>Base implementation returns all messages. Subclasses can override to implement semantic search or other filtering strategies.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional search parameters (implementation-specific).</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of message dictionaries matching the search criteria.</p>"},{"location":"api_reference/memory/#kawai.memory.mem0_memory","title":"<code>kawai.memory.mem0_memory</code>","text":""},{"location":"api_reference/memory/#kawai.memory.mem0_memory.Mem0Memory","title":"<code>Mem0Memory</code>","text":"<p>               Bases: <code>BaseMemory</code></p> <p>Memory implementation with persistent storage and semantic search via Mem0.</p> <p>Extends BaseMemory with long-term conversation storage using the Mem0 library. Conversations are automatically stored in a vector database for semantic search across conversation history. Supports multiple storage backends (Chroma, Qdrant, etc.) and custom LLM/embedder configurations.</p> <p>Attributes:</p> Name Type Description <code>user_id</code> <code>str</code> <p>Unique identifier for the user. Used to namespace memories per user.</p> <code>mem0_config</code> <code>dict[str, Any] | None</code> <p>Configuration dictionary for Mem0. Should include 'llm', 'embedder', and 'vector_store' sections. If None, uses Mem0 defaults.</p> <code>model_config</code> <code>dict[str, Any]</code> <p>Pydantic configuration allowing arbitrary types.</p> <code>_mem0</code> <code>Memory | None</code> <p>Internal Mem0 Memory instance initialized after construction.</p> <p>Example</p> <pre><code>config = {\n    \"llm\": {\n        \"provider\": \"litellm\",\n        \"config\": {\n            \"model\": \"openrouter/google/gemini-3-flash-preview\",\n            \"api_key\": \"your-api-key\",\n        },\n    },\n    \"embedder\": {\n        \"provider\": \"huggingface\",\n        \"config\": {\"model\": \"sentence-transformers/all-MiniLM-L6-v2\"},\n    },\n    \"vector_store\": {\n        \"provider\": \"chroma\",\n        \"config\": {\"path\": \"./mem0_db\"}\n    },\n}\nmemory = Mem0Memory(user_id=\"user_123\", mem0_config=config)\nmemory.add(\"I like Python\", role=\"user\")\ncontext = memory.get_relevant_context(\"programming languages\")\n</code></pre> Note <p>Requires installing memory extras: <code>uv pip install -e \".[memory]\"</code></p>"},{"location":"api_reference/memory/#kawai.memory.mem0_memory.Mem0Memory.add","title":"<code>add(content, role, **kwargs)</code>","text":"<p>Add a message to both in-memory history and Mem0 persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content text.</p> required <code>role</code> <code>str</code> <p>The message role (e.g., \"system\", \"user\", \"assistant\", \"tool\").</p> required <code>**kwargs</code> <code>Any</code> <p>Additional message fields (e.g., tool_calls, tool_call_id).</p> <code>{}</code>"},{"location":"api_reference/memory/#kawai.memory.mem0_memory.Mem0Memory.search","title":"<code>search(query, **kwargs)</code>","text":"<p>Search conversation history using semantic similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query string for semantic matching.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional Mem0 search parameters (e.g., limit, filters).</p> <code>{}</code> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of memory dictionaries from Mem0 with similarity scores and metadata.</p>"},{"location":"api_reference/memory/#kawai.memory.mem0_memory.Mem0Memory.get_relevant_context","title":"<code>get_relevant_context(query, limit=5)</code>","text":"<p>Retrieve semantically relevant past conversations as formatted text.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query string to find relevant memories.</p> required <code>limit</code> <code>int</code> <p>Maximum number of memories to retrieve. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with bullet points of relevant memories, or empty</p> <code>str</code> <p>string if no memories found.</p> <p>Example</p> <pre><code>memory.add(\"I prefer TypeScript for web dev\", role=\"user\")\ncontext = memory.get_relevant_context(\"web development\", limit=3)\nprint(context)\n</code></pre>"},{"location":"api_reference/models/","title":"Kawai Models","text":""},{"location":"api_reference/models/#kawai.models.openai","title":"<code>kawai.models.openai</code>","text":""},{"location":"api_reference/models/#kawai.models.openai.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LLM interface that wraps OpenAI-compatible chat completion APIs.</p> <p>This class provides a unified interface for interacting with OpenAI-compatible APIs, including OpenAI, OpenRouter, and other providers that follow the OpenAI Chat Completions API format. It handles conversation memory management, tool calling, and integrates with Weave for experiment tracking.</p> <p>The model automatically initializes an OpenAI client on instantiation and maintains conversation history in the standard OpenAI message format.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>Model identifier to use for completions. Format depends on the provider: - OpenAI: \"gpt-4\", \"gpt-3.5-turbo\", etc. - OpenRouter: \"openai/gpt-4\", \"anthropic/claude-3-sonnet\",   \"google/gemini-3-flash-preview\", etc.</p> <code>system_prompt</code> <code>str | None</code> <p>Optional system prompt. Note: This attribute exists for backward compatibility but is not currently used. System prompts should be added to memory directly. Defaults to None.</p> <code>base_url</code> <code>str | None</code> <p>Base URL for the API endpoint. If None, uses OpenAI's default endpoint. Common values: - OpenRouter: \"https://openrouter.ai/api/v1\" - OpenAI: None or \"https://api.openai.com/v1\" Defaults to None.</p> <code>api_key_env_var</code> <code>str</code> <p>Name of the environment variable containing the API key. Defaults to \"OPENAI_API_KEY\".</p> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate in the completion. If None, uses the model's default limit. Setting this can help ensure complete reasoning responses. Defaults to None.</p> <code>memory</code> <code>list[dict[str, Any]]</code> <p>Conversation history in OpenAI Chat Completions format. Each message is a dict with \"role\" and \"content\" keys, plus optional \"tool_calls\" or \"tool_call_id\" for function calling. Defaults to empty list.</p> <p>Example</p> <pre><code>from kawai import OpenAIModel\n\n# Using OpenRouter\nmodel = OpenAIModel(\n    model_id=\"google/gemini-3-flash-preview\",\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key_env_var=\"OPENROUTER_API_KEY\",\n    max_tokens=4096  # Ensure complete responses\n)\n\n# Using OpenAI directly\nmodel = OpenAIModel(\n    model_id=\"gpt-4\",\n    api_key_env_var=\"OPENAI_API_KEY\"\n)\n\n# Add messages to memory\nmodel.update_memory(content=\"You are a helpful assistant.\", role=\"system\")\nmodel.update_memory(content=\"What is 2+2?\", role=\"user\")\n\n# Generate completion from memory\nresponse = model.predict_from_memory()\nprint(response.choices[0].message.content)\n</code></pre> Note <ul> <li>All prediction methods are tracked by Weave via <code>@weave.op</code> decorator</li> <li>The API key must be set in the specified environment variable</li> <li>Memory persists across multiple predictions until manually cleared</li> </ul>"},{"location":"api_reference/models/#kawai.models.openai.OpenAIModel.update_memory","title":"<code>update_memory(content, role, **kwargs)</code>","text":"<p>Append a message to the conversation memory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The message content.</p> required <code>role</code> <code>str</code> <p>The role of the message sender. Common values: - \"system\": System instructions - \"user\": User messages - \"assistant\": Assistant responses - \"tool\": Tool execution results</p> required <code>**kwargs</code> <code>Any</code> <p>Additional fields to include in the message dict, such as: - tool_calls: List of tool call objects for assistant messages - tool_call_id: ID linking tool responses to their calls</p> <code>{}</code> <p>Example</p> <pre><code># Add system message\nmodel.update_memory(content=\"You are helpful.\", role=\"system\")\n\n# Add user message\nmodel.update_memory(content=\"Hello!\", role=\"user\")\n\n# Add assistant message with tool calls\nmodel.update_memory(\n    content=\"Let me search for that.\",\n    role=\"assistant\",\n    tool_calls=[{\n        \"id\": \"call_123\",\n        \"type\": \"function\",\n        \"function\": {\"name\": \"web_search\", \"arguments\": '{\"query\": \"AI\"}'}\n    }]\n)\n\n# Add tool result\nmodel.update_memory(\n    content='{\"results\": [...]}',\n    role=\"tool\",\n    tool_call_id=\"call_123\"\n)\n</code></pre>"},{"location":"api_reference/models/#kawai.models.openai.OpenAIModel.get_cumulative_token_usage","title":"<code>get_cumulative_token_usage()</code>","text":"<p>Get cumulative token usage across all LLM calls.</p> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: Dictionary with keys: - input_tokens: Total input/prompt tokens used - output_tokens: Total output/completion tokens used - total_tokens: Total tokens (input + output)</p>"},{"location":"api_reference/models/#kawai.models.openai.OpenAIModel.predict_from_messages","title":"<code>predict_from_messages(messages, tools=None)</code>","text":"<p>Generate a chat completion from provided messages.</p> <p>This method calls the LLM with explicitly provided messages, independent of the internal memory. Useful for one-off predictions or when you need full control over the conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict[str, Any]]</code> <p>List of message dicts in OpenAI format. Each message should have \"role\" and \"content\" keys.</p> required <code>tools</code> <code>list[dict[str, Any]] | None</code> <p>Optional list of tool/function schemas in OpenAI function calling format. If provided, the model can call these tools in its response. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The model's response in OpenAI ChatCompletion format, containing choices with message content and optional tool calls.</p> <p>Example</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\n\nresponse = model.predict_from_messages(messages)\nprint(response.choices[0].message.content)\n\n# With tools\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather\",\n        \"parameters\": {\"type\": \"object\", \"properties\": {...}}\n    }\n}]\n\nresponse = model.predict_from_messages(messages, tools=tools)\n</code></pre> Note <ul> <li>This method does NOT update the internal memory</li> <li>Tracked by Weave for observability</li> <li>Tool calls in the response follow OpenAI's function calling format</li> </ul>"},{"location":"api_reference/models/#kawai.models.openai.OpenAIModel.predict_from_memory","title":"<code>predict_from_memory(tools=None)</code>","text":"<p>Generate a chat completion using the internal conversation memory.</p> <p>This method calls the LLM with the current state of the internal memory, making it easy to maintain multi-turn conversations. This is the primary method used by KawaiReactAgent for generating responses.</p> <p>Parameters:</p> Name Type Description Default <code>tools</code> <code>list[dict[str, Any]] | None</code> <p>Optional list of tool/function schemas in OpenAI function calling format. If provided, the model can call these tools in its response. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ChatCompletion</code> <code>ChatCompletion</code> <p>The model's response in OpenAI ChatCompletion format, containing choices with message content and optional tool calls.</p> <p>Example</p> <pre><code># Build conversation in memory\nmodel.update_memory(content=\"You are helpful.\", role=\"system\")\nmodel.update_memory(content=\"Hello!\", role=\"user\")\n\n# Generate response from memory\nresponse = model.predict_from_memory()\nassistant_message = response.choices[0].message.content\n\n# Add response to memory for next turn\nmodel.update_memory(content=assistant_message, role=\"assistant\")\n\n# With tools enabled\ntools = [tool.to_json_schema() for tool in agent_tools]\nresponse = model.predict_from_memory(tools=tools)\n</code></pre> Note <ul> <li>Uses the current state of self.memory</li> <li>Does NOT automatically update memory with the response</li> <li>Tracked by Weave for observability</li> <li>This is how KawaiReactAgent generates reasoning and tool calls</li> </ul>"},{"location":"api_reference/models/#kawai.models.prompt_cache","title":"<code>kawai.models.prompt_cache</code>","text":""},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCacheEntry","title":"<code>PromptCacheEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single cached API response with metadata.</p> <p>Each cache entry stores a ChatCompletion response along with timing and usage statistics. Entries are managed by the PromptCache class and subject to LRU eviction and TTL expiration.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>ChatCompletion</code> <p>The cached OpenAI ChatCompletion response object containing the model's output, including message content, tool calls, and usage statistics.</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp (seconds since epoch) when this entry was created. Used for TTL-based expiration.</p> <code>hits</code> <code>int</code> <p>Number of times this cached entry has been retrieved. Incremented on each cache hit. Defaults to 0.</p> <p>Example</p> <pre><code>from openai.types.chat import ChatCompletion\nfrom time import time\n\n# Create a cache entry\nentry = PromptCacheEntry(\n    response=chat_completion_response,\n    timestamp=time(),\n    hits=0\n)\n\n# Access entry data\nprint(entry.response.choices[0].message.content)\nprint(f\"Created at: {entry.timestamp}\")\nprint(f\"Hit count: {entry.hits}\")\n</code></pre> Note <ul> <li>Entries are automatically created by PromptCache.set()</li> <li>The hits counter is managed by PromptCache.get()</li> <li>Timestamps are used for automatic expiration checks</li> </ul>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache","title":"<code>PromptCache</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Native application-level prompt caching for OpenAI-compatible APIs.</p> <p>Implements an exact-match caching strategy with LRU eviction, TTL expiration, and persistent storage. Caches API responses based on deterministic hashing of messages, tools, and model ID to avoid redundant API calls and reduce costs and latency.</p> <p>The cache is provider-agnostic and works with any OpenAI-compatible API (OpenAI, OpenRouter, Anthropic via OpenAI SDK, etc.). It operates transparently at the application level without requiring provider-specific features.</p> <p>Attributes:</p> Name Type Description <code>max_size</code> <code>int</code> <p>Maximum number of cache entries to store. When exceeded, the least recently used (LRU) entry is evicted. Defaults to 100.</p> <code>time_to_live</code> <code>int</code> <p>Time-to-live in seconds for cache entries. Entries older than this are considered expired and automatically removed. Defaults to 3600 (1 hour).</p> <code>enabled</code> <code>bool</code> <p>Whether caching is active. When False, all cache operations are no-ops. Defaults to True.</p> <code>persist</code> <code>bool</code> <p>Whether to persist cache to disk. When True, cache is saved to disk after modifications and loaded on initialization. Defaults to True.</p> <code>cache_dir</code> <code>PathLike</code> <p>Directory path for persistent cache storage. Created automatically if it doesn't exist. Defaults to \"./.kawaicache\".</p> <code>cache</code> <code>OrderedDict[str, PromptCacheEntry]</code> <p>Internal cache storage mapping SHA256 hash keys to PromptCacheEntry objects. Maintains insertion order for LRU eviction.</p> <code>_hits</code> <code>int</code> <p>Internal counter for cache hits. Incremented when a cached response is successfully retrieved.</p> <code>_misses</code> <code>int</code> <p>Internal counter for cache misses. Incremented when a requested key is not found or has expired.</p> <p>Basic Usage</p> <pre><code>from kawai.models.openai import OpenAIModel\nfrom kawai.models.prompt_cache import PromptCache\n\n# Create model with default caching\nmodel = OpenAIModel(\n    model_id=\"openai/gpt-4\",\n    enable_cache=True  # Auto-creates PromptCache with defaults\n)\n\n# Or create with custom cache configuration\ncache = PromptCache(\n    max_size=200,\n    time_to_live=7200,  # 2 hours\n    cache_dir=\"./my_cache\"\n)\n\nmodel = OpenAIModel(\n    model_id=\"openai/gpt-4\",\n    enable_cache=True,\n    cache=cache\n)\n\n# Use normally - caching is transparent\nresponse = model.predict_from_messages(messages)\n\n# Check cache statistics\nstats = model.cache.stats()\nprint(f\"Hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Cache size: {stats['size']}/{stats['max_size']}\")\n</code></pre> <p>Manual Cache Management</p> <pre><code>cache = PromptCache()\n\n# Generate cache key\nkey = cache._generate_key(messages, tools, model_id)\n\n# Check for cached response\ncached = cache.get(key)\nif cached:\n    print(\"Cache hit!\")\nelse:\n    # Make API call and cache result\n    response = client.chat.completions.create(...)\n    cache.set(key, response)\n\n# Manually evict expired entries\nevicted_count = cache.evict_expired_entries()\n\n# Clear entire cache\ncache.clear()\n</code></pre> <p>In-Memory Only Cache</p> <pre><code># Disable persistence for temporary caching\ncache = PromptCache(\n    persist=False,\n    max_size=50,\n    time_to_live=1800  # 30 minutes\n)\n</code></pre> Note <ul> <li>Cache keys are SHA256 hashes of (model_id, messages, tools)</li> <li>Only exact matches are cached - no semantic similarity</li> <li>Persistence uses pickle format for simplicity</li> <li>Cache is automatically loaded on initialization if persist=True</li> <li>All cache modifications trigger disk saves when persist=True</li> <li>Thread-safety is not guaranteed - use separate instances per thread</li> </ul> Cache Strategy <ol> <li>Exact Match: Only identical requests (same messages, tools, model)    result in cache hits</li> <li>LRU Eviction: When max_size is reached, least recently used entry    is removed</li> <li>TTL Expiration: Entries older than time_to_live are automatically    removed on access</li> <li>Persistent Storage: Cache survives process restarts when persist=True</li> </ol> Performance Characteristics <ul> <li>Get: O(1) average case for hash lookup</li> <li>Set: O(1) average case, O(n) worst case for eviction</li> <li>Evict Expired: O(n) where n is cache size</li> <li>Disk I/O: Synchronous pickle serialization on every modification</li> </ul>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache.evict_expired_entries","title":"<code>evict_expired_entries()</code>","text":"<p>Remove all expired entries from the cache.</p> <p>Iterates through all cache entries and removes those whose age exceeds the configured time_to_live.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of entries that were evicted.</p> Note <ul> <li>Called automatically during cache initialization</li> <li>Saves to disk after eviction if persist=True</li> </ul>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve a cached response if valid.</p> <p>Looks up the cache key and returns the cached ChatCompletion if found and not expired. Updates LRU ordering and hit statistics on successful retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>SHA256 hash key generated by _generate_key().</p> required <p>Returns:</p> Type Description <code>ChatCompletion | None</code> <p>ChatCompletion | None: The cached response if found and valid, None if key not found, expired, or caching is disabled.</p> Side Effects <ul> <li>Increments _hits counter on cache hit</li> <li>Increments _misses counter on cache miss or expiration</li> <li>Updates LRU ordering (moves entry to end)</li> <li>Increments entry.hits counter</li> <li>Removes expired entries</li> <li>Saves to disk if persist=True</li> </ul> Note <ul> <li>Returns None immediately if enabled=False</li> <li>Expired entries are removed during lookup</li> <li>O(1) average case performance</li> </ul> <p>Example</p> <pre><code>cache = PromptCache()\nkey = cache._generate_key(messages, tools, model_id)\n\nresponse = cache.get(key)\nif response:\n    print(\"Cache hit!\")\n    print(response.choices[0].message.content)\nelse:\n    print(\"Cache miss - need to call API\")\n</code></pre>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache.set","title":"<code>set(key, response)</code>","text":"<p>Store a response in cache with automatic LRU eviction.</p> <p>Adds a new cache entry or updates an existing one. If the cache is at capacity, evicts the least recently used entry before adding the new one.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>SHA256 hash key generated by _generate_key().</p> required <code>response</code> <code>ChatCompletion</code> <p>The OpenAI ChatCompletion response to cache.</p> required Side Effects <ul> <li>Creates new PromptCacheEntry with current timestamp</li> <li>Evicts LRU entry if cache is full</li> <li>Updates LRU ordering (moves entry to end)</li> <li>Saves to disk if persist=True</li> </ul> Note <ul> <li>No-op if enabled=False</li> <li>Updating an existing key refreshes its timestamp</li> <li>O(1) average case, O(n) worst case for eviction</li> </ul> <p>Example</p> <pre><code>cache = PromptCache(max_size=100)\n\n# Make API call\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=messages\n)\n\n# Cache the response\nkey = cache._generate_key(messages, None, \"gpt-4\")\ncache.set(key, response)\n\n# Future identical requests will hit cache\ncached = cache.get(key)  # Returns response instantly\n</code></pre>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache.clear","title":"<code>clear()</code>","text":"<p>Clear all cache entries and reset statistics.</p> <p>Removes all cached responses and resets hit/miss counters to zero. Persists the empty cache to disk if persist=True.</p> Side Effects <ul> <li>Removes all cache entries</li> <li>Resets _hits to 0</li> <li>Resets _misses to 0</li> <li>Saves empty cache to disk if persist=True</li> </ul> <p>Example</p> <pre><code>cache = PromptCache()\n\n# ... use cache ...\n\n# Clear everything\ncache.clear()\nassert cache.stats()[\"size\"] == 0\nassert cache.stats()[\"hits\"] == 0\n</code></pre>"},{"location":"api_reference/models/#kawai.models.prompt_cache.PromptCache.stats","title":"<code>stats()</code>","text":"<p>Get cache performance statistics.</p> <p>Returns a dictionary containing cache usage metrics including hit/miss counts, current size, and calculated hit rate.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary with the following keys: - hits (int): Total number of cache hits - misses (int): Total number of cache misses - size (int): Current number of entries in cache - max_size (int): Maximum cache capacity - hit_rate (float): Cache hit rate (hits / total requests), 0.0-1.0 - enabled (bool): Whether caching is currently enabled - persist (bool): Whether cache persistence is enabled</p> Note <ul> <li>Hit rate is 0.0 if no requests have been made</li> <li>Statistics persist across cache reloads when persist=True</li> <li>Useful for monitoring cache effectiveness</li> </ul> <p>Example</p> <pre><code>cache = PromptCache()\n\n# ... use cache ...\n\nstats = cache.stats()\nprint(f\"Cache hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Cache utilization: {stats['size']}/{stats['max_size']}\")\nprint(f\"Total requests: {stats['hits'] + stats['misses']}\")\n\n# Example output:\n# Cache hit rate: 67.50%\n# Cache utilization: 45/100\n# Total requests: 80\n</code></pre>"},{"location":"api_reference/models/#kawai.models.tool_cache","title":"<code>kawai.models.tool_cache</code>","text":""},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCacheEntry","title":"<code>ToolCacheEntry</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a cached tool execution result with metadata.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>Any</code> <p>The tool execution result (can be dict, str, int, etc.)</p> <code>timestamp</code> <code>float</code> <p>Unix timestamp when this entry was created</p> <code>hits</code> <code>int</code> <p>Number of times this cached entry has been retrieved</p>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache","title":"<code>ToolCache</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Native application-level caching for tool execution results.</p> <p>Implements an exact-match caching strategy with LRU eviction, TTL expiration, and persistent storage. Caches tool execution results based on deterministic hashing of tool name and arguments to avoid redundant tool executions and improve agent performance.</p> <p>This cache operates independently from the LLM prompt cache and is specifically designed for caching deterministic tool outputs. It's particularly useful for expensive operations like API calls, web scraping, or database queries.</p> <p>Attributes:</p> Name Type Description <code>max_size</code> <code>int</code> <p>Maximum number of cache entries to store. When exceeded, the least recently used (LRU) entry is evicted. Defaults to 100.</p> <code>time_to_live</code> <code>int</code> <p>Time-to-live in seconds for cache entries. Entries older than this are considered expired and automatically removed. Defaults to 3600 (1 hour).</p> <code>enabled</code> <code>bool</code> <p>Whether caching is active. When False, all cache operations are no-ops. Defaults to True.</p> <code>persist</code> <code>bool</code> <p>Whether to persist cache to disk. When True, cache is saved to disk after modifications and loaded on initialization. Defaults to True.</p> <code>cache_dir</code> <code>PathLike</code> <p>Directory path for persistent cache storage. Created automatically if it doesn't exist. Defaults to \"./.kawai_cache\".</p> <code>cache</code> <code>OrderedDict[str, ToolCacheEntry]</code> <p>Internal cache storage mapping SHA256 hash keys to ToolCacheEntry objects. Maintains insertion order for LRU eviction.</p> <code>_hits</code> <code>int</code> <p>Internal counter for cache hits. Incremented when a cached result is successfully retrieved.</p> <code>_misses</code> <code>int</code> <p>Internal counter for cache misses. Incremented when a requested key is not found or has expired.</p> <p>Basic Usage with Agent</p> <pre><code>from kawai import KawaiReactAgent, OpenAIModel, WebSearchTool\n\n# Create model with tool caching enabled\nmodel = OpenAIModel(\n    model_id=\"google/gemini-3-flash-preview\",\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key_env_var=\"OPENROUTER_API_KEY\",\n    enable_tool_cache=True  # Auto-creates ToolCache with defaults\n)\n\nagent = KawaiReactAgent(\n    model=model,\n    tools=[WebSearchTool()],\n    max_steps=10\n)\n\n# First run - tool executes normally\nresult1 = agent.run(\"Search for Python tutorials\")\n\n# Second run with same query - uses cached tool result\nresult2 = agent.run(\"Search for Python tutorials\")\n\n# Check cache statistics\nstats = model.tool_cache.stats()\nprint(f\"Hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Cache size: {stats['size']}/{stats['max_size']}\")\n</code></pre> <p>Custom Cache Configuration</p> <pre><code>from kawai.models.tool_cache import ToolCache\nfrom kawai import OpenAIModel\n\n# Create cache with custom settings\ntool_cache = ToolCache(\n    max_size=200,\n    time_to_live=1800,  # 30 minutes for time-sensitive data\n    cache_dir=\"./my_tool_cache\"\n)\n\nmodel = OpenAIModel(\n    model_id=\"openai/gpt-4\",\n    enable_tool_cache=True,\n    tool_cache=tool_cache\n)\n\n# Check cache stats\nprint(tool_cache.stats())\n</code></pre> <p>In-Memory Only Cache</p> <pre><code># Disable persistence for temporary caching\ncache = ToolCache(\n    persist=False,\n    max_size=50,\n    time_to_live=900  # 15 minutes\n)\n</code></pre> Note <ul> <li>Cache keys are SHA256 hashes of (tool_name, tool_arguments)</li> <li>Only exact matches are cached - no semantic similarity</li> <li>Persistence uses pickle format for simplicity</li> <li>Cache is automatically loaded on initialization if persist=True</li> <li>All cache modifications trigger disk saves when persist=True</li> <li>Thread-safety is not guaranteed - use separate instances per thread</li> </ul> Cache Strategy <ol> <li>Exact Match: Only identical tool calls (same name, same arguments)    result in cache hits</li> <li>LRU Eviction: When max_size is reached, least recently used entry    is removed</li> <li>TTL Expiration: Entries older than time_to_live are automatically    removed on access</li> <li>Persistent Storage: Cache survives process restarts when persist=True</li> </ol> Performance Characteristics <ul> <li>Get: O(1) average case for hash lookup</li> <li>Set: O(1) average case, O(n) worst case for eviction</li> <li>Evict Expired: O(n) where n is cache size</li> <li>Disk I/O: Synchronous pickle serialization on every modification</li> </ul> Warning <ul> <li>Non-deterministic tools (e.g., current_time, random_number) should   not be cached or should use very short TTLs</li> <li>Tools with side effects (e.g., send_email, create_record) should   typically not be cached</li> <li>Consider setting cacheable=False on tool definitions for such cases</li> </ul>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache.evict_expired_entries","title":"<code>evict_expired_entries()</code>","text":"<p>Remove all expired entries from the cache.</p> <p>Iterates through all cache entries and removes those whose age exceeds the configured time_to_live.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of entries that were evicted.</p> Note <ul> <li>Called automatically during cache initialization</li> <li>Saves to disk after eviction if persist=True</li> <li>O(n) time complexity where n is the number of cache entries</li> </ul>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache.get","title":"<code>get(key)</code>","text":"<p>Retrieve a cached tool result if valid.</p> <p>Looks up the cache key and returns the cached tool execution result if found and not expired. Updates LRU ordering and hit statistics on successful retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>SHA256 hash key generated by _generate_key().</p> required <p>Returns:</p> Type Description <code>Any | None</code> <p>Any | None: The cached tool result if found and valid, None if key not found, expired, or caching is disabled. Result type depends on what the tool returned (dict, str, int, etc.).</p> Side Effects <ul> <li>Increments _hits counter on cache hit</li> <li>Increments _misses counter on cache miss or expiration</li> <li>Updates LRU ordering (moves entry to end)</li> <li>Increments entry.hits counter</li> <li>Removes expired entries</li> <li>Saves to disk if persist=True</li> </ul> Note <ul> <li>Returns None immediately if enabled=False</li> <li>Expired entries are removed during lookup</li> <li>O(1) average case performance</li> </ul> <p>Example</p> <pre><code>cache = ToolCache()\n\n# Generate key for a tool call\nkey = cache._generate_key(\"web_search\", {\"query\": \"AI news\"})\n\n# Try to get cached result\nresult = cache.get(key)\nif result:\n    print(\"Cache hit!\")\n    print(result)\nelse:\n    print(\"Cache miss - need to execute tool\")\n</code></pre>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache.set","title":"<code>set(key, result)</code>","text":"<p>Store a tool result in cache with automatic LRU eviction.</p> <p>Adds a new cache entry or updates an existing one. If the cache is at capacity, evicts the least recently used entry before adding the new one.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>SHA256 hash key generated by _generate_key().</p> required <code>result</code> <code>Any</code> <p>The tool execution result to cache. Can be any type (dict, str, int, list, etc.) as long as it's pickle-serializable.</p> required Side Effects <ul> <li>Creates new ToolCacheEntry with current timestamp</li> <li>Evicts LRU entry if cache is full</li> <li>Updates LRU ordering (moves entry to end)</li> <li>Saves to disk if persist=True</li> </ul> Note <ul> <li>No-op if enabled=False</li> <li>Updating an existing key refreshes its timestamp</li> <li>O(1) average case, O(n) worst case for eviction</li> </ul> <p>Example</p> <pre><code>cache = ToolCache(max_size=100)\n\n# Execute a tool\ntool_result = web_search_tool.forward(query=\"Python tutorials\")\n\n# Cache the result\nkey = cache._generate_key(\"web_search\", {\"query\": \"Python tutorials\"})\ncache.set(key, tool_result)\n\n# Future identical calls will hit cache\ncached = cache.get(key)  # Returns tool_result instantly\n</code></pre>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache.clear","title":"<code>clear()</code>","text":"<p>Clear all cache entries and reset statistics.</p> <p>Removes all cached tool results and resets hit/miss counters to zero. Persists the empty cache to disk if persist=True.</p> Side Effects <ul> <li>Removes all cache entries</li> <li>Resets _hits to 0</li> <li>Resets _misses to 0</li> <li>Saves empty cache to disk if persist=True</li> </ul> <p>Example</p> <pre><code>cache = ToolCache()\n\n# ... use cache ...\n\n# Clear everything\ncache.clear()\nassert cache.stats()[\"size\"] == 0\nassert cache.stats()[\"hits\"] == 0\n</code></pre>"},{"location":"api_reference/models/#kawai.models.tool_cache.ToolCache.stats","title":"<code>stats()</code>","text":"<p>Get cache performance statistics.</p> <p>Returns a dictionary containing cache usage metrics including hit/miss counts, current size, and calculated hit rate.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Dictionary with the following keys: - hits (int): Total number of cache hits - misses (int): Total number of cache misses - size (int): Current number of entries in cache - max_size (int): Maximum cache capacity - hit_rate (float): Cache hit rate (hits / total requests), 0.0-1.0 - enabled (bool): Whether caching is currently enabled - persist (bool): Whether cache persistence is enabled</p> Note <ul> <li>Hit rate is 0.0 if no requests have been made</li> <li>Statistics persist across cache reloads when persist=True</li> <li>Useful for monitoring cache effectiveness</li> </ul> <p>Example</p> <pre><code>cache = ToolCache()\n\n# ... use cache ...\n\nstats = cache.stats()\nprint(f\"Cache hit rate: {stats['hit_rate']:.2%}\")\nprint(f\"Cache utilization: {stats['size']}/{stats['max_size']}\")\nprint(f\"Total requests: {stats['hits'] + stats['misses']}\")\n\n# Example output:\n# Cache hit rate: 75.00%\n# Cache utilization: 42/100\n# Total requests: 56\n</code></pre>"},{"location":"api_reference/tools/","title":"Kawai Tools","text":""},{"location":"api_reference/tools/#kawai.tools.tool","title":"<code>kawai.tools.tool</code>","text":""},{"location":"api_reference/tools/#kawai.tools.tool.KawaiToolParameter","title":"<code>KawaiToolParameter</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Defines a parameter for a KawaiTool.</p> <p>This class specifies the schema for a single parameter that a tool accepts, including its name, type, description, and validation constraints.</p> <p>Attributes:</p> Name Type Description <code>param_name</code> <code>str</code> <p>The name of the parameter as it will appear in tool calls.</p> <code>description</code> <code>str</code> <p>Human-readable description of what the parameter does.</p> <code>tool_type</code> <code>Literal['string', 'number', 'boolean', 'object', 'array', 'any']</code> <p>JSON schema type of the parameter. Supported types are:     - \"string\": Text values     - \"number\": Numeric values (int or float)     - \"boolean\": True/False values     - \"object\": JSON objects/dictionaries     - \"array\": JSON arrays/lists     - \"any\": Any JSON value (no type constraint)</p> <code>required</code> <code>bool</code> <p>Whether this parameter must be provided in tool calls. Defaults to True.</p> <code>nullable</code> <code>bool</code> <p>Whether the parameter can be null in addition to its declared type. Defaults to False.</p> <p>Example</p> <pre><code>param = KawaiToolParameter(\n    param_name=\"query\",\n    description=\"The search query to execute\",\n    tool_type=\"string\",\n    required=True,\n    nullable=False\n)\n</code></pre>"},{"location":"api_reference/tools/#kawai.tools.tool.KawaiTool","title":"<code>KawaiTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for all tools that can be used by KawaiReactAgent.</p> <p>Tools are functions that agents can call to interact with external systems, retrieve information, or perform actions. Each tool must define its name, description, parameters, and implement the <code>forward()</code> method.</p> <p>All tool executions are automatically tracked by Weave when the <code>forward()</code> method is decorated with <code>@weave.op</code>.</p> <p>Attributes:</p> Name Type Description <code>tool_name</code> <code>str</code> <p>Unique identifier for the tool. Used in function calling.</p> <code>description</code> <code>str</code> <p>Human-readable description of what the tool does. Shown to the agent to help it decide when to use this tool.</p> <code>parameters</code> <code>list[KawaiToolParameter]</code> <p>List of KawaiToolParameter objects defining the tool's inputs.</p> <code>cacheable</code> <code>bool</code> <p>Whether the tool's output can be cached. Defaults to True. Set to False for non-deterministic tools or tools with side effects.</p> <p>Example</p> <pre><code>class CalculatorTool(KawaiTool):\n    tool_name: str = \"calculator\"\n    description: str = \"Performs basic arithmetic operations\"\n    parameters: list[KawaiToolParameter] = [\n        KawaiToolParameter(\n            param_name=\"expression\",\n            description=\"Mathematical expression to evaluate\",\n            tool_type=\"string\"\n        )\n    ]\n    cacheable: bool = True\n\n    @weave.op\n    def forward(self, expression: str) -&gt; dict[str, Any]:\n        result = eval(expression)  # Note: unsafe, just for demo\n        return {\"result\": result}\n</code></pre> Note <ul> <li>Subclasses must implement the forward() method</li> <li>The forward() method should be decorated with @weave.op for tracking</li> <li>Return values should be JSON-serializable (dict, str, int, float, list, etc.)</li> </ul>"},{"location":"api_reference/tools/#kawai.tools.tool.KawaiTool.to_json_schema","title":"<code>to_json_schema()</code>","text":"<p>Convert the tool definition to OpenAI function calling JSON schema format.</p> <p>This method generates the schema that gets passed to the LLM so it knows how to call this tool. The schema follows the OpenAI function calling format.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the tool schema with structure:</p> <code>dict</code> <p>```python</p> <code>dict</code> <p>{ \"type\": \"function\", \"function\": {     \"name\": \"tool_name\",     \"description\": \"tool description\",     \"parameters\": {         \"type\": \"object\",         \"properties\": {...},         \"required\": [...],         \"additionalProperties\": False     } }</p> <code>dict</code> <p>}</p> <code>dict</code> <p>```</p> Note <p>This is called automatically by the agent when registering tools. You typically don't need to call this method directly.</p>"},{"location":"api_reference/tools/#kawai.tools.tool.KawaiTool.forward","title":"<code>forward(**kwargs)</code>","text":"<p>Execute the tool with the provided arguments.</p> <p>This is the main method that implements the tool's functionality. It must be overridden by subclasses to define what the tool actually does.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>The tool parameters as specified in the parameters list. Parameter names and types must match the tool's parameter definitions.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The tool's output, which should be JSON-serializable. Can be:</p> <code>dict[str, Any]</code> <ul> <li>dict: For structured results</li> </ul> <code>dict[str, Any]</code> <ul> <li>str: For simple text results</li> </ul> <code>dict[str, Any]</code> <ul> <li>int/float: For numeric results</li> </ul> <code>dict[str, Any]</code> <ul> <li>list: For multiple results</li> </ul> <code>dict[str, Any]</code> <ul> <li>Any other JSON-serializable type</li> </ul> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass hasn't implemented this method.</p> Note <ul> <li>This method should be decorated with @weave.op in subclasses</li> <li>Return values are automatically serialized to JSON by the agent</li> <li>Errors raised here are caught and returned as {\"error\": \"...\"} to the agent</li> </ul>"},{"location":"api_reference/tools/#kawai.tools.answer","title":"<code>kawai.tools.answer</code>","text":""},{"location":"api_reference/tools/#kawai.tools.answer.FinalAnswerTool","title":"<code>FinalAnswerTool</code>","text":"<p>               Bases: <code>KawaiTool</code></p> <p>Tool for providing the final answer to complete a task.</p> <p>This is a special tool that MUST be called by the agent to complete any task. The agent will loop until either this tool is called or max_steps is reached. It's automatically added to every agent if not explicitly provided.</p> <p>The tool accepts any type of answer (string, number, dict, list, etc.) and returns it unchanged (passthrough behavior). The agent extracts this value as the final result.</p> <p>Attributes:</p> Name Type Description <code>tool_name</code> <code>str</code> <p>Always \"final_answer\"</p> <code>description</code> <code>str</code> <p>Brief description shown to the agent</p> <code>parameters</code> <code>list[KawaiToolParameter]</code> <p>Single parameter \"answer\" of type \"any\"</p> Example <pre><code># Agent calls this when ready to complete task\nfinal_answer(answer=\"The population of Shanghai is 26 million\")\n# Or with structured data\nfinal_answer(answer={\"city\": \"Shanghai\", \"population\": 26000000})\n</code></pre> Note <ul> <li>This tool is required for task completion</li> <li>The agent cannot complete a task without calling this</li> <li>Automatically added to agent.tools if not present</li> </ul>"},{"location":"api_reference/tools/#kawai.tools.answer.FinalAnswerTool.forward","title":"<code>forward(answer)</code>","text":"<p>Return the answer unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>Any</code> <p>The final answer to the task. Can be any JSON-serializable type.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The answer exactly as provided (passthrough).</p>"},{"location":"api_reference/tools/#kawai.tools.answer.UserInputTool","title":"<code>UserInputTool</code>","text":"<p>               Bases: <code>KawaiTool</code></p> <p>Tool for prompting the user for interactive input during agent execution.</p> <p>This tool allows the agent to ask clarifying questions or gather additional information from the user via command-line input. Useful for interactive agents that need user decisions or information that can't be obtained through other means.</p> <p>Attributes:</p> Name Type Description <code>tool_name</code> <code>str</code> <p>Always \"user_input\"</p> <code>description</code> <code>str</code> <p>Brief description shown to the agent</p> <code>parameters</code> <code>list[KawaiToolParameter]</code> <p>Single parameter \"question\" of type \"string\"</p> Example <pre><code># Agent usage in a task\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[UserInputTool()],\n    max_steps=10\n)\n\n# The agent might call this when it needs clarification:\n# user_input(question=\"What year should I filter the search results by?\")\n# User then types their response at the prompt\n</code></pre> Note <ul> <li>This tool blocks execution waiting for user input</li> <li>Input is collected via the command line (stdin)</li> <li>Returns the raw string entered by the user</li> </ul>"},{"location":"api_reference/tools/#kawai.tools.answer.UserInputTool.forward","title":"<code>forward(question)</code>","text":"<p>Prompt the user for input and return their response.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question or prompt to display to the user.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The user's input as a string.</p>"},{"location":"api_reference/tools/#kawai.tools.web_search","title":"<code>kawai.tools.web_search</code>","text":""},{"location":"api_reference/tools/#kawai.tools.web_search.WebSearchTool","title":"<code>WebSearchTool</code>","text":"<p>               Bases: <code>KawaiTool</code></p> <p>Tool for performing Google web searches using the Serper API.</p> <p>This tool allows agents to search the web for current information. It uses the Serper API to perform Google searches and returns formatted results including titles, links, dates, sources, and snippets.</p> <p>Requires the <code>SERPER_API_KEY</code> environment variable to be set.</p> <p>Attributes:</p> Name Type Description <code>tool_name</code> <code>str</code> <p>Always \"web_search\"</p> <code>description</code> <code>str</code> <p>Brief description shown to the agent</p> <code>parameters</code> <code>list[KawaiToolParameter]</code> <p>Two parameters: - query (required, string): The search query - filter_year (optional, number): Restrict results to a specific year</p> <p>Example</p> <pre><code>from kawai import KawaiReactAgent, WebSearchTool\n\nagent = KawaiReactAgent(\n    model=\"openai/gpt-4\",\n    tools=[WebSearchTool()],\n    max_steps=5\n)\n\n# Agent will use this to search:\n# web_search(query=\"Python ReAct agents\", filter_year=2024)\n</code></pre> Note <ul> <li>Requires SERPER_API_KEY environment variable</li> <li>Returns up to 10 search results by default (Serper API default)</li> <li>Year filtering uses Google's date restriction syntax</li> <li>If no results found, returns helpful error message</li> </ul>"},{"location":"api_reference/tools/#kawai.tools.web_search.WebSearchTool.serper_api_results","title":"<code>serper_api_results(query, filter_year=None)</code>","text":"<p>Call the Serper API to perform a Google search.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query string.</p> required <code>filter_year</code> <code>int | None</code> <p>Optional year to restrict results (e.g., 2024).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Raw JSON response from Serper API containing search results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API request fails (non-200 status code).</p>"},{"location":"api_reference/tools/#kawai.tools.web_search.WebSearchTool.forward","title":"<code>forward(query, filter_year=None)</code>","text":"<p>Execute a web search and return formatted results.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to execute.</p> required <code>filter_year</code> <code>int | None</code> <p>Optional year (e.g., 2024) to restrict results to that specific year.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict[str, Any]</code> <p>A dictionary with a \"results\" key containing either:</p> <code>dict[str, Any]</code> <ul> <li>A list of formatted search result strings (if results found)</li> </ul> <code>dict[str, Any]</code> <ul> <li>An error message string (if no results found)</li> </ul> <code>dict[str, Any]</code> <p>Each result string has the format:</p> <code>dict[str, Any]</code> <p>```text</p> <code>dict[str, Any]</code> <ol> <li>Title</li> </ol> <code>dict[str, Any]</code> <p>Date published: YYYY-MM-DD</p> <code>Source</code> <code>dict[str, Any]</code> <p>source_name</p> <code>dict[str, Any]</code> <p>snippet text...</p> <code>dict[str, Any]</code> <p>```</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If no results found with year filter applied. This helps the agent understand it should retry without the filter.</p> <code>ValueError</code> <p>If the Serper API request fails.</p> <p>Example</p> <pre><code>tool = WebSearchTool()\nresults = tool.forward(query=\"Python asyncio\", filter_year=2024)\n# Returns: {\"results\": [\"0. [Title](https://example.com)\\n...\", ...]}\n</code></pre>"}]}
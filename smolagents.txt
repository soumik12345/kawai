Directory structure:
â””â”€â”€ huggingface-smolagents/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ AGENTS.md
    â”œâ”€â”€ CODE_OF_CONDUCT.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ e2b.toml
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ SECURITY.md
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ docs/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ source/
    â”‚       â”œâ”€â”€ en/
    â”‚       â”‚   â”œâ”€â”€ _config.py
    â”‚       â”‚   â”œâ”€â”€ _toctree.yml
    â”‚       â”‚   â”œâ”€â”€ guided_tour.md
    â”‚       â”‚   â”œâ”€â”€ index.md
    â”‚       â”‚   â”œâ”€â”€ installation.md
    â”‚       â”‚   â”œâ”€â”€ conceptual_guides/
    â”‚       â”‚   â”‚   â”œâ”€â”€ intro_agents.md
    â”‚       â”‚   â”‚   â””â”€â”€ react.md
    â”‚       â”‚   â”œâ”€â”€ examples/
    â”‚       â”‚   â”‚   â”œâ”€â”€ async_agent.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ multiagents.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ plan_customization.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ rag.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ text_to_sql.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ using_different_models.md
    â”‚       â”‚   â”‚   â””â”€â”€ web_browser.md
    â”‚       â”‚   â”œâ”€â”€ reference/
    â”‚       â”‚   â”‚   â”œâ”€â”€ agents.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ default_tools.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ models.md
    â”‚       â”‚   â”‚   â””â”€â”€ tools.md
    â”‚       â”‚   â””â”€â”€ tutorials/
    â”‚       â”‚       â”œâ”€â”€ building_good_agents.md
    â”‚       â”‚       â”œâ”€â”€ inspect_runs.md
    â”‚       â”‚       â”œâ”€â”€ memory.md
    â”‚       â”‚       â”œâ”€â”€ secure_code_execution.md
    â”‚       â”‚       â””â”€â”€ tools.md
    â”‚       â”œâ”€â”€ es/
    â”‚       â”‚   â”œâ”€â”€ _config.py
    â”‚       â”‚   â”œâ”€â”€ _toctree.yml
    â”‚       â”‚   â”œâ”€â”€ index.md
    â”‚       â”‚   â””â”€â”€ installation.md
    â”‚       â”œâ”€â”€ hi/
    â”‚       â”‚   â”œâ”€â”€ _config.py
    â”‚       â”‚   â”œâ”€â”€ _toctree.yml
    â”‚       â”‚   â”œâ”€â”€ guided_tour.md
    â”‚       â”‚   â”œâ”€â”€ index.md
    â”‚       â”‚   â”œâ”€â”€ conceptual_guides/
    â”‚       â”‚   â”‚   â”œâ”€â”€ intro_agents.md
    â”‚       â”‚   â”‚   â””â”€â”€ react.md
    â”‚       â”‚   â”œâ”€â”€ examples/
    â”‚       â”‚   â”‚   â”œâ”€â”€ multiagents.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ rag.md
    â”‚       â”‚   â”‚   â””â”€â”€ text_to_sql.md
    â”‚       â”‚   â”œâ”€â”€ reference/
    â”‚       â”‚   â”‚   â”œâ”€â”€ agents.md
    â”‚       â”‚   â”‚   â””â”€â”€ tools.md
    â”‚       â”‚   â””â”€â”€ tutorials/
    â”‚       â”‚       â”œâ”€â”€ building_good_agents.md
    â”‚       â”‚       â”œâ”€â”€ inspect_runs.md
    â”‚       â”‚       â”œâ”€â”€ secure_code_execution.md
    â”‚       â”‚       â””â”€â”€ tools.md
    â”‚       â”œâ”€â”€ ko/
    â”‚       â”‚   â”œâ”€â”€ _config.py
    â”‚       â”‚   â”œâ”€â”€ _toctree.yml
    â”‚       â”‚   â”œâ”€â”€ guided_tour.md
    â”‚       â”‚   â”œâ”€â”€ index.md
    â”‚       â”‚   â”œâ”€â”€ installation.md
    â”‚       â”‚   â”œâ”€â”€ conceptual_guides/
    â”‚       â”‚   â”‚   â””â”€â”€ react.md
    â”‚       â”‚   â”œâ”€â”€ examples/
    â”‚       â”‚   â”‚   â”œâ”€â”€ async_agent.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ multiagents.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ plan_customization.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ rag.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ text_to_sql.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ using_different_models.md
    â”‚       â”‚   â”‚   â””â”€â”€ web_browser.md
    â”‚       â”‚   â”œâ”€â”€ reference/
    â”‚       â”‚   â”‚   â”œâ”€â”€ agents.md
    â”‚       â”‚   â”‚   â”œâ”€â”€ models.md
    â”‚       â”‚   â”‚   â””â”€â”€ tools.md
    â”‚       â”‚   â””â”€â”€ tutorials/
    â”‚       â”‚       â”œâ”€â”€ building_good_agents.md
    â”‚       â”‚       â”œâ”€â”€ inspect_runs.md
    â”‚       â”‚       â””â”€â”€ memory.md
    â”‚       â””â”€â”€ zh/
    â”‚           â”œâ”€â”€ _config.py
    â”‚           â”œâ”€â”€ _toctree.yml
    â”‚           â”œâ”€â”€ guided_tour.md
    â”‚           â”œâ”€â”€ index.md
    â”‚           â”œâ”€â”€ conceptual_guides/
    â”‚           â”‚   â”œâ”€â”€ intro_agents.md
    â”‚           â”‚   â””â”€â”€ react.md
    â”‚           â”œâ”€â”€ examples/
    â”‚           â”‚   â”œâ”€â”€ multiagents.md
    â”‚           â”‚   â”œâ”€â”€ rag.md
    â”‚           â”‚   â”œâ”€â”€ text_to_sql.md
    â”‚           â”‚   â””â”€â”€ web_browser.md
    â”‚           â”œâ”€â”€ reference/
    â”‚           â”‚   â”œâ”€â”€ agents.md
    â”‚           â”‚   â”œâ”€â”€ models.md
    â”‚           â”‚   â””â”€â”€ tools.md
    â”‚           â””â”€â”€ tutorials/
    â”‚               â”œâ”€â”€ building_good_agents.md
    â”‚               â”œâ”€â”€ inspect_runs.md
    â”‚               â”œâ”€â”€ memory.md
    â”‚               â”œâ”€â”€ secure_code_execution.md
    â”‚               â””â”€â”€ tools.md
    â”œâ”€â”€ examples/
    â”‚   â”œâ”€â”€ agent_from_any_llm.py
    â”‚   â”œâ”€â”€ gradio_ui.py
    â”‚   â”œâ”€â”€ inspect_multiagent_run.py
    â”‚   â”œâ”€â”€ multi_llm_agent.py
    â”‚   â”œâ”€â”€ multiple_tools.py
    â”‚   â”œâ”€â”€ rag.py
    â”‚   â”œâ”€â”€ rag_using_chromadb.py
    â”‚   â”œâ”€â”€ sandboxed_execution.py
    â”‚   â”œâ”€â”€ structured_output_tool.py
    â”‚   â”œâ”€â”€ text_to_sql.py
    â”‚   â”œâ”€â”€ async_agent/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â””â”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ open_deep_research/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ analysis.ipynb
    â”‚   â”‚   â”œâ”€â”€ app.py
    â”‚   â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”‚   â”œâ”€â”€ run.py
    â”‚   â”‚   â”œâ”€â”€ run_gaia.py
    â”‚   â”‚   â”œâ”€â”€ visual_vs_text_browser.ipynb
    â”‚   â”‚   â””â”€â”€ scripts/
    â”‚   â”‚       â”œâ”€â”€ cookies.py
    â”‚   â”‚       â”œâ”€â”€ gaia_scorer.py
    â”‚   â”‚       â”œâ”€â”€ mdconvert.py
    â”‚   â”‚       â”œâ”€â”€ reformulator.py
    â”‚   â”‚       â”œâ”€â”€ run_agents.py
    â”‚   â”‚       â”œâ”€â”€ text_inspector_tool.py
    â”‚   â”‚       â”œâ”€â”€ text_web_browser.py
    â”‚   â”‚       â””â”€â”€ visual_qa.py
    â”‚   â”œâ”€â”€ plan_customization/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â””â”€â”€ plan_customization.py
    â”‚   â”œâ”€â”€ server/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â””â”€â”€ main.py
    â”‚   â””â”€â”€ smolagents_benchmark/
    â”‚       â”œâ”€â”€ run.py
    â”‚       â””â”€â”€ score.ipynb
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ smolagents/
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ _function_type_hints_utils.py
    â”‚       â”œâ”€â”€ agent_types.py
    â”‚       â”œâ”€â”€ cli.py
    â”‚       â”œâ”€â”€ default_tools.py
    â”‚       â”œâ”€â”€ gradio_ui.py
    â”‚       â”œâ”€â”€ mcp_client.py
    â”‚       â”œâ”€â”€ memory.py
    â”‚       â”œâ”€â”€ monitoring.py
    â”‚       â”œâ”€â”€ remote_executors.py
    â”‚       â”œâ”€â”€ tool_validation.py
    â”‚       â”œâ”€â”€ utils.py
    â”‚       â”œâ”€â”€ vision_web_browser.py
    â”‚       â””â”€â”€ prompts/
    â”‚           â”œâ”€â”€ code_agent.yaml
    â”‚           â”œâ”€â”€ structured_code_agent.yaml
    â”‚           â””â”€â”€ toolcalling_agent.yaml
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ conftest.py
    â”‚   â”œâ”€â”€ test_all_docs.py
    â”‚   â”œâ”€â”€ test_cli.py
    â”‚   â”œâ”€â”€ test_default_tools.py
    â”‚   â”œâ”€â”€ test_final_answer.py
    â”‚   â”œâ”€â”€ test_function_type_hints_utils.py
    â”‚   â”œâ”€â”€ test_gradio_ui.py
    â”‚   â”œâ”€â”€ test_import.py
    â”‚   â”œâ”€â”€ test_mcp_client.py
    â”‚   â”œâ”€â”€ test_memory.py
    â”‚   â”œâ”€â”€ test_models.py
    â”‚   â”œâ”€â”€ test_monitoring.py
    â”‚   â”œâ”€â”€ test_remote_executors.py
    â”‚   â”œâ”€â”€ test_search.py
    â”‚   â”œâ”€â”€ test_telemetry.py
    â”‚   â”œâ”€â”€ test_tool_validation.py
    â”‚   â”œâ”€â”€ test_tools.py
    â”‚   â”œâ”€â”€ test_types.py
    â”‚   â”œâ”€â”€ test_utils.py
    â”‚   â”œâ”€â”€ test_vision_web_browser.py
    â”‚   â”œâ”€â”€ fixtures/
    â”‚   â”‚   â”œâ”€â”€ agents.py
    â”‚   â”‚   â””â”€â”€ tools.py
    â”‚   â””â”€â”€ utils/
    â”‚       â””â”€â”€ markers.py
    â””â”€â”€ .github/
        â”œâ”€â”€ ISSUE_TEMPLATE/
        â”‚   â”œâ”€â”€ bug_report.md
        â”‚   â”œâ”€â”€ doc_improvement.md
        â”‚   â””â”€â”€ feature_request.md
        â””â”€â”€ workflows/
            â”œâ”€â”€ build_documentation.yml
            â”œâ”€â”€ build_pr_documentation.yml
            â”œâ”€â”€ quality.yml
            â”œâ”€â”€ tests.yml
            â”œâ”€â”€ trufflehog.yml
            â””â”€â”€ upload_pr_documentation.yml


Files Content:

================================================
FILE: README.md
================================================
<!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->
<p align="center">
    <!-- Uncomment when CircleCI is set up
    <a href="https://circleci.com/gh/huggingface/accelerate"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/master"></a>
    -->
    <a href="https://github.com/huggingface/smolagents/blob/main/LICENSE"><img alt="License" src="https://img.shields.io/github/license/huggingface/smolagents.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/smolagents"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/smolagents/index.html.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/smolagents/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/smolagents.svg"></a>
    <a href="https://github.com/huggingface/smolagents/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
</p>

<h3 align="center">
  <div style="display:flex;flex-direction:row;">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/smolagents.png" alt="Hugging Face mascot as James Bond" width=400px>
    <p>Agents that think in code!</p>
  </div>
</h3>

`smolagents` is a library that enables you to run powerful agents in a few lines of code. It offers:

âœ¨ **Simplicity**: the logic for agents fits in ~1,000 lines of code (see [agents.py](https://github.com/huggingface/smolagents/blob/main/src/smolagents/agents.py)). We kept abstractions to their minimal shape above raw code!

ðŸ§‘â€ðŸ’» **First-class support for Code Agents**. Our [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) writes its actions in code (as opposed to "agents being used to write code"). To make it secure, we support executing in sandboxed environments via [Blaxel](https://blaxel.ai), [E2B](https://e2b.dev/), [Modal](https://modal.com/), Docker, or Pyodide+Deno WebAssembly sandbox.

ðŸ¤— **Hub integrations**: you can [share/pull tools or agents to/from the Hub](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_hub) for instant sharing of the most efficient agents!

ðŸŒ **Model-agnostic**: smolagents supports any LLM. It can be a local `transformers` or `ollama` model, one of [many providers on the Hub](https://huggingface.co/blog/inference-providers), or any model from OpenAI, Anthropic and many others via our [LiteLLM](https://www.litellm.ai/) integration.

ðŸ‘ï¸ **Modality-agnostic**: Agents support text, vision, video, even audio inputs! Cf [this tutorial](https://huggingface.co/docs/smolagents/examples/web_browser) for vision.

ðŸ› ï¸ **Tool-agnostic**: you can use tools from any [MCP server](https://huggingface.co/docs/smolagents/reference/tools#smolagents.ToolCollection.from_mcp), from [LangChain](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_langchain), you can even use a [Hub Space](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_space) as a tool.

Full documentation can be found [here](https://huggingface.co/docs/smolagents/index).

> [!NOTE]
> Check the our [launch blog post](https://huggingface.co/blog/smolagents) to learn more about `smolagents`!

## Quick demo

First install the package with a default set of tools:
```bash
pip install "smolagents[toolkit]"
```
Then define your agent, give it the tools it needs and run it!
```py
from smolagents import CodeAgent, WebSearchTool, InferenceClientModel

model = InferenceClientModel()
agent = CodeAgent(tools=[WebSearchTool()], model=model, stream_outputs=True)

agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")
```

https://github.com/user-attachments/assets/84b149b4-246c-40c9-a48d-ba013b08e600

You can even share your agent to the Hub, as a Space repository:
```py
agent.push_to_hub("m-ric/my_agent")

# agent.from_hub("m-ric/my_agent") to load an agent from Hub
```

Our library is LLM-agnostic: you could switch the example above to any inference provider.

<details>
<summary> <b>InferenceClientModel, gateway for all <a href="https://huggingface.co/docs/inference-providers/index">inference providers</a> supported on HF</b></summary>

```py
from smolagents import InferenceClientModel

model = InferenceClientModel(
    model_id="deepseek-ai/DeepSeek-R1",
    provider="together",
)
```
</details>
<details>
<summary> <b>LiteLLM to access 100+ LLMs</b></summary>

```py
from smolagents import LiteLLMModel

model = LiteLLMModel(
    model_id="anthropic/claude-4-sonnet-latest",
    temperature=0.2,
    api_key=os.environ["ANTHROPIC_API_KEY"]
)
```
</details>
<details>
<summary> <b>OpenAI-compatible servers: Together AI</b></summary>

```py
import os
from smolagents import OpenAIModel

model = OpenAIModel(
    model_id="deepseek-ai/DeepSeek-R1",
    api_base="https://api.together.xyz/v1/", # Leave this blank to query OpenAI servers.
    api_key=os.environ["TOGETHER_API_KEY"], # Switch to the API key for the server you're targeting.
)
```
</details>
<details>
<summary> <b>OpenAI-compatible servers: OpenRouter</b></summary>

```py
import os
from smolagents import OpenAIModel

model = OpenAIModel(
    model_id="openai/gpt-4o",
    api_base="https://openrouter.ai/api/v1", # Leave this blank to query OpenAI servers.
    api_key=os.environ["OPENROUTER_API_KEY"], # Switch to the API key for the server you're targeting.
)
```

</details>
<details>
<summary> <b>Local `transformers` model</b></summary>

```py
from smolagents import TransformersModel

model = TransformersModel(
    model_id="Qwen/Qwen3-Next-80B-A3B-Thinking",
    max_new_tokens=4096,
    device_map="auto"
)
```
</details>
<details>
<summary> <b>Azure models</b></summary>

```py
import os
from smolagents import AzureOpenAIModel

model = AzureOpenAIModel(
    model_id = os.environ.get("AZURE_OPENAI_MODEL"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    api_version=os.environ.get("OPENAI_API_VERSION")    
)
```
</details>
<details>
<summary> <b>Amazon Bedrock models</b></summary>

```py
import os
from smolagents import AmazonBedrockModel

model = AmazonBedrockModel(
    model_id = os.environ.get("AMAZON_BEDROCK_MODEL_ID") 
)
```
</details>

## CLI

You can run agents from CLI using two commands: `smolagent` and `webagent`.

`smolagent` is a generalist command to run a multi-step `CodeAgent` that can be equipped with various tools.

```bash
# Run with direct prompt and options
smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "InferenceClientModel" --model-id "Qwen/Qwen3-Next-80B-A3B-Thinking" --imports pandas numpy --tools web_search

# Run in interactive mode (launches setup wizard when no prompt provided)
smolagent
```

Interactive mode guides you through:
- Agent type selection (CodeAgent vs ToolCallingAgent)  
- Tool selection from available toolbox
- Model configuration (type, ID, API settings)
- Advanced options like additional imports
- Task prompt input

Meanwhile `webagent`Â is a specific web-browsing agent using [helium](https://github.com/mherrmann/helium) (read more [here](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py)).

For instance:
```bash
webagent "go to xyz.com/men, get to sale section, click the first clothing item you see. Get the product details, and the price, return them. note that I'm shopping from France" --model-type "LiteLLMModel" --model-id "gpt-5"
```

## How do Code agents work?

Our [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) works mostly like classical ReAct agents - the exception being that the LLM engine writes its actions as Python code snippets.

```mermaid
flowchart TB
    Task[User Task]
    Memory[agent.memory]
    Generate[Generate from agent.model]
    Execute[Execute Code action - Tool calls are written as functions]
    Answer[Return the argument given to 'final_answer']

    Task -->|Add task to agent.memory| Memory

    subgraph ReAct[ReAct loop]
        Memory -->|Memory as chat messages| Generate
        Generate -->|Parse output to extract code action| Execute
        Execute -->|No call to 'final_answer' tool => Store execution logs in memory and keep running| Memory
    end
    
    Execute -->|Call to 'final_answer' tool| Answer

    %% Styling
    classDef default fill:#d4b702,stroke:#8b7701,color:#ffffff
    classDef io fill:#4a5568,stroke:#2d3748,color:#ffffff
    
    class Task,Answer io
```

Actions are now Python code snippets. Hence, tool calls will be performed as Python function calls. For instance, here is how the agent can perform web search over several websites in one single action:
```py
requests_to_search = ["gulf of mexico america", "greenland denmark", "tariffs"]
for request in requests_to_search:
    print(f"Here are the search results for {request}:", web_search(request))
```

Writing actions as code snippets is demonstrated to work better than the current industry practice of letting the LLM output a dictionary of the tools it wants to call: [uses 30% fewer steps](https://huggingface.co/papers/2402.01030) (thus 30% fewer LLM calls) and [reaches higher performance on difficult benchmarks](https://huggingface.co/papers/2411.01747). Head to [our high-level intro to agents](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents) to learn more on that.

Especially, since code execution can be a security concern (arbitrary code execution!), we provide options at runtime:
  - a secure python interpreter to run code more safely in your environment (more secure than raw code execution but still risky)
  - a sandboxed environment using [Blaxel](https://blaxel.ai), [E2B](https://e2b.dev/), or Docker (removes the risk to your own system).

Alongside [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent), we also provide the standard [`ToolCallingAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.ToolCallingAgent) which writes actions as JSON/text blobs. You can pick whichever style best suits your use case.

## How smol is this library?

We strived to keep abstractions to a strict minimum: the main code in `agents.py` has <1,000 lines of code.
Still, we implement several types of agents: `CodeAgent` writes its actions as Python code snippets, and the more classic `ToolCallingAgent` leverages built-in tool calling methods. We also have multi-agent hierarchies, import from tool collections, remote code execution, vision models...

By the way, why use a framework at all? Well, because a big part of this stuff is non-trivial. For instance, the code agent has to keep a consistent format for code throughout its system prompt, its parser, the execution. So our framework handles this complexity for you. But of course we still encourage you to hack into the source code and use only the bits that you need, to the exclusion of everything else!

## How strong are open models for agentic workflows?

We've created [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) instances with some leading models, and compared them on [this benchmark](https://huggingface.co/datasets/m-ric/agents_medium_benchmark_2) that gathers questions from a few different benchmarks to propose a varied blend of challenges.

[Find the benchmarking code here](https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/run.py) for more detail on the agentic setup used, and see a comparison of using LLMs code agents compared to vanilla (spoilers: code agents works better).

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/benchmark_code_agents.jpeg" alt="benchmark of different models on agentic workflows. Open model DeepSeek-R1 beats closed-source models." width=60% max-width=500px>
</p>

This comparison shows that open-source models can now take on the best closed models!

## Security

Security is a critical consideration when working with code-executing agents. Our library provides:
- Sandboxed execution options using [Blaxel](https://blaxel.ai), [E2B](https://e2b.dev/), [Modal](https://modal.com/), Docker, or Pyodide+Deno WebAssembly sandbox
- Best practices for running agent code securely

For security policies, vulnerability reporting, and more information on secure agent execution, please see our [Security Policy](SECURITY.md).

## Contribute

Everyone is welcome to contribute, get started with our [contribution guide](https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md).

## Cite smolagents

If you use `smolagents` in your publication, please cite it by using the following BibTeX entry.

```bibtex
@Misc{smolagents,
  title =        {`smolagents`: a smol library to build great agentic systems.},
  author =       {Aymeric Roucher and Albert Villanova del Moral and Thomas Wolf and Leandro von Werra and Erik KaunismÃ¤ki},
  howpublished = {\url{https://github.com/huggingface/smolagents}},
  year =         {2025}
}
```



================================================
FILE: AGENTS.md
================================================
# Contributor Guidelines
- Follow OOP principles
- Be Pythonic: follow Python best practices and idiomatic patterns
- Write unit tests for new functionality



================================================
FILE: CODE_OF_CONDUCT.md
================================================

# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
feedback@huggingface.co.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations


================================================
FILE: CONTRIBUTING.md
================================================
<!---
Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Contribute to smolagents

Everyone is welcome to contribute, and we value everybody's contribution. Code
contributions are not the only way to help the community. Answering questions, helping
others, and improving the documentation are also immensely valuable.

It also helps us if you spread the word! Reference the library in blog posts
about the awesome projects it made possible, shout out on Twitter every time it has
helped you, or simply â­ï¸ the repository to say thank you.

However you choose to contribute, please be mindful and respect our
[code of conduct](https://github.com/huggingface/smolagents/blob/main/CODE_OF_CONDUCT.md).

**This guide was heavily inspired by the awesome [scikit-learn guide to contributing](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md).**

## Ways to contribute

There are several ways you can contribute to smolagents.

* Submit issues related to bugs or desired new features.
* Contribute to the examples or to the documentation.
* Fix outstanding issues with the existing code.

> All contributions are equally valuable to the community. ðŸ¥°

## Submitting a bug-related issue or feature request

At any moment, feel welcome to open an issue, citing your exact error traces and package versions if it's a bug.
It's often even better to open a PR with your proposed fixes/changes!

Do your best to follow these guidelines when submitting a bug-related issue or a feature
request. It will make it easier for us to come back to you quickly and with good
feedback.

### Did you find a bug?

The smolagents library is robust and reliable thanks to users who report the problems they encounter.

Before you report an issue, we would really appreciate it if you could **make sure the bug was not
already reported** (use the search bar on GitHub under Issues). Your issue should also be related to bugs in the 
library itself, and not your code. 

Once you've confirmed the bug hasn't already been reported, please include the following information in your issue so 
we can quickly resolve it:

* Your **OS type and version**, as well as your environment versions (versions of rust, python, and dependencies).
* A short, self-contained, code snippet that allows us to reproduce the bug.
* The *full* traceback if an exception is raised.
* Attach any other additional information, like screenshots, you think may help.

### Do you want a new feature?

If there is a new feature you'd like to see in smolagents, please open an issue and describe:

1. What is the *motivation* behind this feature? Is it related to a problem or frustration with the library? Is it 
   a feature related to something you need for a project? Is it something you worked on and think it could benefit 
   the community?

   Whatever it is, we'd love to hear about it!

2. Describe your requested feature in as much detail as possible. The more you can tell us about it, the better 
   we'll be able to help you.
3. Provide a *code snippet* that demonstrates the feature's usage.
4. If the feature is related to a paper, please include a link.

If your issue is well written we're already 80% of the way there by the time you create it.

## Do you want to add documentation?

We're always looking for improvements to the documentation that make it more clear and accurate. Please let us know 
how the documentation can be improved such as typos and any content that is missing, unclear or inaccurate. We'll be 
happy to make the changes or help you make a contribution if you're interested!

## Fixing outstanding issues

If you notice an issue with the existing code and have a fix in mind, feel free to [start contributing](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request) and open
a Pull Request!

### Making code changes

To install dev dependencies, run:
<details>
<summary><strong>Using pip</strong></summary>

```
pip install -e ".[dev]"
```

</details>
<details>
<summary><strong>Using uv</strong></summary>

```
uv pip install -e "smolagents[dev] @ ."
```

</details>

When making changes to the codebase, please check that it follows the repo's code quality requirements by running:
To check code quality of the source code:
```
make quality
```

If the checks fail, you can run the formatter with:
```
make style
```

And commit the changes.

To run tests locally, run this command:
```bash
make test
```
</details>

## I want to become a maintainer of the project. How do I get there?

smolagents is a project led and managed by Hugging Face. We are more than
happy to have motivated individuals from other organizations join us as maintainers with the goal of helping smolagents
make a dent in the world of Agents.

If you are such an individual (or organization), please reach out to us and let's collaborate.



================================================
FILE: e2b.toml
================================================
# This is a config for E2B sandbox template.
# You can use template ID (qywp2ctmu2q7jzprcf4j) to create a sandbox:

# Python SDK
# from e2b import Sandbox, AsyncSandbox
# sandbox = Sandbox("qywp2ctmu2q7jzprcf4j") # Sync sandbox
# sandbox = await AsyncSandbox.create("qywp2ctmu2q7jzprcf4j") # Async sandbox

# JS SDK
# import { Sandbox } from 'e2b'
# const sandbox = await Sandbox.create('qywp2ctmu2q7jzprcf4j')

team_id = "f8776d3a-df2f-4a1d-af48-68c2e13b3b87"
start_cmd = "/root/.jupyter/start-up.sh"
dockerfile = "e2b.Dockerfile"
template_id = "qywp2ctmu2q7jzprcf4j"



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: Makefile
================================================
.PHONY: quality style test docs

check_dirs := examples src tests

# Check code quality of the source code
quality:
	ruff check $(check_dirs)
	ruff format --check $(check_dirs)

# Format source code automatically
style:
	ruff check $(check_dirs) --fix
	ruff format $(check_dirs)
	
# Run smolagents tests
test:
	pytest ./tests/


================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "smolagents"
version = "1.24.0.dev0"
description = "ðŸ¤— smolagents: a barebones library for agents. Agents write python code to call tools or orchestrate other agents."
authors = [
  { name="Aymeric Roucher", email="aymeric@hf.co" },
]
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "huggingface-hub>=0.31.2,<1.0.0",
  "requests>=2.32.3",
  "rich>=13.9.4",
  "jinja2>=3.1.4",
  "pillow>=10.0.1",
  # Security fix for CVE-2023-4863: https://pillow.readthedocs.io/en/stable/releasenotes/10.0.1.html
  "python-dotenv",
]

[project.optional-dependencies]
bedrock = [
  "boto3>=1.36.18"
]
blaxel = [
  "blaxel>=0.2.19",
  "websocket-client",
]
torch = [
  "torch",
  "torchvision",
  "numpy>=1.21.2",
]
audio = [
  "soundfile",
  "smolagents[torch]",
]
docker = [
  "docker>=7.1.0",
  "websocket-client",
]
e2b = [
  "e2b-code-interpreter>=1.0.3",
  "python-dotenv>=1.0.1",
]
gradio = [
  "gradio>=5.14.0",  # Sidebar component GH-797
]
litellm = [
  "litellm>=1.60.2",
]
mcp = [
  "mcpadapt>=0.1.13",  # Support structured output
  "mcp",
]
mlx-lm = [
  "mlx-lm",
]
modal = [
  "modal>=1.1.3",
  "websocket-client",
]
openai = [
  "openai>=1.58.1"
]
telemetry = [
  "arize-phoenix",
  "opentelemetry-sdk",
  "opentelemetry-exporter-otlp",
  "openinference-instrumentation-smolagents>=0.1.15"  # Use new TokenUsage structure
]
toolkit = [
  "ddgs>=9.0.0",  # DuckDuckGoSearchTool
  "markdownify>=0.14.1",  # VisitWebpageTool
]
transformers = [
  "accelerate",
  "transformers>=4.0.0",
  "smolagents[torch]",
]
vision = [
  "helium",
  "selenium",
]
vllm = [
  "vllm>=0.10.2",
  "torch"
]
all = [
  "smolagents[audio,blaxel,docker,e2b,gradio,litellm,mcp,mlx-lm,modal,openai,telemetry,toolkit,transformers,vision,bedrock]",
]
quality = [
  "ruff>=0.9.0",
]
test = [
  "ipython>=8.31.0", # for interactive environment tests
  "pandas>=2.2.3",
  "pytest>=8.1.0",
  "pytest-datadir",
  "pytest-timeout",  # For test_all_docs: @pytest.mark.timeout
  "python-dotenv>=1.0.1", # For test_all_docs
  "smolagents[all]",
  "rank-bm25", # For test_all_docs
  "Wikipedia-API>=0.8.1",
  "mlx[cpu]",  # GH-1588
]
dev = [
  "smolagents[quality,test]",
  "sqlalchemy", # for ./examples
]

[tool.pytest.ini_options]
# Add the specified `OPTS` to the set of command line arguments as if they had been specified by the user.
addopts = "-sv --durations=0"

[tool.ruff]
line-length = 119
lint.ignore = [
  "F403", # undefined-local-with-import-star
  "E501", # line-too-long
]
lint.select = ["E", "F", "I", "W"]

[tool.ruff.lint.per-file-ignores]
"examples/*" = [
  "E402", # module-import-not-at-top-of-file
]

[tool.ruff.lint.isort]
known-first-party = ["smolagents"]
lines-after-imports = 2

[tool.setuptools.package-data]
"smolagents.prompts" = ["*.yaml"]

[project.scripts]
smolagent = "smolagents.cli:main"
webagent = "smolagents.vision_web_browser:main"



================================================
FILE: SECURITY.md
================================================
# Security Policy

## Reporting a Vulnerability

To report a security vulnerability, please contact: security@huggingface.co

## Learning More About Security

To learn more about running agents more securely, please see the [Secure Code Execution tutorial](docs/source/en/tutorials/secure_code_execution.mdx) which covers sandboxing with E2B, Docker, and WebAssembly.

### Secure Execution Options

`smolagents` provides several options for secure code execution:

1. **E2B Sandbox**: Uses [E2B](https://e2b.dev/) to run code in a secure, isolated environment.

2. **Modal Sandbox**: Uses [Modal](https://modal.com/) to run code in a secure, isolated environment.

3. **Docker Sandbox**: Runs code in an isolated Docker container.

4. **WebAssembly Sandbox**: Executes Python code securely in a sandboxed WebAssembly environment using Pyodide and Deno's secure runtime.

We recommend using one of these sandboxed execution options when running untrusted code.



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.2.1
    hooks:
      - id: ruff
        args:
          - --fix
      - id: ruff-format
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: check-merge-conflict
      - id: check-yaml



================================================
FILE: docs/README.md
================================================
<!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Generating the documentation

To generate the documentation, you have to build it. Several packages are necessary to build the doc.

First, you need to install the project itself by running the following command at the root of the code repository:

```bash
pip install -e .
```

You also need to install 2 extra packages:

```bash
# `hf-doc-builder` to build the docs
pip install git+https://github.com/huggingface/doc-builder@main
```

---
**NOTE**

You only need to generate the documentation to inspect it locally (if you're planning changes and want to
check how they look before committing for instance). You don't have to commit the built documentation.

---

## Building the documentation

Once you have setup the `doc-builder` and additional packages with the pip install command above,
you can generate the documentation by typing the following command:

```bash
doc-builder build smolagents docs/source/en/ --build_dir ~/tmp/test-build
```

You can adapt the `--build_dir` to set any temporary folder that you prefer. This command will create it and generate
the MDX files that will be rendered as the documentation on the main website. You can inspect them in your favorite
Markdown editor.

## Previewing the documentation

To preview the docs, run the following command:

```bash
doc-builder preview smolagents docs/source/en/
```

The docs will be viewable at [http://localhost:5173](http://localhost:5173). You can also preview the docs once you
have opened a PR. You will see a bot add a comment to a link where the documentation with your changes lives.

---
**NOTE**

The `preview` command only works with existing doc files. When you add a completely new file, you need to update
`_toctree.yml` & restart `preview` command (`ctrl-c` to stop it & call `doc-builder preview ...` again).

---

## Adding a new element to the navigation bar

Accepted files are Markdown (.md).

Create a file with its extension and put it in the source directory. You can then link it to the toc-tree by putting
the filename without the extension in the [`_toctree.yml`](https://github.com/huggingface/smolagents/blob/main/docs/source/_toctree.yml) file.

## Renaming section headers and moving sections

It helps to keep the old links working when renaming the section header and/or moving sections from one document to another. This is because the old links are likely to be used in Issues, Forums, and Social media and it'd make for a much more superior user experience if users reading those months later could still easily navigate to the originally intended information.

Therefore, we simply keep a little map of moved sections at the end of the document where the original section was. The key is to preserve the original anchor.

So if you renamed a section from: "Section A" to "Section B", then you can add at the end of the file:

```
Sections that were moved:

[ <a href="#section-b">Section A</a><a id="section-a"></a> ]
```
and of course, if you moved it to another file, then:

```
Sections that were moved:

[ <a href="../new-file#section-b">Section A</a><a id="section-a"></a> ]
```

Use the relative style to link to the new file so that the versioned docs continue to work.

For an example of a rich moved section set please see the very end of [the transformers Trainer doc](https://github.com/huggingface/transformers/blob/main/docs/source/en/main_classes/trainer.md).


## Writing Documentation - Specification

The `huggingface/smolagents` documentation follows the
[Google documentation](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) style for docstrings,
although we can write them directly in Markdown.

### Adding a new tutorial

Adding a new tutorial or section is done in two steps:

- Add a new Markdown (.md) file under `./source`.
- Link that file in `./source/_toctree.yml` on the correct toc-tree.

Make sure to put your new file under the proper section. If you have a doubt, feel free to ask in a Github Issue or PR.

### Writing source documentation

Values that should be put in `code` should either be surrounded by backticks: \`like so\`. Note that argument names
and objects like True, None, or any strings should usually be put in `code`.

When mentioning a class, function, or method, it is recommended to use our syntax for internal links so that our tool
adds a link to its documentation with this syntax: \[\`XXXClass\`\] or \[\`function\`\]. This requires the class or
function to be in the main package.

If you want to create a link to some internal class or function, you need to
provide its path. For instance: \[\`utils.ModelOutput\`\]. This will be converted into a link with
`utils.ModelOutput` in the description. To get rid of the path and only keep the name of the object you are
linking to in the description, add a ~: \[\`~utils.ModelOutput\`\] will generate a link with `ModelOutput` in the description.

The same works for methods so you can either use \[\`XXXClass.method\`\] or \[~\`XXXClass.method\`\].

#### Defining arguments in a method

Arguments should be defined with the `Args:` (or `Arguments:` or `Parameters:`) prefix, followed by a line return and
an indentation. The argument should be followed by its type, with its shape if it is a tensor, a colon, and its
description:

```
    Args:
        n_layers (`int`): The number of layers of the model.
```

If the description is too long to fit in one line, another indentation is necessary before writing the description
after the argument.

Here's an example showcasing everything so far:

```
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary.

            Indices can be obtained using [`AlbertTokenizer`]. See [`~PreTrainedTokenizer.encode`] and
            [`~PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
```

For optional arguments or arguments with defaults we follow the following syntax: imagine we have a function with the
following signature:

```
def my_function(x: str = None, a: float = 1):
```

then its documentation should look like this:

```
    Args:
        x (`str`, *optional*):
            This argument controls ...
        a (`float`, *optional*, defaults to 1):
            This argument is used to ...
```

Note that we always omit the "defaults to \`None\`" when None is the default for any argument. Also note that even
if the first line describing your argument type and its default gets long, you can't break it on several lines. You can
however write as many lines as you want in the indented description (see the example above with `input_ids`).

#### Writing a multi-line code block

Multi-line code blocks can be useful for displaying examples. They are done between two lines of three backticks as usual in Markdown:


````
```
# first line of code
# second line
# etc
```
````

#### Writing a return block

The return block should be introduced with the `Returns:` prefix, followed by a line return and an indentation.
The first line should be the type of the return, followed by a line return. No need to indent further for the elements
building the return.

Here's an example of a single value return:

```
    Returns:
        `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.
```

Here's an example of a tuple return, comprising several objects:

```
    Returns:
        `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:
        - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --
          Total loss is the sum of the masked language modeling loss and the next sequence prediction (classification) loss.
        - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
          Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
```

#### Adding an image

Due to the rapidly growing repository, it is important to make sure that no files that would significantly weigh down the repository are added. This includes images, videos, and other non-text files. We prefer to leverage a hf.co hosted `dataset` like
the ones hosted on [`hf-internal-testing`](https://huggingface.co/hf-internal-testing) in which to place these files and reference
them by URL. We recommend putting them in the following dataset: [huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images).
If an external contribution, feel free to add the images to your PR and ask a Hugging Face member to migrate your images
to this dataset.

#### Writing documentation examples

The syntax for Example docstrings can look as follows:

```
    Example:

    ```python
    >>> from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
    >>> from datasets import load_dataset
    >>> import torch

    >>> dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
    >>> dataset = dataset.sort("id")
    >>> sampling_rate = dataset.features["audio"].sampling_rate

    >>> processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
    >>> model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

    >>> # audio file is decoded on the fly
    >>> inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
    >>> with torch.no_grad():
    ...     logits = model(**inputs).logits
    >>> predicted_ids = torch.argmax(logits, dim=-1)

    >>> # transcribe speech
    >>> transcription = processor.batch_decode(predicted_ids)
    >>> transcription[0]
    'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'
    ```
```

The docstring should give a minimal, clear example of how the respective model
is to be used in inference and also include the expected (ideally sensible)
output.
Often, readers will try out the example before even going through the function
or class definitions. Therefore, it is of utmost importance that the example
works as expected.




================================================
FILE: docs/source/en/_config.py
================================================
# docstyle-ignore
INSTALL_CONTENT = """
# Installation
! pip install smolagents
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/smolagents.git
"""

notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
black_avoid_patterns = {
    "{processor_class}": "FakeProcessorClass",
    "{model_class}": "FakeModelClass",
    "{object_class}": "FakeObjectClass",
}



================================================
FILE: docs/source/en/_toctree.yml
================================================
- title: Get started
  sections:
  - local: index
    title: Introduction
  - local: installation
    title: Installation options
  - local: guided_tour
    title: Guided tour
- title: Tutorials
  sections:
  - local: tutorials/building_good_agents
    title: âœ¨ Building good agents
  - local: tutorials/inspect_runs
    title: ðŸ“Š Inspect your agent runs using telemetry
  - local: tutorials/tools
    title: ðŸ› ï¸ Tools - in-depth guide
  - local: tutorials/secure_code_execution
    title: ðŸ›¡ï¸ Secure code execution
  - local: tutorials/memory
    title: ðŸ“š Manage your agent's memory
- title: Conceptual guides
  sections:
  - local: conceptual_guides/intro_agents
    title: ðŸ¤– What are agents?
  - local: conceptual_guides/react
    title: ðŸ¤” How do Multi-step agents work?
- title: Examples
  sections:
  - local: examples/text_to_sql
    title: Self-correcting Text-to-SQL
  - local: examples/rag
    title: Master your knowledge base with agentic RAG
  - local: examples/multiagents
    title: Orchestrate a multi-agent system
  - local: examples/web_browser
    title: Build a web browser agent using vision models
  - local: examples/using_different_models
    title: Using different models
  - local: examples/plan_customization
    title: "Human-in-the-Loop: Customize agent plan interactively"
  - local: examples/async_agent
    title: Async Applications with Agents
- title: Reference
  sections:
  - local: reference/agents
    title: Agent-related objects
  - local: reference/models
    title: Model-related objects
  - title: Tools
    sections:
    - title: Tool-related objects
      local: reference/tools
    - title: Built-in Tools
      local: reference/default_tools



================================================
FILE: docs/source/en/guided_tour.md
================================================
# Agents - Guided tour

[[open-in-colab]]

In this guided visit, you will learn how to build an agent, how to run it, and how to customize it to make it work better for your use-case.

## Choosing an agent type: CodeAgent or ToolCallingAgent

`smolagents` comes with two agent classes: [`CodeAgent`] and [`ToolCallingAgent`], which represent two different paradigms for how agents interact with tools.
The key difference lies in how actions are specified and executed: code generation vs structured tool calling.

- [`CodeAgent`] generates tool calls as Python code snippets.
  - The code is executed either locally (potentially unsecure) or in a secure sandbox.
  - Tools are exposed as Python functions (via bindings).
  - Example of tool call:
    ```py
    result = search_docs("What is the capital of France?")
    print(result)
    ```
  - Strengths:
    - Highly expressive: Allows for complex logic and control flow and can combine tools, loop, transform, reason.
    - Flexible: No need to predefine every possible action, can dynamically generate new actions/tools.
    - Emergent reasoning: Ideal for multi-step problems or dynamic logic.
  - Limitations
    - Risk of errors: Must handle syntax errors, exceptions.
    - Less predictable: More prone to unexpected or unsafe outputs.
    - Requires secure execution environment.

- [`ToolCallingAgent`] writes tool calls as structured JSON.
  - This is the common format used in many frameworks (OpenAI API), allowing for structured tool interactions without code execution.
  - Tools are defined with a JSON schema: name, description, parameter types, etc.
  - Example of tool call:
    ```json
    {
      "tool_call": {
        "name": "search_docs",
        "arguments": {
          "query": "What is the capital of France?"
        }
      }
    }
    ```
  - Strengths:
    - Reliable: Less prone to hallucination, outputs are structured and validated.
    - Safe: Arguments are strictly validated, no risk of arbitrary code running.
    - Interoperable: Easy to map to external APIs or services.
  - Limitations:
    - Low expressivity: Can't easily combine or transform results dynamically, or perform complex logic or control flow.
    - Inflexible: Must define all possible actions in advance, limited to predefined tools.
    - No code synthesis: Limited to tool capabilities.

When to use which agent type:
- Use [`CodeAgent`] when:
  - You need reasoning, chaining, or dynamic composition.
  - Tools are functions that can be combined (e.g., parsing + math + querying).
  - Your agent is a problem solver or programmer.

- Use [`ToolCallingAgent`] when:
  - You have simple, atomic tools (e.g., call an API, fetch a document).
  - You want high reliability and clear validation.
  - Your agent is like a dispatcher or controller.

## CodeAgent

[`CodeAgent`] generates Python code snippets to perform actions and solve tasks.

By default, the Python code execution is done in your local environment.
This should be safe because the only functions that can be called are the tools you provided (especially if it's only tools by Hugging Face) and a set of predefined safe functions like `print` or functions from the `math` module, so you're already limited in what can be executed.

The Python interpreter also doesn't allow imports by default outside of a safe list, so all the most obvious attacks shouldn't be an issue.
You can authorize additional imports by passing the authorized modules as a list of strings in argument `additional_authorized_imports` upon initialization of your [`CodeAgent`]:

```py
model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

Additionally, as an extra security layer, access to submodule is forbidden by default, unless explicitly authorized within the import list.
For instance, to access the `numpy.random` submodule, you need to add `'numpy.random'` to the `additional_authorized_imports` list.
This could also be authorized by using `numpy.*`, which will allow `numpy` as well as any subpackage like `numpy.random` and its own subpackages.

> [!WARNING]
> The LLM can generate arbitrary code that will then be executed: do not add any unsafe imports!

The execution will stop at any code trying to perform an illegal operation or if there is a regular Python error with the code generated by the agent.

You can also use [Blaxel](https://blaxel.ai), [E2B](https://e2b.dev/docs#what-is-e2-b), or Docker instead of a local Python interpreter. For Blaxel, first [set the `BL_API_KEY` and `BL_WORKSPACE` environment variables](https://app.blaxel.ai/profile/security) and then pass `executor_type="blaxel"` upon agent initialization. For E2B, first [set the `E2B_API_KEY` environment variable](https://e2b.dev/dashboard?tab=keys) and then pass `executor_type="e2b"`. For Docker, pass `executor_type="docker"`.


> [!TIP]
> Learn more about code execution [in this tutorial](tutorials/secure_code_execution).

### ToolCallingAgent

[`ToolCallingAgent`] outputs JSON tool calls, which is the common format used in many frameworks (OpenAI API), allowing for structured tool interactions without code execution. We utilize the built-in WebSearchTool (from the smolagents toolkit extra, which will be described in more detail later) to enable our agent to perform web searches.   

It works much in the same way like [`CodeAgent`], of course without `additional_authorized_imports` since it doesn't execute code:

```py
from smolagents import ToolCallingAgent, WebSearchTool

agent = ToolCallingAgent(tools=[WebSearchTool()], model=model)
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

## Using the CLI

You can quickly get started with smolagents using the command line interface:

```bash
# Run with direct prompt and options
smolagent "Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7."  --model-type "InferenceClientModel" --model-id "Qwen/Qwen2.5-Coder-32B-Instruct" --imports "pandas numpy" --tools "web_search"

# Run in interactive mode: launches when no prompt is provided, will guide you through argument selection
smolagent
```

## Building your agent

To initialize a minimal agent, you need at least these two arguments:

- `model`, a text-generation model to power your agent - because the agent is different from a simple LLM, it is a system that uses a LLM as its engine. You can use any of these options:
    - [`TransformersModel`] takes a pre-initialized `transformers` pipeline to run inference on your local machine using `transformers`.
    - [`InferenceClientModel`] leverages a `huggingface_hub.InferenceClient` under the hood and supports all Inference Providers on the Hub: Cerebras, Cohere, Fal, Fireworks, HF-Inference, Hyperbolic, Nebius, Novita, Replicate, SambaNova, Together, and more.
    - [`LiteLLMModel`] similarly lets you call 100+ different models and providers through [LiteLLM](https://docs.litellm.ai/)!
    - [`AzureOpenAIModel`] allows you to use OpenAI models deployed in [Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service).
    - [`AmazonBedrockModel`] allows you to use Amazon Bedrock in [AWS](https://aws.amazon.com/bedrock/?nc1=h_ls).
    - [`MLXModel`] creates a [mlx-lm](https://pypi.org/project/mlx-lm/) pipeline to run inference on your local machine.

- `tools`, a list of `Tools` that the agent can use to solve the task. It can be an empty list. You can also add the default toolbox on top of your `tools` list by defining the optional argument `add_base_tools=True`.

Once you have these two arguments, `tools` and `model`,  you can create an agent and run it. You can use any LLM you'd like, either through [Inference Providers](https://huggingface.co/blog/inference-providers), [transformers](https://github.com/huggingface/transformers/), [ollama](https://ollama.com/), [LiteLLM](https://www.litellm.ai/), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Amazon Bedrock](https://aws.amazon.com/bedrock/?nc1=h_ls), or [mlx-lm](https://pypi.org/project/mlx-lm/).

All model classes support passing additional keyword arguments (like `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time.
These parameters are automatically forwarded to the underlying model's completion calls, allowing you to configure model behavior such as creativity, response length, and sampling strategies.

<hfoptions id="Pick a LLM">
<hfoption id="Inference Providers">

Inference Providers need a `HF_TOKEN` to authenticate, but a free HF account already comes with included credits. Upgrade to PRO to raise your included credits.

To access gated models or rise your rate limits with a PRO account, you need to set the environment variable `HF_TOKEN` or pass `token` variable upon initialization of `InferenceClientModel`. You can get your token from your [settings page](https://huggingface.co/settings/tokens)

```python
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

model = InferenceClientModel(model_id=model_id, token="<YOUR_HUGGINGFACEHUB_API_TOKEN>") # You can choose to not pass any model_id to InferenceClientModel to use a default model
# you can also specify a particular provider e.g. provider="together" or provider="sambanova"
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Local Transformers Model">

```python
# !pip install 'smolagents[transformers]'
from smolagents import CodeAgent, TransformersModel

model_id = "meta-llama/Llama-3.2-3B-Instruct"

model = TransformersModel(model_id=model_id)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="OpenAI or Anthropic API">

To use `LiteLLMModel`, you need to set the environment variable `ANTHROPIC_API_KEY` or `OPENAI_API_KEY`, or pass `api_key` variable upon initialization.

```python
# !pip install 'smolagents[litellm]'
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(model_id="anthropic/claude-3-5-sonnet-latest", api_key="YOUR_ANTHROPIC_API_KEY") # Could use 'gpt-4o'
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Ollama">

```python
# !pip install 'smolagents[litellm]'
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(
    model_id="ollama_chat/llama3.2", # This model is a bit weak for agentic behaviours though
    api_base="http://localhost:11434", # replace with 127.0.0.1:11434 or remote open-ai compatible server if necessary
    api_key="YOUR_API_KEY", # replace with API key if necessary
    num_ctx=8192, # ollama default is 2048 which will fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Azure OpenAI">

To connect to Azure OpenAI, you can either use `AzureOpenAIModel` directly, or use `LiteLLMModel` and configure it accordingly.

To initialize an instance of `AzureOpenAIModel`, you need to pass your model deployment name and then either pass the `azure_endpoint`, `api_key`, and `api_version` arguments, or set the environment variables `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION`.

```python
# !pip install 'smolagents[openai]'
from smolagents import CodeAgent, AzureOpenAIModel

model = AzureOpenAIModel(model_id="gpt-4o-mini")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

Similarly, you can configure `LiteLLMModel` to connect to Azure OpenAI as follows:

- pass your model deployment name as `model_id`, and make sure to prefix it with `azure/`
- make sure to set the environment variable `AZURE_API_VERSION`
- either pass the `api_base` and `api_key` arguments, or set the environment variables `AZURE_API_KEY`, and `AZURE_API_BASE`

```python
import os
from smolagents import CodeAgent, LiteLLMModel

AZURE_OPENAI_CHAT_DEPLOYMENT_NAME="gpt-35-turbo-16k-deployment" # example of deployment name

os.environ["AZURE_API_KEY"] = "" # api_key
os.environ["AZURE_API_BASE"] = "" # "https://example-endpoint.openai.azure.com"
os.environ["AZURE_API_VERSION"] = "" # "2024-10-01-preview"

model = LiteLLMModel(model_id="azure/" + AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
   "Could you give me the 118th number in the Fibonacci sequence?",
)
```

</hfoption>
<hfoption id="Amazon Bedrock">

The `AmazonBedrockModel` class provides native integration with Amazon Bedrock, allowing for direct API calls and comprehensive configuration.

Basic Usage:

```python
# !pip install 'smolagents[bedrock]'
from smolagents import CodeAgent, AmazonBedrockModel

model = AmazonBedrockModel(model_id="anthropic.claude-3-sonnet-20240229-v1:0")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

Advanced Configuration:

```python
import boto3
from smolagents import AmazonBedrockModel

# Create a custom Bedrock client
bedrock_client = boto3.client(
    'bedrock-runtime',
    region_name='us-east-1',
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY'
)

additional_api_config = {
    "inferenceConfig": {
        "maxTokens": 3000
    },
    "guardrailConfig": {
        "guardrailIdentifier": "identify1",
        "guardrailVersion": 'v1'
    },
}

# Initialize with comprehensive configuration
model = AmazonBedrockModel(
    model_id="us.amazon.nova-pro-v1:0",
    client=bedrock_client,  # Use custom client
    **additional_api_config
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

Using LiteLLMModel:

Alternatively, you can use `LiteLLMModel` with Bedrock models:

```python
from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(model_name="bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
agent = CodeAgent(tools=[], model=model)

agent.run("Explain the concept of quantum computing")
```

</hfoption>
<hfoption id="mlx-lm">

```python
# !pip install 'smolagents[mlx-lm]'
from smolagents import CodeAgent, MLXModel

mlx_model = MLXModel("mlx-community/Qwen2.5-Coder-32B-Instruct-4bit")
agent = CodeAgent(model=mlx_model, tools=[], add_base_tools=True)

agent.run("Could you give me the 118th number in the Fibonacci sequence?")
```

</hfoption>
</hfoptions>

### Model parameter management

When initializing models, you can pass keyword arguments that will be forwarded as completion parameters to the
underlying model API during inference.

For fine-grained control over parameter handling, the `REMOVE_PARAMETER` sentinel value allows you to explicitly exclude
parameters that might otherwise be set by default or passed through elsewhere:

```python
from smolagents import OpenAIModel, REMOVE_PARAMETER

# Remove "stop" parameter
model = OpenAIModel(
    model_id="gpt-5",
    stop=REMOVE_PARAMETER,  # Ensures "stop" is not included in API calls
    temperature=0.7
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)
```

This is particularly useful when:
- You want to override default parameters that might be applied automatically
- You need to ensure certain parameters are completely excluded from API calls
- You want to let the model provider use their own defaults for specific parameters

## Advanced agent configuration

### Customizing agent termination conditions

By default, an agent continues running until it calls the `final_answer` function or reaches the maximum number of steps.
The `final_answer_checks` parameter gives you more control over when and how an agent terminates its execution:

```python
from smolagents import CodeAgent, InferenceClientModel

# Define a custom final answer check function
def is_integer(final_answer: str, agent_memory=None) -> bool:
    """Return True if final_answer is an integer."""
    try:
        int(final_answer)
        return True
    except ValueError:
        return False

# Initialize agent with custom final answer check
agent = CodeAgent(
    tools=[],
    model=InferenceClientModel(),
    final_answer_checks=[is_integer]
)

agent.run("Calculate the least common multiple of 3 and 7")
```

The `final_answer_checks` parameter accepts a list of functions that each:
- Take the agent's final_answer and the agent itself as parameters
- Return a boolean indicating whether the final_answer is valid (True) or not (False)

If any function returns `False`, the agent will log the error message and continue the run.
This validation mechanism enables:
- Enforcing output format requirements (e.g., ensuring numeric answers for math problems)
- Implementing domain-specific validation rules
- Creating more robust agents that validate their own outputs

## Inspecting an agent run

Here are a few useful attributes to inspect what happened after a run:
- `agent.logs` stores the fine-grained logs of the agent. At every step of the agent's run, everything gets stored in a dictionary that then is appended to `agent.logs`.
- Running `agent.write_memory_to_messages()` writes the agent's memory as list of chat messages for the Model to view. This method goes over each step of the log and only stores what it's interested in as a message: for instance, it will save the system prompt and task in separate messages, then for each step it will store the LLM output as a message, and the tool call output as another message. Use this if you want a higher-level view of what has happened - but not every log will be transcripted by this method.

## Tools

A tool is an atomic function to be used by an agent. To be used by an LLM, it also needs a few attributes that constitute its API and will be used to describe to the LLM how to call this tool:
- A name
- A description
- Input types and descriptions
- An output type

You can for instance check the [`PythonInterpreterTool`]: it has a name, a description, input descriptions, an output type, and a `forward` method to perform the action.

When the agent is initialized, the tool attributes are used to generate a tool description which is baked into the agent's system prompt. This lets the agent know which tools it can use and why.

**Schema Information**: For tools that have an `output_schema` defined (such as MCP tools with structured output), the `CodeAgent` system prompt automatically includes the JSON schema information. This helps the agent understand the expected structure of tool outputs and access the data appropriately.

### Default toolbox

If you install `smolagents` with the "toolkit" extra, it comes with a default toolbox for empowering agents, that you can add to your agent upon initialization with argument `add_base_tools=True`:

- **DuckDuckGo web search***: performs a web search using DuckDuckGo browser.
- **Python code interpreter**: runs your LLM generated Python code in a secure environment. This tool will only be added to [`ToolCallingAgent`] if you initialize it with `add_base_tools=True`, since code-based agent can already natively execute Python code
- **Transcriber**: a speech-to-text pipeline built on Whisper-Turbo that transcribes an audio to text.

You can manually use a tool by calling it with its arguments.

```python
# !pip install 'smolagents[toolkit]'
from smolagents import WebSearchTool

search_tool = WebSearchTool()
print(search_tool("Who's the current president of Russia?"))
```

### Create a new tool

You can create your own tool for use cases not covered by the default tools from Hugging Face.
For example, let's create a tool that returns the most downloaded model for a given task from the Hub.

You'll start with the code below.

```python
from huggingface_hub import list_models

task = "text-classification"

most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
print(most_downloaded_model.id)
```

This code can quickly be converted into a tool, just by wrapping it in a function and adding the `tool` decorator:
This is not the only way to build the tool: you can directly define it as a subclass of [`Tool`], which gives you more flexibility, for instance the possibility to initialize heavy class attributes.

Let's see how it works for both options:

<hfoptions id="build-a-tool">
<hfoption id="Decorate a function with @tool">

```py
from smolagents import tool

@tool
def model_download_tool(task: str) -> str:
    """
    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
    It returns the name of the checkpoint.

    Args:
        task: The task for which to get the download count.
    """
    most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
    return most_downloaded_model.id
```

The function needs:
- A clear name. The name should be descriptive enough of what this tool does to help the LLM brain powering the agent. Since this tool returns the model with the most downloads for a task, let's name it `model_download_tool`.
- Type hints on both inputs and output
- A description, that includes an 'Args:' part where each argument is described (without a type indication this time, it will be pulled from the type hint). Same as for the tool name, this description is an instruction manual for the LLM powering your agent, so do not neglect it.

All these elements will be automatically baked into the agent's system prompt upon initialization: so strive to make them as clear as possible!

> [!TIP]
> This definition format is the same as tool schemas used in `apply_chat_template`, the only difference is the added `tool` decorator: read more on our tool use API [here](https://huggingface.co/blog/unified-tool-use#passing-tools-to-a-chat-template).


Then you can directly initialize your agent:
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
</hfoption>
<hfoption id="Subclass Tool">

```py
from smolagents import Tool

class ModelDownloadTool(Tool):
    name = "model_download_tool"
    description = "This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It returns the name of the checkpoint."
    inputs = {"task": {"type": "string", "description": "The task for which to get the download count."}}
    output_type = "string"

    def forward(self, task: str) -> str:
        most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
        return most_downloaded_model.id
```

The subclass needs the following attributes:
- A clear `name`. The name should be descriptive enough of what this tool does to help the LLM brain powering the agent. Since this tool returns the model with the most downloads for a task, let's name it `model_download_tool`.
- A `description`. Same as for the `name`, this description is an instruction manual for the LLM powering your agent, so do not neglect it.
- Input types and descriptions
- Output type
All these attributes will be automatically baked into the agent's system prompt upon initialization: so strive to make them as clear as possible!


Then you can directly initialize your agent:
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[ModelDownloadTool()], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
</hfoption>
</hfoptions>

You get the following logs:
```text
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                          â”‚
â”‚ Can you give me the name of the model that has the most downloads in the 'text-to-video' â”‚
â”‚ task on the Hugging Face Hub?                                                            â”‚
â”‚                                                                                          â”‚
â•°â”€ InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 model_name = model_download_tool(task="text-to-video")                               â”‚
â”‚   2 print(model_name)                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Execution logs:
ByteDance/AnimateDiff-Lightning

Out: None
[Step 0: Duration 0.27 seconds| Input tokens: 2,069 | Output tokens: 60]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 final_answer("ByteDance/AnimateDiff-Lightning")                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Out - Final answer: ByteDance/AnimateDiff-Lightning
[Step 1: Duration 0.10 seconds| Input tokens: 4,288 | Output tokens: 148]
Out[20]: 'ByteDance/AnimateDiff-Lightning'
```

> [!TIP]
> Read more on tools in the [dedicated tutorial](./tutorials/tools#what-is-a-tool-and-how-to-build-one).

## Multi-agents

Multi-agent systems have been introduced with Microsoft's framework [Autogen](https://huggingface.co/papers/2308.08155).

In this type of framework, you have several agents working together to solve your task instead of only one.
It empirically yields better performance on most benchmarks. The reason for this better performance is conceptually simple: for many tasks, rather than using a do-it-all system, you would prefer to specialize units on sub-tasks. Here, having agents with separate tool sets and memories allows to achieve efficient specialization. For instance, why fill the memory of the code generating agent with all the content of webpages visited by the web search agent? It's better to keep them separate.

You can easily build hierarchical multi-agent systems with `smolagents`.

To do so, just ensure your agent has `name` and`description` attributes, which will then be embedded in the manager agent's system prompt to let it know how to call this managed agent, as we also do for tools.
Then you can pass this managed agent in the parameter managed_agents upon initialization of the manager agent.

Here's an example of making an agent that managed a specific web search agent using our native [`WebSearchTool`]:

```py
from smolagents import CodeAgent, InferenceClientModel, WebSearchTool

model = InferenceClientModel()

web_agent = CodeAgent(
    tools=[WebSearchTool()],
    model=model,
    name="web_search_agent",
    description="Runs web searches for you. Give it your query as an argument."
)

manager_agent = CodeAgent(
    tools=[], model=model, managed_agents=[web_agent]
)

manager_agent.run("Who is the CEO of Hugging Face?")
```

> [!TIP]
> For an in-depth example of an efficient multi-agent implementation, see [how we pushed our multi-agent system to the top of the GAIA leaderboard](https://huggingface.co/blog/beating-gaia).


## Talk with your agent and visualize its thoughts in a cool Gradio interface

You can use `GradioUI` to interactively submit tasks to your agent and observe its thought and execution process, here is an example:

```py
from smolagents import (
    load_tool,
    CodeAgent,
    InferenceClientModel,
    GradioUI
)

# Import tool from Hub
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

model = InferenceClientModel(model_id=model_id)

# Initialize the agent with the image generation tool
agent = CodeAgent(tools=[image_generation_tool], model=model)

GradioUI(agent).launch()
```

Under the hood, when the user types a new answer, the agent is launched with `agent.run(user_request, reset=False)`.
The `reset=False` flag means the agent's memory is not flushed before launching this new task, which lets the conversation go on.

You can also use this `reset=False` argument to keep the conversation going in any other agentic application.

In gradio UIs, if you want to allow users to interrupt a running agent, you could do this with a button that triggers method `agent.interrupt()`.
This will stop the agent at the end of its current step, then raise an error.

## Next steps

Finally, when you've configured your agent to your needs, you can share it to the Hub!

```py
agent.push_to_hub("m-ric/my_agent")
```

Similarly, to load an agent that has been pushed to hub, if you trust the code from its tools, use:
```py
agent.from_hub("m-ric/my_agent", trust_remote_code=True)
```

For more in-depth usage, you will then want to check out our tutorials:
- [the explanation of how our code agents work](./tutorials/secure_code_execution)
- [this guide on how to build good agents](./tutorials/building_good_agents).
- [the in-depth guide for tool usage](./tutorials/building_good_agents).



================================================
FILE: docs/source/en/index.md
================================================
# `smolagents`

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/license_to_call.png" style="max-width:700px"/>
</div>

## What is smolagents?

`smolagents` is an open-source Python library designed to make it extremely easy to build and run agents using just a few lines of code.

Key features of `smolagents` include:

âœ¨ **Simplicity**: The logic for agents fits in ~thousand lines of code. We kept abstractions to their minimal shape above raw code!

ðŸ§‘â€ðŸ’» **First-class support for Code Agents**: [`CodeAgent`](reference/agents#smolagents.CodeAgent) writes its actions in code (as opposed to "agents being used to write code") to invoke tools or perform computations, enabling natural composability (function nesting, loops, conditionals). To make it secure, we support [executing in sandboxed environment](tutorials/secure_code_execution) via [Modal](https://modal.com/), [Blaxel](https://blaxel.ai), [E2B](https://e2b.dev/), or Docker.

ðŸ“¡ **Common Tool-Calling Agent Support**: In addition to CodeAgents, [`ToolCallingAgent`](reference/agents#smolagents.ToolCallingAgent) supports usual JSON/text-based tool-calling for scenarios where that paradigm is preferred.

ðŸ¤— **Hub integrations**: Seamlessly share and load agents and tools to/from the Hub as Gradio Spaces.

ðŸŒ **Model-agnostic**: Easily integrate any large language model (LLM), whether it's hosted on the Hub via [Inference providers](https://huggingface.co/docs/inference-providers/index), accessed via APIs such as OpenAI, Anthropic, or many others via LiteLLM integration, or run locally using Transformers or Ollama. Powering an agent with your preferred LLM is straightforward and flexible.

ðŸ‘ï¸ **Modality-agnostic**: Beyond text, agents can handle vision, video, and audio inputs, broadening the range of possible applications. Check out [this tutorial](examples/web_browser) for vision.

ðŸ› ï¸ **Tool-agnostic**: You can use tools from any [MCP server](reference/tools#smolagents.ToolCollection.from_mcp), from [LangChain](reference/tools#smolagents.Tool.from_langchain), you can even use a [Hub Space](reference/tools#smolagents.Tool.from_space) as a tool.

ðŸ’» **CLI Tools**: Comes with command-line utilities (smolagent, webagent) for quickly running agents without writing boilerplate code.

## Quickstart

[[open-in-colab]]

Get started with smolagents in just a few minutes! This guide will show you how to create and run your first agent.

### Installation

Install smolagents with pip:

```bash
pip install 'smolagents[toolkit]'  # Includes default tools like web search
```

### Create Your First Agent

Here's a minimal example to create and run an agent:

```python
from smolagents import CodeAgent, InferenceClientModel

# Initialize a model (using Hugging Face Inference API)
model = InferenceClientModel()  # Uses a default model

# Create an agent with no tools
agent = CodeAgent(tools=[], model=model)

# Run the agent with a task
result = agent.run("Calculate the sum of numbers from 1 to 10")
print(result)
```

That's it! Your agent will use Python code to solve the task and return the result.

### Adding Tools

Let's make our agent more capable by adding some tools:

```python
from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool

model = InferenceClientModel()
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
)

# Now the agent can search the web!
result = agent.run("What is the current weather in Paris?")
print(result)
```

### Using Different Models

You can use various models with your agent:

```python
# Using a specific model from Hugging Face
model = InferenceClientModel(model_id="meta-llama/Llama-2-70b-chat-hf")

# Using OpenAI/Anthropic (requires 'smolagents[litellm]')
from smolagents import LiteLLMModel
model = LiteLLMModel(model_id="gpt-4")

# Using local models (requires 'smolagents[transformers]')
from smolagents import TransformersModel
model = TransformersModel(model_id="meta-llama/Llama-2-7b-chat-hf")
```

## Next Steps

- Learn how to set up smolagents with various models and tools in the [Installation Guide](installation)
- Check out the [Guided Tour](guided_tour) for more advanced features
- Learn about [building custom tools](tutorials/tools)
- Explore [secure code execution](tutorials/secure_code_execution)
- See how to create [multi-agent systems](tutorials/building_good_agents)

<div class="mt-10">
  <div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5">
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./guided_tour"
      ><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Guided tour</div>
      <p class="text-gray-700">Learn the basics and become familiar with using Agents. Start here if you are using Agents for the first time!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./examples/text_to_sql"
      ><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">How-to guides</div>
      <p class="text-gray-700">Practical guides to help you achieve a specific goal: create an agent to generate and test SQL queries!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual_guides/intro_agents"
      ><div class="w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Conceptual guides</div>
      <p class="text-gray-700">High-level explanations for building a better understanding of important topics.</p>
   </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/building_good_agents"
      ><div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Tutorials</div>
      <p class="text-gray-700">Horizontal tutorials that cover important aspects of building agents.</p>
    </a>
  </div>
</div>



================================================
FILE: docs/source/en/installation.md
================================================
# Installation Options

The `smolagents` library can be installed using pip. Here are the different installation methods and options available.

## Prerequisites
- Python 3.10 or newer
- Python package manager: [`pip`](https://pip.pypa.io/en/stable/) or [`uv`](https://docs.astral.sh/uv/)

## Virtual Environment

It's strongly recommended to install `smolagents` within a Python virtual environment.
Virtual environments isolate your project dependencies from other Python projects and your system Python installation,
preventing version conflicts and making package management more reliable.

<hfoptions id="virtual-environment">
<hfoption id="venv">

Using [`venv`](https://docs.python.org/3/library/venv.html):

```bash
python -m venv .venv
source .venv/bin/activate
```

</hfoption>
<hfoption id="uv">

Using [`uv`](https://docs.astral.sh/uv/):

```bash
uv venv .venv
source .venv/bin/activate
```

</hfoption>
</hfoptions>

## Basic Installation

Install `smolagents` core library with:

<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install smolagents
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install smolagents
```
</hfoption>
</hfoptions>

## Installation with Extras

`smolagents` provides several optional dependencies (extras) that can be installed based on your needs.
You can install these extras using the following syntax:
<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[extra1,extra2]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[extra1,extra2]"
```
</hfoption>
</hfoptions>

### Tools
These extras include various tools and integrations:
<hfoptions id="installation">
<hfoption id="pip">
- **toolkit**: Install a default set of tools for common tasks.
  ```bash
  pip install "smolagents[toolkit]"
  ```
- **mcp**: Add support for the Model Context Protocol (MCP) to integrate with external tools and services.
  ```bash
  pip install "smolagents[mcp]"
  ```
</hfoption>
<hfoption id="uv">
- **toolkit**: Install a default set of tools for common tasks.
  ```bash
  uv pip install "smolagents[toolkit]"
  ```
- **mcp**: Add support for the Model Context Protocol (MCP) to integrate with external tools and services.
  ```bash
  uv pip install "smolagents[mcp]"
  ```
</hfoption>
</hfoptions>

### Model Integration
These extras enable integration with various AI models and frameworks:
<hfoptions id="installation">
<hfoption id="pip">
- **openai**: Add support for OpenAI API models.
  ```bash
  pip install "smolagents[openai]"
  ```
- **transformers**: Enable Hugging Face Transformers models.
  ```bash
  pip install "smolagents[transformers]"
  ```
- **vllm**: Add VLLM support for efficient model inference.
  ```bash
  pip install "smolagents[vllm]"
  ```
- **mlx-lm**: Enable support for MLX-LM models.
  ```bash
  pip install "smolagents[mlx-lm]"
  ```
- **litellm**: Add LiteLLM support for lightweight model inference.
  ```bash
  pip install "smolagents[litellm]"
  ```
- **bedrock**: Enable support for AWS Bedrock models.
  ```bash
  pip install "smolagents[bedrock]"
  ```
</hfoption>
<hfoption id="uv">
- **openai**: Add support for OpenAI API models.
  ```bash
  uv pip install "smolagents[openai]"
  ```
- **transformers**: Enable Hugging Face Transformers models.
  ```bash
  uv pip install "smolagents[transformers]"
  ```
- **vllm**: Add VLLM support for efficient model inference.
  ```bash
  uv pip install "smolagents[vllm]"
  ```
- **mlx-lm**: Enable support for MLX-LM models.
  ```bash
  uv pip install "smolagents[mlx-lm]"
  ```
- **litellm**: Add LiteLLM support for lightweight model inference.
  ```bash
  uv pip install "smolagents[litellm]"
  ```
- **bedrock**: Enable support for AWS Bedrock models.
  ```bash
  uv pip install "smolagents[bedrock]"
  ```
</hfoption>
</hfoptions>

### Multimodal Capabilities
Extras for handling different types of media and input:
<hfoptions id="installation">
<hfoption id="pip">
- **vision**: Add support for image processing and computer vision tasks.
  ```bash
  pip install "smolagents[vision]"
  ```
- **audio**: Enable audio processing capabilities.
  ```bash
  pip install "smolagents[audio]"
  ```
</hfoption>
<hfoption id="uv">
- **vision**: Add support for image processing and computer vision tasks.
  ```bash
  uv pip install "smolagents[vision]"
  ```
- **audio**: Enable audio processing capabilities.
  ```bash
  uv pip install "smolagents[audio]"
  ```
</hfoption>
</hfoptions>

### Remote Execution
Extras for executing code remotely:
<hfoptions id="installation">
<hfoption id="pip">
- **blaxel**: Add support for Blaxel sandboxes - fast-launching VMs with hibernation (recommended).
  ```bash
  pip install "smolagents[blaxel]"
  ```
- **e2b**: Enable E2B support for remote execution.
  ```bash
  pip install "smolagents[e2b]"
  ```
- **docker**: Add support for executing code in Docker containers.
  ```bash
  pip install "smolagents[docker]"
  ```
</hfoption>
<hfoption id="uv">
- **blaxel**: Add support for Blaxel sandboxes - fast-launching VMs with hibernation (recommended).
  ```bash
  uv pip install "smolagents[blaxel]"
  ```
- **e2b**: Enable E2B support for remote execution.
  ```bash
  uv pip install "smolagents[e2b]"
  ```
- **docker**: Add support for executing code in Docker containers.
  ```bash
  uv pip install "smolagents[docker]"
  ```
</hfoption>
</hfoptions>

### Telemetry and User Interface
Extras for telemetry, monitoring and user interface components:
<hfoptions id="installation">
<hfoption id="pip">
- **telemetry**: Add support for monitoring and tracing.
  ```bash
  pip install "smolagents[telemetry]"
  ```
- **gradio**: Add support for interactive Gradio UI components.
  ```bash
  pip install "smolagents[gradio]"
  ```
</hfoption>
<hfoption id="uv">
- **telemetry**: Add support for monitoring and tracing.
  ```bash
  uv pip install "smolagents[telemetry]"
  ```
- **gradio**: Add support for interactive Gradio UI components.
  ```bash
  uv pip install "smolagents[gradio]"
  ```
</hfoption>
</hfoptions>

### Complete Installation
To install all available extras, you can use:
<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[all]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[all]"
```
</hfoption>
</hfoptions>

## Verifying Installation
After installation, you can verify that `smolagents` is installed correctly by running:
```python
import smolagents
print(smolagents.__version__)
```

## Next Steps
Once you have successfully installed `smolagents`, you can:
- Follow the [guided tour](./guided_tour) to learn the basics.
- Explore the [how-to guides](./examples/text_to_sql) for practical examples.
- Read the [conceptual guides](./conceptual_guides/intro_agents) for high-level explanations.
- Check out the [tutorials](./tutorials/building_good_agents) for in-depth tutorials on building agents.
- Explore the [API reference](./reference/index) for detailed information on classes and functions.



================================================
FILE: docs/source/en/conceptual_guides/intro_agents.md
================================================
# What are agents? ðŸ¤”

## An introduction to agentic systems.

Any efficient system using AI will need to provide LLMs some kind of access to the real world: for instance the possibility to call a search tool to get external information, or to act on certain programs in order to solve a task. In other words, LLMs should have ***agency***. Agentic programs are the gateway to the outside world for LLMs.

> [!TIP]
> AI Agents are **programs where LLM outputs control the workflow**.

Any system leveraging LLMs will integrate the LLM outputs into code. The influence of the LLM's input on the code workflow is the level of agency of LLMs in the system.

Note that with this definition, "agent" is not a discrete, 0 or 1 definition: instead, "agency" evolves on a continuous spectrum, as you give more or less power to the LLM on your workflow.

See in the table below how agency can vary across systems:

| Agency Level | Description                                                     | Short name       | Example Code                                       |
| ------------ | --------------------------------------------------------------- | ---------------- | -------------------------------------------------- |
| â˜†â˜†â˜†          | LLM output has no impact on program flow                        | Simple processor | `process_llm_output(llm_response)`                 |
| â˜…â˜†â˜†          | LLM output controls an if/else switch                           | Router           | `if llm_decision(): path_a() else: path_b()`       |
| â˜…â˜…â˜†          | LLM output controls function execution                          | Tool call        | `run_function(llm_chosen_tool, llm_chosen_args)`   |
| â˜…â˜…â˜†          | LLM output controls iteration and program continuation          | Multi-step Agent | `while llm_should_continue(): execute_next_step()` |
| â˜…â˜…â˜…          | One agentic workflow can start another agentic workflow         | Multi-Agent      | `if llm_trigger(): execute_agent()`                |
| â˜…â˜…â˜…          | LLM acts in code, can define its own tools / start other agents | Code Agents      | `def custom_tool(args): ...`                       |

The multi-step agent has this code structure:

```python
memory = [user_defined_task]
while llm_should_continue(memory): # this loop is the multi-step part
    action = llm_get_next_action(memory) # this is the tool-calling part
    observations = execute_action(action)
    memory += [action, observations]
```

This agentic system runs in a loop, executing a new action at each step (the action can involve calling some pre-determined *tools* that are just functions), until its observations make it apparent that a satisfactory state has been reached to solve the given task. Hereâ€™s an example of how a multi-step agent can solve a simple math question:

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"/>
</div>


## âœ…Â When to use agents / â›”Â when to avoid them

Agents are useful when you need an LLM to determine the workflow of an app. But theyâ€™re often overkill. The question is: do I really need flexibility in the workflow to efficiently solve the task at hand?
If the pre-determined workflow falls short too often, that means you need more flexibility.
Let's take an example: say you're making an app that handles customer requests on a surfing trip website.

You could know in advance that the requests will belong to either of 2 buckets (based on user choice), and you have a predefined workflow for each of these 2 cases.

1. Want some knowledge on the trips? â‡’ give them access to a search bar to search your knowledge base
2. Wants to talk to sales? â‡’ let them type in a contact form.

If that deterministic workflow fits all queries, by all means just code everything! This will give you a 100% reliable system with no risk of error introduced by letting unpredictable LLMs meddle in your workflow. For the sake of simplicity and robustness, it's advised to regularize towards not using any agentic behaviour. 

But what if the workflow can't be determined that well in advance? 

For instance, a user wants to ask: `"I can come on Monday, but I forgot my passport so risk being delayed to Wednesday, is it possible to take me and my stuff to surf on Tuesday morning, with a cancellation insurance?"` This question hinges on many factors, and probably none of the predetermined criteria above will suffice for this request.

If the pre-determined workflow falls short too often, that means you need more flexibility.

That is where an agentic setup helps.

In the above example, you could just make a multi-step agent that has access to a weather API for weather forecasts, Google Maps API to compute travel distance, an employee availability dashboard and a RAG system on your knowledge base.

Until recently, computer programs were restricted to pre-determined workflows, trying to handle complexity by piling up  if/else switches. They focused on extremely narrow tasks, like "compute the sum of these numbers" or "find the shortest path in this graph". But actually, most real-life tasks, like our trip example above, do not fit in pre-determined workflows. Agentic systems open up the vast world of real-world tasks to programs!

## Why `smolagents`?

For some low-level agentic use cases, like chains or routers, you can write all the code yourself. You'll be much better that way, since it will let you control and understand your system better.

But once you start going for more complicated behaviours like letting an LLM call a function (that's "tool calling") or letting an LLM run a while loop ("multi-step agent"), some abstractions become necessary:
- For tool calling, you need to parse the agent's output, so this output needs a predefined format like "Thought: I should call tool 'get_weather'. Action: get_weather(Paris).", that you parse with a predefined function, and system prompt given to the LLM should notify it about this format.
- For a multi-step agent where the LLM output determines the loop, you need to give a different prompt to the LLM based on what happened in the last loop iteration: so you need some kind of memory.

See? With these two examples, we already found the need for a few items to help us:

- Of course, an LLM that acts as the engine powering the system
- A list of tools that the agent can access
- A system prompt guiding the LLM on the agent logic: ReAct loop of Reflection -> Action -> Observation, available tools, tool calling format to use...
- A parser that extracts tool calls from the LLM output, in the format indicated by system prompt above.
- A memory

But wait, since we give room to LLMs in decisions, surely they will make mistakes: so we need error logging and retry mechanisms.

All these elements need tight coupling to make a well-functioning system. That's why we decided we needed to make basic building blocks to make all this stuff work together.

## Code agents

In a multi-step agent, at each step, the LLM can write an action, in the form of some calls to external tools. A common format (used by Anthropic, OpenAI, and many others) for writing these actions is generally different shades of "writing actions as a JSON of tools names and arguments to use, which you then parse to know which tool to execute and with which arguments".

[Multiple](https://huggingface.co/papers/2402.01030) [research](https://huggingface.co/papers/2411.01747) [papers](https://huggingface.co/papers/2401.00812) have shown that having the LLMs actions written as code snippets is a more natural and flexible way of writing them.

The reason for this simply that *we crafted our code languages specifically to express the actions performed by a computer*.
In other words, our agent is going to write programs in order to solve the user's issues : do you think their programming will be easier in blocks of Python or JSON?

The figure below, taken from [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030), illustrates some advantages of writing actions in code:

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png">

Writing actions in code rather than JSON-like snippets provides better:

- **Composability:** could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?
- **Object management:** how do you store the output of an action like `generate_image` in JSON?
- **Generality:** code is built to express simply anything you can have a computer do.
- **Representation in LLM training data:** plenty of quality code actions are already included in LLMsâ€™ training data which means theyâ€™re already trained for this!



================================================
FILE: docs/source/en/conceptual_guides/react.md
================================================
# How do multi-step agents work?

The ReAct framework ([Yao et al., 2022](https://huggingface.co/papers/2210.03629)) is currently the main approach to building agents.

The name is based on the concatenation of two words, "Reason" and "Act." Indeed, agents following this architecture will solve their task in as many steps as needed, each step consisting of a Reasoning step, then an Action step where it formulates tool calls that will bring it closer to solving the task at hand.

All agents in `smolagents` are based on singular `MultiStepAgent` class, which is an abstraction of ReAct framework.

On a basic level, this class performs actions on a cycle of following steps, where existing variables and knowledge is incorporated into the agent logs like below: 

Initialization: the system prompt is stored in a `SystemPromptStep`, and the user query is logged into a `TaskStep` .

While loop (ReAct loop):

- Use `agent.write_memory_to_messages()` to write the agent logs into a list of LLM-readable [chat messages](https://huggingface.co/docs/transformers/en/chat_templating).
- Send these messages to a `Model` object to get its completion. Parse the completion to get the action (a JSON blob for `ToolCallingAgent`, a code snippet for `CodeAgent`).
- Execute the action and logs result into memory (an `ActionStep`).
- At the end of each step, we run all callback functions defined in `agent.step_callbacks` .

Optionally, when planning is activated, a plan can be periodically revised and stored in a `PlanningStep` . This includes feeding facts about the task at hand to the memory.

For a `CodeAgent`, it looks like the figure below.

<div class="flex justify-center">
    <img
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/codeagent_docs.png"
    />
</div>

Here is a video overview of how that works:

<div class="flex justify-center">
    <img
        class="block dark:hidden"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
    <img
        class="hidden dark:block"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
</div>

We implement two versions of agents:
- [`CodeAgent`] generates its tool calls as Python code snippets.
- [`ToolCallingAgent`] writes its tool calls as JSON, as is common in many frameworks. Depending on your needs, either approach can be used. For instance, web browsing often requires waiting after each page interaction, so JSON tool calls can fit well.

> [!TIP]
> Read [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) blog post to learn more about multi-step agents.



================================================
FILE: docs/source/en/examples/async_agent.md
================================================
# Async Applications with Agents

This guide demonstrates how to integrate a synchronous agent from the `smolagents` library into an asynchronous Python web application using Starlette.
The example is designed to help users new to async Python and agent integration understand best practices for combining synchronous agent logic with async web servers.

## Overview

- **Starlette**: A lightweight ASGI framework for building asynchronous web applications in Python.
- **anyio.to_thread.run_sync**: Utility to run blocking (synchronous) code in a background thread, preventing it from blocking the async event loop.
- **CodeAgent**: An agent from the `smolagents` library capable of programmatically solving tasks.

## Why Use a Background Thread?

`CodeAgent.run()` executes Python code synchronously. If called directly in an async endpoint, it would block Starlette's event loop, reducing performance and scalability. By offloading this operation to a background thread with `anyio.to_thread.run_sync`, you keep the app responsive and efficient, even under high concurrency.

## Example Workflow

- The Starlette app exposes a `/run-agent` endpoint that accepts a JSON payload with a `task` string.
- When a request is received, the agent is run in a background thread using `anyio.to_thread.run_sync`.
- The result is returned as a JSON response.

## Building a Starlette App with a CodeAgent

### 1. Install Dependencies

```bash
pip install smolagents starlette anyio uvicorn
```

### 2. Application Code (`main.py`)

```python
import anyio.to_thread
from starlette.applications import Starlette
from starlette.requests import Request
from starlette.responses import JSONResponse
from starlette.routing import Route

from smolagents import CodeAgent, InferenceClientModel

agent = CodeAgent(
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
    tools=[],
)

async def run_agent(request: Request):
    data = await request.json()
    task = data.get("task", "")
    # Run the agent synchronously in a background thread
    result = await anyio.to_thread.run_sync(agent.run, task)
    return JSONResponse({"result": result})

app = Starlette(routes=[
    Route("/run-agent", run_agent, methods=["POST"]),
])
```

### 3. Run the App

```bash
uvicorn async_agent.main:app --reload
```

### 4. Test the Endpoint

```bash
curl -X POST http://localhost:8000/run-agent -H 'Content-Type: application/json' -d '{"task": "What is 2+2?"}'
```

**Expected Response:**

```json
{"result": "4"}
```

## Further Reading

- [Starlette Documentation](https://www.starlette.io/)
- [anyio Documentation](https://anyio.readthedocs.io/)

---

For the full code, see [`examples/async_agent`](https://github.com/huggingface/smolagents/tree/main/examples/async_agent).



================================================
FILE: docs/source/en/examples/multiagents.md
================================================
# Orchestrate a multi-agent system ðŸ¤–ðŸ¤ðŸ¤–

[[open-in-colab]]

In this notebook we will make a **multi-agent web browser: an agentic system with several agents collaborating to solve problems using the web!**

It will be a simple hierarchy:

```
              +----------------+
              | Manager agent  |
              +----------------+
                       |
        _______________|______________
       |                              |
Code Interpreter            +------------------+
    tool                    | Web Search agent |
                            +------------------+
                               |            |
                        Web Search tool     |
                                   Visit webpage tool
```
Let's set up this system. 

Run the line below to install the required dependencies:

```py
!pip install 'smolagents[toolkit]' --upgrade -q
```

Let's login to HF in order to call Inference Providers:

```py
from huggingface_hub import login

login()
```

âš¡ï¸ Our agent will be powered by [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking) using `InferenceClientModel` class that uses HF's Inference API: the Inference API allows to quickly and easily run any OS model.

> [!TIP]
> Inference Providers give access to hundreds of models, powered by serverless inference partners. A list of supported providers can be found [here](https://huggingface.co/docs/inference-providers/index).

```py
model_id = "Qwen/Qwen3-Next-80B-A3B-Thinking"
```

## ðŸ” Create a web search tool

For web browsing, we can already use our native [`WebSearchTool`] tool to provide a Google search equivalent.

But then we will also need to be able to peak into the page found by the `WebSearchTool`.
To do so, we could import the library's built-in `VisitWebpageTool`, but we will build it again to see how it's done.

So let's create our `VisitWebpageTool` tool from scratch using `markdownify`.

```py
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool


@tool
def visit_webpage(url: str) -> str:
    """Visits a webpage at the given URL and returns its content as a markdown string.

    Args:
        url: The URL of the webpage to visit.

    Returns:
        The content of the webpage converted to Markdown, or an error message if the request fails.
    """
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Convert the HTML content to Markdown
        markdown_content = markdownify(response.text).strip()

        # Remove multiple line breaks
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

Ok, now let's initialize and test our tool!

```py
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## Build our multi-agent system ðŸ¤–ðŸ¤ðŸ¤–

Now that we have all the tools `search` and `visit_webpage`, we can use them to create the web agent.

Which configuration to choose for this agent?
- Web browsing is a single-timeline task that does not require parallel tool calls, so JSON tool calling works well for that. We thus choose a `ToolCallingAgent`.
- Also, since sometimes web search requires exploring many pages before finding the correct answer, we prefer to increase the number of `max_steps` to 10.

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    InferenceClientModel,
    WebSearchTool,
)

model = InferenceClientModel(model_id=model_id)

web_agent = ToolCallingAgent(
    tools=[WebSearchTool(), visit_webpage],
    model=model,
    max_steps=10,
    name="web_search_agent",
    description="Runs web searches for you.",
)
```

Note that we gave this agent attributes `name` and `description`, mandatory attributes to make this agent callable by its manager agent.

Then we create a manager agent, and upon initialization we pass our managed agent to it in its `managed_agents` argument.

Since this agent is the one tasked with the planning and thinking, advanced reasoning will be beneficial, so a `CodeAgent` will work well.

Also, we want to ask a question that involves the current year and does additional data calculations: so let us add `additional_authorized_imports=["time", "numpy", "pandas"]`, just in case the agent needs these packages.

```py
manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

That's all! Now let's run our system! We select a question that requires both some calculation and research:

```py
answer = manager_agent.run("If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.")
```

We get this report as the answer:
```
Based on current growth projections and energy consumption estimates, if LLM trainings continue to scale up at the 
current rhythm until 2030:

1. The electric power required to power the biggest training runs by 2030 would be approximately 303.74 GW, which 
translates to about 2,660,762 GWh/year.

2. Comparing this to countries' electricity consumption:
   - It would be equivalent to about 34% of China's total electricity consumption.
   - It would exceed the total electricity consumption of India (184%), Russia (267%), and Japan (291%).
   - It would be nearly 9 times the electricity consumption of countries like Italy or Mexico.

3. Source of numbers:
   - The initial estimate of 5 GW for future LLM training comes from AWS CEO Matt Garman.
   - The growth projection used a CAGR of 79.80% from market research by Springs.
   - Country electricity consumption data is from the U.S. Energy Information Administration, primarily for the year 
2021.
```

Seems like we'll need some sizeable powerplants if the [scaling hypothesis](https://gwern.net/scaling-hypothesis) continues to hold true.

Our agents managed to efficiently collaborate towards solving the task! âœ…

ðŸ’¡ You can easily extend this orchestration to more agents: one does the code execution, one the web search, one handles file loadings...



================================================
FILE: docs/source/en/examples/plan_customization.md
================================================
# Human-in-the-Loop: Customize Agent Plan Interactively

This page demonstrates advanced usage of the smolagents library, with a special focus on **Human-in-the-Loop (HITL)** approaches for interactive plan creation, user-driven plan modification, and memory preservation in agentic workflows.
The example is based on the code in `examples/plan_customization/plan_customization.py`.

## Overview

This example teaches you how to implement Human-in-the-Loop strategies to:

- Interrupt agent execution after a plan is created (using step callbacks)
- Allow users to review and modify the agent's plan before execution (Human-in-the-Loop)
- Resume execution while preserving the agent's memory
- Dynamically update plans based on user feedback, keeping the human in control

## Key Concepts

### Step Callbacks for Plan Interruption

The agent is configured to pause after creating a plan. This is achieved by registering a step callback for the `PlanningStep`:

```python
agent = CodeAgent(
    model=InferenceClientModel(),
    tools=[DuckDuckGoSearchTool()],
    planning_interval=5,  # Plan every 5 steps
    step_callbacks={PlanningStep: interrupt_after_plan},
    max_steps=10,
    verbosity_level=1
)
```

### Human-in-the-Loop: Interactive Plan Review and Modification

When the agent creates a plan, the callback displays it and prompts the human user to:

1. Approve the plan
2. Modify the plan
3. Cancel execution

Example interaction:

```
============================================================
ðŸ¤– AGENT PLAN CREATED
============================================================
1. Search for recent AI developments
2. Analyze the top results
3. Summarize the 3 most significant breakthroughs
4. Include sources for each breakthrough
============================================================

Choose an option:
1. Approve plan
2. Modify plan
3. Cancel
Your choice (1-3):
```

This Human-in-the-Loop step enables a human to intervene and review or modify the plan before execution continues, and ensures that the agent's actions align with human intent.

If the user chooses to modify, they can edit the plan directly. The updated plan is then used for subsequent execution steps.

### Memory Preservation and Resuming Execution

By running the agent with `reset=False`, all previous steps and memory are preserved. This allows you to resume execution after an interruption or plan modification:

```python
# First run (may be interrupted)
agent.run(task, reset=True)

# Resume with preserved memory
agent.run(task, reset=False)
```

### Inspecting Agent Memory

You can inspect the agent's memory to see all steps taken so far:

```python
print(f"Current memory contains {len(agent.memory.steps)} steps:")
for i, step in enumerate(agent.memory.steps):
    step_type = type(step).__name__
    print(f"  {i+1}. {step_type}")
```

## Example Human-in-the-Loop Workflow

1. Agent starts with a complex task
2. Planning step is created and execution pauses for human review
3. Human reviews and optionally modifies the plan (Human-in-the-Loop)
4. Execution resumes with the approved/modified plan
5. All steps are preserved for future runs, maintaining transparency and control

## Error Handling

The example includes error handling for:
- User cancellation
- Plan modification errors
- Resume execution failures

## Requirements

- smolagents library
- DuckDuckGoSearchTool (included with smolagents)
- InferenceClientModel (requires HuggingFace API token)

## Educational Value

This example demonstrates:
- Step callback implementation for custom agent behavior
- Memory management in multi-step agents
- User interaction patterns in agentic systems
- Plan modification techniques for dynamic agent control
- Error handling in interactive agent systems

---

For the full code, see [`examples/plan_customization`](https://github.com/huggingface/smolagents/tree/main/examples/plan_customization).



================================================
FILE: docs/source/en/examples/rag.md
================================================
# Agentic RAG

[[open-in-colab]]

## Introduction to Retrieval-Augmented Generation (RAG)

Retrieval-Augmented Generation (RAG) combines the power of large language models with external knowledge retrieval to produce more accurate, factual, and contextually relevant responses. At its core, RAG is about "using an LLM to answer a user query, but basing the answer on information retrieved from a knowledge base."

### Why Use RAG?

RAG offers several significant advantages over using vanilla or fine-tuned LLMs:

1. **Factual Grounding**: Reduces hallucinations by anchoring responses in retrieved facts
2. **Domain Specialization**: Provides domain-specific knowledge without model retraining
3. **Knowledge Recency**: Allows access to information beyond the model's training cutoff
4. **Transparency**: Enables citation of sources for generated content
5. **Control**: Offers fine-grained control over what information the model can access

### Limitations of Traditional RAG

Despite its benefits, traditional RAG approaches face several challenges:

- **Single Retrieval Step**: If the initial retrieval results are poor, the final generation will suffer
- **Query-Document Mismatch**: User queries (often questions) may not match well with documents containing answers (often statements)
- **Limited Reasoning**: Simple RAG pipelines don't allow for multi-step reasoning or query refinement
- **Context Window Constraints**: Retrieved documents must fit within the model's context window

## Agentic RAG: A More Powerful Approach

We can overcome these limitations by implementing an **Agentic RAG** system - essentially an agent equipped with retrieval capabilities. This approach transforms RAG from a rigid pipeline into an interactive, reasoning-driven process.

### Key Benefits of Agentic RAG

An agent with retrieval tools can:

1. âœ… **Formulate optimized queries**: The agent can transform user questions into retrieval-friendly queries
2. âœ… **Perform multiple retrievals**: The agent can retrieve information iteratively as needed
3. âœ… **Reason over retrieved content**: The agent can analyze, synthesize, and draw conclusions from multiple sources
4. âœ… **Self-critique and refine**: The agent can evaluate retrieval results and adjust its approach

This approach naturally implements advanced RAG techniques:
- **Hypothetical Document Embedding (HyDE)**: Instead of using the user query directly, the agent formulates retrieval-optimized queries ([paper reference](https://huggingface.co/papers/2212.10496))
- **Self-Query Refinement**: The agent can analyze initial results and perform follow-up retrievals with refined queries ([technique reference](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery/))

## Building an Agentic RAG System

Let's build a complete Agentic RAG system step by step. We'll create an agent that can answer questions about the Hugging Face Transformers library by retrieving information from its documentation.

You can follow along with the code snippets below, or check out the full example in the smolagents GitHub repository: [examples/rag.py](https://github.com/huggingface/smolagents/blob/main/examples/rag.py).

### Step 1: Install Required Dependencies

First, we need to install the necessary packages:

```bash
pip install smolagents pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade
```

If you plan to use Hugging Face's Inference API, you'll need to set up your API token:

```python
# Load environment variables (including HF_TOKEN)
from dotenv import load_dotenv
load_dotenv()
```

### Step 2: Prepare the Knowledge Base

We'll use a dataset containing Hugging Face documentation and prepare it for retrieval:

```python
import datasets
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever

# Load the Hugging Face documentation dataset
knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")

# Filter to include only Transformers documentation
knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))

# Convert dataset entries to Document objects with metadata
source_docs = [
    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]})
    for doc in knowledge_base
]

# Split documents into smaller chunks for better retrieval
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # Characters per chunk
    chunk_overlap=50,  # Overlap between chunks to maintain context
    add_start_index=True,
    strip_whitespace=True,
    separators=["\n\n", "\n", ".", " ", ""],  # Priority order for splitting
)
docs_processed = text_splitter.split_documents(source_docs)

print(f"Knowledge base prepared with {len(docs_processed)} document chunks")
```

### Step 3: Create a Retriever Tool

Now we'll create a custom tool that our agent can use to retrieve information from the knowledge base:

```python
from smolagents import Tool

class RetrieverTool(Tool):
    name = "retriever"
    description = "Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    inputs = {
        "query": {
            "type": "string",
            "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
        }
    }
    output_type = "string"

    def __init__(self, docs, **kwargs):
        super().__init__(**kwargs)
        # Initialize the retriever with our processed documents
        self.retriever = BM25Retriever.from_documents(
            docs, k=10  # Return top 10 most relevant documents
        )

    def forward(self, query: str) -> str:
        """Execute the retrieval based on the provided query."""
        assert isinstance(query, str), "Your search query must be a string"

        # Retrieve relevant documents
        docs = self.retriever.invoke(query)

        # Format the retrieved documents for readability
        return "\nRetrieved documents:\n" + "".join(
            [
                f"\n\n===== Document {str(i)} =====\n" + doc.page_content
                for i, doc in enumerate(docs)
            ]
        )

# Initialize our retriever tool with the processed documents
retriever_tool = RetrieverTool(docs_processed)
```

> [!TIP]
> We're using BM25, a lexical retrieval method, for simplicity and speed. For production systems, you might want to use semantic search with embeddings for better retrieval quality. Check the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for high-quality embedding models.

### Step 4: Create an Advanced Retrieval Agent

Now we'll create an agent that can use our retriever tool to answer questions:

```python
from smolagents import InferenceClientModel, CodeAgent

# Initialize the agent with our retriever tool
agent = CodeAgent(
    tools=[retriever_tool],  # List of tools available to the agent
    model=InferenceClientModel(),  # Default model "Qwen/Qwen3-Next-80B-A3B-Thinking"
    max_steps=4,  # Limit the number of reasoning steps
    verbosity_level=2,  # Show detailed agent reasoning
)

# To use a specific model, you can specify it like this:
# model=InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct")
```

> [!TIP]
> Inference Providers give access to hundreds of models, powered by serverless inference partners. A list of supported providers can be found [here](https://huggingface.co/docs/inference-providers/index).

### Step 5: Run the Agent to Answer Questions

Let's use our agent to answer a question about Transformers:

```python
# Ask a question that requires retrieving information
question = "For a transformers model training, which is slower, the forward or the backward pass?"

# Run the agent to get an answer
agent_output = agent.run(question)

# Display the final answer
print("\nFinal answer:")
print(agent_output)
```

## Practical Applications of Agentic RAG

Agentic RAG systems can be applied to various use cases:

1. **Technical Documentation Assistance**: Help users navigate complex technical documentation
2. **Research Paper Analysis**: Extract and synthesize information from scientific papers
3. **Legal Document Review**: Find relevant precedents and clauses in legal documents
4. **Customer Support**: Answer questions based on product documentation and knowledge bases
5. **Educational Tutoring**: Provide explanations based on textbooks and learning materials

## Conclusion

Agentic RAG represents a significant advancement over traditional RAG pipelines. By combining the reasoning capabilities of LLM agents with the factual grounding of retrieval systems, we can build more powerful, flexible, and accurate information systems.

The approach we've demonstrated:
- Overcomes the limitations of single-step retrieval
- Enables more natural interactions with knowledge bases
- Provides a framework for continuous improvement through self-critique and query refinement

As you build your own Agentic RAG systems, consider experimenting with different retrieval methods, agent architectures, and knowledge sources to find the optimal configuration for your specific use case.



================================================
FILE: docs/source/en/examples/text_to_sql.md
================================================
# Text-to-SQL

[[open-in-colab]]

In this tutorial, weâ€™ll see how to implement an agent that leverages SQL using `smolagents`.

> Let's start with the golden question: why not keep it simple and use a standard text-to-SQL pipeline?

A standard text-to-sql pipeline is brittle, since the generated SQL query can be incorrect. Even worse, the query could be incorrect, but not raise an error, instead giving some incorrect/useless outputs without raising an alarm.

ðŸ‘‰ Instead, an agent system is able to critically inspect outputs and decide if the query needs to be changed or not, thus giving it a huge performance boost.

Letâ€™s build this agent! ðŸ’ª

Run the line below to install required dependencies:
```bash
!pip install smolagents python-dotenv sqlalchemy --upgrade -q
```
To call Inference Providers, you will need a valid token as your environment variable `HF_TOKEN`.
We use python-dotenv to load it.
```py
from dotenv import load_dotenv
load_dotenv()
```

Then, we setup the SQL environment:
```py
from sqlalchemy import (
    create_engine,
    MetaData,
    Table,
    Column,
    String,
    Integer,
    Float,
    insert,
    inspect,
    text,
)

engine = create_engine("sqlite:///:memory:")
metadata_obj = MetaData()

def insert_rows_into_table(rows, table, engine=engine):
    for row in rows:
        stmt = insert(table).values(**row)
        with engine.begin() as connection:
            connection.execute(stmt)

table_name = "receipts"
receipts = Table(
    table_name,
    metadata_obj,
    Column("receipt_id", Integer, primary_key=True),
    Column("customer_name", String(16), primary_key=True),
    Column("price", Float),
    Column("tip", Float),
)
metadata_obj.create_all(engine)

rows = [
    {"receipt_id": 1, "customer_name": "Alan Payne", "price": 12.06, "tip": 1.20},
    {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
    {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
    {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
]
insert_rows_into_table(rows, receipts)
```

### Build our agent

Now letâ€™s make our SQL table retrievable by a tool.

The toolâ€™s description attribute will be embedded in the LLMâ€™s prompt by the agent system: it gives the LLM information about how to use the tool. This is where we want to describe the SQL table.

```py
inspector = inspect(engine)
columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]

table_description = "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
print(table_description)
```

```text
Columns:
  - receipt_id: INTEGER
  - customer_name: VARCHAR(16)
  - price: FLOAT
  - tip: FLOAT
```

Now letâ€™s build our tool. It needs the following: (read [the tool doc](../tutorials/tools) for more detail)
- A docstring with an `Args:` part listing arguments.
- Type hints on both inputs and output.

```py
from smolagents import tool

@tool
def sql_engine(query: str) -> str:
    """
    Allows you to perform SQL queries on the table. Returns a string representation of the result.
    The table is named 'receipts'. Its description is as follows:
        Columns:
        - receipt_id: INTEGER
        - customer_name: VARCHAR(16)
        - price: FLOAT
        - tip: FLOAT

    Args:
        query: The query to perform. This should be correct SQL.
    """
    output = ""
    with engine.connect() as con:
        rows = con.execute(text(query))
        for row in rows:
            output += "\n" + str(row)
    return output
```

Now let us create an agent that leverages this tool.

We use the `CodeAgent`, which is smolagentsâ€™ main agent class: an agent that writes actions in code and can iterate on previous output according to the ReAct framework.

The model is the LLM that powers the agent system. `InferenceClientModel` allows you to call LLMs using HFâ€™s Inference API, either via Serverless or Dedicated endpoint, but you could also use any proprietary API.

```py
from smolagents import CodeAgent, InferenceClientModel

agent = CodeAgent(
    tools=[sql_engine],
    model=InferenceClientModel(model_id="meta-llama/Llama-3.1-8B-Instruct"),
)
agent.run("Can you give me the name of the client who got the most expensive receipt?")
```

### Level 2: Table joins

Now letâ€™s make it more challenging! We want our agent to handle joins across multiple tables.

So letâ€™s make a second table recording the names of waiters for each receipt_id!

```py
table_name = "waiters"
waiters = Table(
    table_name,
    metadata_obj,
    Column("receipt_id", Integer, primary_key=True),
    Column("waiter_name", String(16), primary_key=True),
)
metadata_obj.create_all(engine)

rows = [
    {"receipt_id": 1, "waiter_name": "Corey Johnson"},
    {"receipt_id": 2, "waiter_name": "Michael Watts"},
    {"receipt_id": 3, "waiter_name": "Michael Watts"},
    {"receipt_id": 4, "waiter_name": "Margaret James"},
]
insert_rows_into_table(rows, waiters)
```
Since we changed the table, we update the `SQLExecutorTool` with this tableâ€™s description to let the LLM properly leverage information from this table.

```py
updated_description = """Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.
It can use the following tables:"""

inspector = inspect(engine)
for table in ["receipts", "waiters"]:
    columns_info = [(col["name"], col["type"]) for col in inspector.get_columns(table)]

    table_description = f"Table '{table}':\n"

    table_description += "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
    updated_description += "\n\n" + table_description

print(updated_description)
```
Since this request is a bit harder than the previous one, weâ€™ll switch the LLM engine to use the more powerful [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)!

```py
sql_engine.description = updated_description

agent = CodeAgent(
    tools=[sql_engine],
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
)

agent.run("Which waiter got more total money from tips?")
```
It directly works! The setup was surprisingly simple, wasnâ€™t it?

This example is done! We've touched upon these concepts:
- Building new tools.
- Updating a tool's description.
- Switching to a stronger LLM helps agent reasoning.

âœ… Now you can go build this text-to-SQL system youâ€™ve always dreamt of! âœ¨



================================================
FILE: docs/source/en/examples/using_different_models.md
================================================
# Using different models

[[open-in-colab]]

`smolagents` provides a flexible framework that allows you to use various language models from different providers.
This guide will show you how to use different model types with your agents.

## Available model types

`smolagents` supports several model types out of the box:
1. [`InferenceClientModel`]: Uses Hugging Face's Inference API to access models
2. [`TransformersModel`]: Runs models locally using the Transformers library
3. [`VLLMModel`]: Uses vLLM for fast inference with optimized serving
4. [`MLXModel`]: Optimized for Apple Silicon devices using MLX
5. [`LiteLLMModel`]: Provides access to hundreds of LLMs through LiteLLM
6. [`LiteLLMRouterModel`]: Distributes requests among multiple models
7. [`OpenAIModel`]: Provides access to any provider that implements an OpenAI-compatible API
8. [`AzureOpenAIModel`]: Uses Azure's OpenAI service
9. [`AmazonBedrockModel`]: Connects to AWS Bedrock's API

All model classes support passing additional keyword arguments (like `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time.
These parameters are automatically forwarded to the underlying model's completion calls, allowing you to configure model behavior such as creativity, response length, and sampling strategies.

## Using Google Gemini Models

As explained in the Google Gemini API documentation (https://ai.google.dev/gemini-api/docs/openai),
Google provides an OpenAI-compatible API for Gemini models, allowing you to use the [`OpenAIModel`]
with Gemini models by setting the appropriate base URL.

First, install the required dependencies:
```bash
pip install 'smolagents[openai]'
```

Then, [get a Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) and set it in your code:
```python
GEMINI_API_KEY = <YOUR-GEMINI-API-KEY>
```

Now, you can initialize the Gemini model using the `OpenAIModel` class
and setting the `api_base` parameter to the Gemini API base URL:
```python
from smolagents import OpenAIModel

model = OpenAIModel(
    model_id="gemini-2.0-flash",
    # Google Gemini OpenAI-compatible API base URL
    api_base="https://generativelanguage.googleapis.com/v1beta/openai/",
    api_key=GEMINI_API_KEY,
)
```

## Using OpenRouter Models

OpenRouter provides access to a wide variety of language models through a unified OpenAI-compatible API.
You can use the [`OpenAIModel`] to connect to OpenRouter by setting the appropriate base URL.

First, install the required dependencies:
```bash
pip install 'smolagents[openai]'
```

Then, [get an OpenRouter API key](https://openrouter.ai/keys) and set it in your code:
```python
OPENROUTER_API_KEY = <YOUR-OPENROUTER-API-KEY>
```

Now, you can initialize any model available on OpenRouter using the `OpenAIModel` class:
```python
from smolagents import OpenAIModel

model = OpenAIModel(
    # You can use any model ID available on OpenRouter
    model_id="openai/gpt-4o",
    # OpenRouter API base URL
    api_base="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)
```

## Using xAI's Grok Models

xAI's Grok models can be accessed through [`LiteLLMModel`].

Some models (such as "grok-4" and "grok-3-mini") don't support the `stop` parameter, so you'll need to use
`REMOVE_PARAMETER` to exclude it from API calls.

First, install the required dependencies:
```bash
pip install smolagents[litellm]
```

Then, [get an xAI API key](https://console.x.ai/) and set it in your code:
```python
XAI_API_KEY = <YOUR-XAI-API-KEY>
```

Now, you can initialize Grok models using the `LiteLLMModel` class and remove the `stop` parameter if applicable:
```python
from smolagents import LiteLLMModel, REMOVE_PARAMETER

# Using Grok-4
model = LiteLLMModel(
    model_id="xai/grok-4",
    api_key=XAI_API_KEY,
    stop=REMOVE_PARAMETER,  # Remove stop parameter as grok-4 model doesn't support it
    temperature=0.7
)

# Or using Grok-3-mini
model_mini = LiteLLMModel(
    model_id="xai/grok-3-mini",
    api_key=XAI_API_KEY,
    stop=REMOVE_PARAMETER,  # Remove stop parameter as grok-3-mini model doesn't support it
    max_tokens=1000
)
```



================================================
FILE: docs/source/en/examples/web_browser.md
================================================
# Web Browser Automation with Agents ðŸ¤–ðŸŒ

[[open-in-colab]]

In this notebook, we'll create an **agent-powered web browser automation system**! This system can navigate websites, interact with elements, and extract information automatically.

The agent will be able to:

- [x] Navigate to web pages
- [x] Click on elements
- [x] Search within pages
- [x] Handle popups and modals
- [x] Extract information

Let's set up this system step by step!

First, run these lines to install the required dependencies:

```bash
pip install smolagents selenium helium pillow -q
```

Let's import our required libraries and set up environment variables:

```python
from io import BytesIO
from time import sleep

import helium
from dotenv import load_dotenv
from PIL import Image
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from smolagents import CodeAgent, tool
from smolagents.agents import ActionStep

# Load environment variables
load_dotenv()
```

Now let's create our core browser interaction tools that will allow our agent to navigate and interact with web pages:

```python
@tool
def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
    """
    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.
    Args:
        text: The text to search for
        nth_result: Which occurrence to jump to (default: 1)
    """
    elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{text}')]")
    if nth_result > len(elements):
        raise Exception(f"Match nÂ°{nth_result} not found (only {len(elements)} matches found)")
    result = f"Found {len(elements)} matches for '{text}'."
    elem = elements[nth_result - 1]
    driver.execute_script("arguments[0].scrollIntoView(true);", elem)
    result += f"Focused on element {nth_result} of {len(elements)}"
    return result

@tool
def go_back() -> None:
    """Goes back to previous page."""
    driver.back()

@tool
def close_popups() -> str:
    """
    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows!
    This does not work on cookie consent banners.
    """
    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()
```

Let's set up our browser with Chrome and configure screenshot capabilities:

```python
# Configure Chrome options
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--force-device-scale-factor=1")
chrome_options.add_argument("--window-size=1000,1350")
chrome_options.add_argument("--disable-pdf-viewer")
chrome_options.add_argument("--window-position=0,0")

# Initialize the browser
driver = helium.start_chrome(headless=False, options=chrome_options)

# Set up screenshot callback
def save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    driver = helium.get_driver()
    current_step = memory_step.step_number
    if driver is not None:
        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing
            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:
                previous_memory_step.observations_images = None
        png_bytes = driver.get_screenshot_as_png()
        image = Image.open(BytesIO(png_bytes))
        print(f"Captured a browser screenshot: {image.size} pixels")
        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists

    # Update observations with current URL
    url_info = f"Current url: {driver.current_url}"
    memory_step.observations = (
        url_info if memory_step.observations is None else memory_step.observations + "\n" + url_info
    )
```

Now let's create our web automation agent:

```python
from smolagents import InferenceClientModel

# Initialize the model
model_id = "Qwen/Qwen2-VL-72B-Instruct"  # You can change this to your preferred VLM model
model = InferenceClientModel(model_id=model_id)

# Create the agent
agent = CodeAgent(
    tools=[go_back, close_popups, search_item_ctrl_f],
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[save_screenshot],
    max_steps=20,
    verbosity_level=2,
)

# Import helium for the agent
agent.python_executor("from helium import *", agent.state)
```

The agent needs instructions on how to use Helium for web automation. Here are the instructions we'll provide:

```python
helium_instructions = """
You can use helium to access websites. Don't bother about the helium driver, it's already managed.
We've already ran "from helium import *"
Then you can go to pages!
Code:
```py
go_to('github.com/trending')
```<end_code>

You can directly click clickable elements by inputting the text that appears on them.
Code:
```py
click("Top products")
```<end_code>

If it's a link:
Code:
```py
click(Link("Top products"))
```<end_code>

If you try to interact with an element and it's not found, you'll get a LookupError.
In general stop your action after each button click to see what happens on your screenshot.
Never try to login in a page.

To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
Code:
```py
scroll_down(num_pixels=1200) # This will scroll one viewport down
```<end_code>

When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
Just use your built-in tool `close_popups` to close them:
Code:
```py
close_popups()
```<end_code>

You can use .exists() to check for the existence of an element. For example:
Code:
```py
if Text('Accept cookies?').exists():
    click('I accept')
```<end_code>
"""
```

Now we can run our agent with a task! Let's try finding information on Wikipedia:

```python
search_request = """
Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
"""

agent_output = agent.run(search_request + helium_instructions)
print("Final output:")
print(agent_output)
```

You can run different tasks by modifying the request. For example, here's for me to know if I should work harder:

```python
github_request = """
I'm trying to find how hard I have to work to get a repo in github.com/trending.
Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
"""

agent_output = agent.run(github_request + helium_instructions)
print("Final output:")
print(agent_output)
```

The system is particularly effective for tasks like:
- Data extraction from websites
- Web research automation
- UI testing and verification
- Content monitoring


================================================
FILE: docs/source/en/reference/agents.md
================================================
# Agents

<Tip warning={true}>

Smolagents is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.

</Tip>

To learn more about agents and tools make sure to read the [introductory guide](../index). This page
contains the API docs for the underlying classes.

## Agents

Our agents inherit from [`MultiStepAgent`], which means they can act in multiple steps, each step consisting of one thought, then one tool call and execution. Read more in [this conceptual guide](../conceptual_guides/react).

We provide two types of agents, based on the main [`Agent`] class.
  - [`CodeAgent`] writes its tool calls in Python code (this is the default).
  - [`ToolCallingAgent`] writes its tool calls in JSON.

Both require arguments `model` and list of tools `tools` at initialization.

### Classes of agents

[[autodoc]] MultiStepAgent

[[autodoc]] CodeAgent

[[autodoc]] ToolCallingAgent

### stream_to_gradio

[[autodoc]] stream_to_gradio

### GradioUI

> [!TIP]
> You must have `gradio` installed to use the UI. Please run `pip install 'smolagents[gradio]'` if it's not the case.

[[autodoc]] GradioUI

## Prompts

[[autodoc]] smolagents.agents.PromptTemplates

[[autodoc]] smolagents.agents.PlanningPromptTemplate

[[autodoc]] smolagents.agents.ManagedAgentPromptTemplate

[[autodoc]] smolagents.agents.FinalAnswerPromptTemplate

## Memory

Smolagents use memory to store information across multiple steps.

[[autodoc]] smolagents.memory.AgentMemory

## Python code executors

[[autodoc]] smolagents.local_python_executor.PythonExecutor

### Local Python executor

[[autodoc]] smolagents.local_python_executor.LocalPythonExecutor

### Remote Python executors

[[autodoc]] smolagents.remote_executors.RemotePythonExecutor

#### BlaxelExecutor

[[autodoc]] smolagents.remote_executors.BlaxelExecutor

#### E2BExecutor

[[autodoc]] smolagents.remote_executors.E2BExecutor

#### ModalExecutor

[[autodoc]] smolagents.remote_executors.ModalExecutor

#### DockerExecutor

[[autodoc]] smolagents.remote_executors.DockerExecutor

#### WasmExecutor

[[autodoc]] smolagents.remote_executors.WasmExecutor



================================================
FILE: docs/source/en/reference/default_tools.md
================================================
# Built-in Tools

Ready-to-use tool implementations provided by the `smolagents` library.

These built-in tools are concrete implementations of the [`Tool`] base class, each designed for specific tasks such as web searching, Python code execution, webpage retrieval, and user interaction.
You can use these tools directly in your agents without having to implement the underlying functionality yourself.
Each tool handles a particular capability and follows a consistent interface, making it easy to compose them into powerful agent workflows.

The built-in tools can be categorized by their primary functions:
- **Information Retrieval**: Search and retrieve information from the web and specific knowledge sources.
  - [`ApiWebSearchTool`]
  - [`DuckDuckGoSearchTool`]
  - [`GoogleSearchTool`]
  - [`WebSearchTool`]
  - [`WikipediaSearchTool`]
- **Web Interaction**: Fetch and process content from specific web pages.
  - [`VisitWebpageTool`]
- **Code Execution**: Dynamic execution of Python code for computational tasks.
  - [`PythonInterpreterTool`]
- **User Interaction**: Enable Human-in-the-Loop collaboration between agents and users.
  - [`UserInputTool`]: Collect input from users.
- **Speech Processing**: Convert audio to textual data.
  - [`SpeechToTextTool`]
- **Workflow Control**: Manage and direct the flow of agent operations.
  - [`FinalAnswerTool`]: Conclude agent workflow with final response.

## ApiWebSearchTool

[[autodoc]] smolagents.default_tools.ApiWebSearchTool

## DuckDuckGoSearchTool

[[autodoc]] smolagents.default_tools.DuckDuckGoSearchTool

## FinalAnswerTool

[[autodoc]] smolagents.default_tools.FinalAnswerTool

## GoogleSearchTool

[[autodoc]] smolagents.default_tools.GoogleSearchTool

## PythonInterpreterTool

[[autodoc]] smolagents.default_tools.PythonInterpreterTool

## SpeechToTextTool

[[autodoc]] smolagents.default_tools.SpeechToTextTool

## UserInputTool

[[autodoc]] smolagents.default_tools.UserInputTool

## VisitWebpageTool

[[autodoc]] smolagents.default_tools.VisitWebpageTool

## WebSearchTool

[[autodoc]] smolagents.default_tools.WebSearchTool

## WikipediaSearchTool

[[autodoc]] smolagents.default_tools.WikipediaSearchTool



================================================
FILE: docs/source/en/reference/models.md
================================================
# Models

<Tip warning={true}>

Smolagents is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.

</Tip>

To learn more about agents and tools make sure to read the [introductory guide](../index). This page
contains the API docs for the underlying classes.

## Models

All model classes in smolagents support passing additional keyword arguments (like `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time.
These parameters are automatically forwarded to the underlying model's completion calls, allowing you to configure model behavior such as creativity, response length, and sampling strategies.

### Base Model

The `Model` class serves as the foundation for all model implementations, providing the core interface that custom models must implement to work with agents.

[[autodoc]] Model

### API Model

The `ApiModel` class serves as the foundation for all API-based model implementations, providing common functionality for external API interactions, rate limiting, and client management that API-specific models inherit.

[[autodoc]] ApiModel

### TransformersModel

For convenience, we have added a `TransformersModel` that implements the points above by building a local `transformers` pipeline for the model_id given at initialization.

```python
from smolagents import TransformersModel

model = TransformersModel(model_id="HuggingFaceTB/SmolLM-135M-Instruct")

print(model([{"role": "user", "content": [{"type": "text", "text": "Ok!"}]}], stop_sequences=["great"]))
```
```text
>>> What a
```

You can pass any keyword arguments supported by the underlying model (such as `temperature`, `max_new_tokens`, `top_p`, etc.) directly at instantiation time. These are forwarded to the model completion call:

```python
model = TransformersModel(
    model_id="HuggingFaceTB/SmolLM-135M-Instruct",
    temperature=0.7,
    max_new_tokens=1000
)
```

> [!TIP]
> You must have `transformers` and `torch` installed on your machine. Please run `pip install 'smolagents[transformers]'` if it's not the case.

[[autodoc]] TransformersModel

### InferenceClientModel

The `InferenceClientModel` wraps huggingface_hub's [InferenceClient](https://huggingface.co/docs/huggingface_hub/main/en/guides/inference) for the execution of the LLM. It supports all [Inference Providers](https://huggingface.co/docs/inference-providers/index) available on the Hub: Cerebras, Cohere, Fal, Fireworks, HF-Inference, Hyperbolic, Nebius, Novita, Replicate, SambaNova, Together, and more.

You can also set a rate limit in requests per minute by using the `requests_per_minute` argument:

```python
from smolagents import InferenceClientModel

messages = [
  {"role": "user", "content": [{"type": "text", "text": "Hello, how are you?"}]}
]

model = InferenceClientModel(provider="novita", requests_per_minute=60)
print(model(messages))
```
```text
>>> Of course! If you change your mind, feel free to reach out. Take care!
```

You can pass any keyword arguments supported by the underlying model (such as `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time. These are forwarded to the model completion call:

```python
model = InferenceClientModel(
    provider="novita",
    requests_per_minute=60,
    temperature=0.8,
    max_tokens=500
)
```

[[autodoc]] InferenceClientModel

### LiteLLMModel

The `LiteLLMModel` leverages [LiteLLM](https://www.litellm.ai/) to support 100+ LLMs from various providers.
You can pass kwargs upon model initialization that will then be used whenever using the model, for instance below we pass `temperature`. You can also set a rate limit in requests per minute by using the `requests_per_minute` argument.

```python
from smolagents import LiteLLMModel

messages = [
  {"role": "user", "content": [{"type": "text", "text": "Hello, how are you?"}]}
]

model = LiteLLMModel(model_id="anthropic/claude-3-5-sonnet-latest", temperature=0.2, max_tokens=10, requests_per_minute=60)
print(model(messages))
```

[[autodoc]] LiteLLMModel

### LiteLLMRouterModel

The `LiteLLMRouterModel` is a wrapper around the [LiteLLM Router](https://docs.litellm.ai/docs/routing) that leverages
advanced routing strategies: load-balancing across multiple deployments, prioritizing critical requests via queueing,
and implementing basic reliability measures such as cooldowns, fallbacks, and exponential backoff retries.

```python
from smolagents import LiteLLMRouterModel

messages = [
  {"role": "user", "content": [{"type": "text", "text": "Hello, how are you?"}]}
]

model = LiteLLMRouterModel(
    model_id="llama-3.3-70b",
    model_list=[
        {
            "model_name": "llama-3.3-70b",
            "litellm_params": {"model": "groq/llama-3.3-70b", "api_key": os.getenv("GROQ_API_KEY")},
        },
        {
            "model_name": "llama-3.3-70b",
            "litellm_params": {"model": "cerebras/llama-3.3-70b", "api_key": os.getenv("CEREBRAS_API_KEY")},
        },
    ],
    client_kwargs={
        "routing_strategy": "simple-shuffle",
    },
)
print(model(messages))
```

[[autodoc]] LiteLLMRouterModel

### OpenAIModel

This class lets you call any OpenAIServer compatible model.
Here's how you can set it (you can customise the `api_base` url to point to another server):
```py
import os
from smolagents import OpenAIModel

model = OpenAIModel(
    model_id="gpt-4o",
    api_base="https://api.openai.com/v1",
    api_key=os.environ["OPENAI_API_KEY"],
)
```

You can pass any keyword arguments supported by the underlying model (such as `temperature`, `max_tokens`, `top_p`, etc.) directly at instantiation time. These are forwarded to the model completion call:

```py
model = OpenAIModel(
    model_id="gpt-4o",
    api_base="https://api.openai.com/v1",
    api_key=os.environ["OPENAI_API_KEY"],
    temperature=0.7,
    max_tokens=1000,
    top_p=0.9,
)
```

[[autodoc]] OpenAIModel

### AzureOpenAIModel

`AzureOpenAIModel` allows you to connect to any Azure OpenAI deployment. 

Below you can find an example of how to set it up, note that you can omit the `azure_endpoint`, `api_key`, and `api_version` arguments, provided you've set the corresponding environment variables -- `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION`.

Pay attention to the lack of an `AZURE_` prefix for `OPENAI_API_VERSION`, this is due to the way the underlying [openai](https://github.com/openai/openai-python) package is designed. 

```py
import os

from smolagents import AzureOpenAIModel

model = AzureOpenAIModel(
    model_id = os.environ.get("AZURE_OPENAI_MODEL"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    api_version=os.environ.get("OPENAI_API_VERSION")    
)
```

[[autodoc]] AzureOpenAIModel

### AmazonBedrockModel

`AmazonBedrockModel` helps you connect to Amazon Bedrock and run your agent with any available models.

Below is an example setup. This class also offers additional options for customization.

```py
import os

from smolagents import AmazonBedrockModel

model = AmazonBedrockModel(
    model_id = os.environ.get("AMAZON_BEDROCK_MODEL_ID"),
)
```

[[autodoc]] AmazonBedrockModel

### MLXModel


```python
from smolagents import MLXModel

model = MLXModel(model_id="HuggingFaceTB/SmolLM-135M-Instruct")

print(model([{"role": "user", "content": "Ok!"}], stop_sequences=["great"]))
```
```text
>>> What a
```

> [!TIP]
> You must have `mlx-lm` installed on your machine. Please run `pip install 'smolagents[mlx-lm]'` if it's not the case.

[[autodoc]] MLXModel

### VLLMModel

Model to use [vLLM](https://docs.vllm.ai/) for fast LLM inference and serving.

```python
from smolagents import VLLMModel

model = VLLMModel(model_id="HuggingFaceTB/SmolLM-135M-Instruct")

print(model([{"role": "user", "content": "Ok!"}], stop_sequences=["great"]))
```

> [!TIP]
> You must have `vllm` installed on your machine. Please run `pip install 'smolagents[vllm]'` if it's not the case.

[[autodoc]] VLLMModel

### Custom Model

You're free to create and use your own models to power your agent.

You could subclass the base `Model` class to create a model for your agent.
The main criteria is to subclass the `generate` method, with these two criteria:
1. It follows the [messages format](./chat_templating) (`List[Dict[str, str]]`) for its input `messages`, and it returns an object with a `.content` attribute.
2. It stops generating outputs at the sequences passed in the argument `stop_sequences`.

For defining your LLM, you can make a `CustomModel` class that inherits from the base `Model` class.
It should have a generate method that takes a list of [messages](./chat_templating) and returns an object with a .content attribute containing the text. The `generate` method also needs to accept a `stop_sequences` argument that indicates when to stop generating.

```python
from huggingface_hub import login, InferenceClient

from smolagents import Model

login("<YOUR_HUGGINGFACEHUB_API_TOKEN>")

model_id = "meta-llama/Llama-3.3-70B-Instruct"

client = InferenceClient(model=model_id)

class CustomModel(Model):
    def generate(messages, stop_sequences=["Task"]):
        response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1024)
        answer = response.choices[0].message
        return answer

custom_model = CustomModel()
```

Additionally, `generate` can also take a `grammar` argument to allow [constrained generation](https://huggingface.co/docs/text-generation-inference/conceptual/guidance) in order to force properly-formatted agent outputs.



================================================
FILE: docs/source/en/reference/tools.md
================================================
# Tools

<Tip warning={true}>

Smolagents is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.

</Tip>

To learn more about agents and tools make sure to read the [introductory guide](../index). This page
contains the API docs for the underlying classes.

## Tool Base Classes

### load_tool

[[autodoc]] load_tool

### tool

[[autodoc]] tool

### Tool

[[autodoc]] Tool

### launch_gradio_demo

[[autodoc]] launch_gradio_demo

## ToolCollection

[[autodoc]] ToolCollection

## MCP Client

[[autodoc]] smolagents.mcp_client.MCPClient

## Agent Types

Agents can handle any type of object in-between tools; tools, being completely multimodal, can accept and return
text, image, audio, video, among other types. In order to increase compatibility between tools, as well as to
correctly render these returns in ipython (jupyter, colab, ipython notebooks, ...), we implement wrapper classes
around these types.

The wrapped objects should continue behaving as initially; a text object should still behave as a string, an image
object should still behave as a `PIL.Image`.

These types have three specific purposes:

- Calling `to_raw` on the type should return the underlying object
- Calling `to_string` on the type should return the object as a string: that can be the string in case of an `AgentText`
  but will be the path of the serialized version of the object in other instances
- Displaying it in an ipython kernel should display the object correctly

### AgentText

[[autodoc]] smolagents.agent_types.AgentText

### AgentImage

[[autodoc]] smolagents.agent_types.AgentImage

### AgentAudio

[[autodoc]] smolagents.agent_types.AgentAudio



================================================
FILE: docs/source/en/tutorials/building_good_agents.md
================================================
# Building good agents

[[open-in-colab]]

There's a world of difference between building an agent that works and one that doesn't.
How can we build agents that fall into the former category?
In this guide, we're going to talk about best practices for building agents.

> [!TIP]
> If you're new to building agents, make sure to first read the [intro to agents](../conceptual_guides/intro_agents) and the [guided tour of smolagents](../guided_tour).

### The best agentic systems are the simplest: simplify the workflow as much as you can

Giving an LLM some agency in your workflow introduces some risk of errors.

Well-programmed agentic systems have good error logging and retry mechanisms anyway, so the LLM engine has a chance to self-correct their mistake. But to reduce the risk of LLM error to the maximum, you should simplify your workflow!

Let's revisit the example from the [intro to agents](../conceptual_guides/intro_agents): a bot that answers user queries for a surf trip company.
Instead of letting the agent do 2 different calls for "travel distance API" and "weather API" each time they are asked about a new surf spot, you could just make one unified tool "return_spot_information", a function that calls both APIs at once and returns their concatenated outputs to the user.

This will reduce costs, latency, and error risk!

The main guideline is: Reduce the number of LLM calls as much as you can.

This leads to a few takeaways:
- Whenever possible, group 2 tools in one, like in our example of the two APIs.
- Whenever possible, logic should be based on deterministic functions rather than agentic decisions.

### Improve the information flow to the LLM engine

Remember that your LLM engine is like an *intelligent* robot, trapped into a room with the only communication with the outside world being notes passed under a door.

It won't know of anything that happened if you don't explicitly put that into its prompt.

So first start with making your task very clear!
Since an agent is powered by an LLM, minor variations in your task formulation might yield completely different results.

Then, improve the information flow towards your agent in tool use.

Particular guidelines to follow:
- Each tool should log (by simply using `print` statements inside the tool's `forward` method) everything that could be useful for the LLM engine.
  - In particular, logging detail on tool execution errors would help a lot!

For instance, here's a tool that retrieves weather data based on location and date-time:

First, here's a poor version:
```python
import datetime
from smolagents import tool

def get_weather_report_at_coordinates(coordinates, date_time):
    # Dummy function, returns a list of [temperature in Â°C, risk of rain on a scale 0-1, wave height in m]
    return [28.0, 0.35, 0.85]

def convert_location_to_coordinates(location):
    # Returns dummy coordinates
    return [3.3, -42.0]

@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for.
        date_time: the date and time for which you want the report.
    """
    lon, lat = convert_location_to_coordinates(location)
    date_time = datetime.strptime(date_time)
    return str(get_weather_report_at_coordinates((lon, lat), date_time))
```

Why is it bad?
- there's no precision of the format that should be used for `date_time`
- there's no detail on how location should be specified.
- there's no logging mechanism trying to make explicit failure cases like location not being in a proper format, or date_time not being properly formatted.
- the output format is hard to understand

If the tool call fails, the error trace logged in memory can help the LLM reverse engineer the tool to fix the errors. But why leave it with so much heavy lifting to do?

A better way to build this tool would have been the following:
```python
@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for. Should be a place name, followed by possibly a city name, then a country, like "Anchor Point, Taghazout, Morocco".
        date_time: the date and time for which you want the report, formatted as '%m/%d/%y %H:%M:%S'.
    """
    lon, lat = convert_location_to_coordinates(location)
    try:
        date_time = datetime.strptime(date_time)
    except Exception as e:
        raise ValueError("Conversion of `date_time` to datetime format failed, make sure to provide a string in format '%m/%d/%y %H:%M:%S'. Full trace:" + str(e))
    temperature_celsius, risk_of_rain, wave_height = get_weather_report_at_coordinates((lon, lat), date_time)
    return f"Weather report for {location}, {date_time}: Temperature will be {temperature_celsius}Â°C, risk of rain is {risk_of_rain*100:.0f}%, wave height is {wave_height}m."
```

In general, to ease the load on your LLM, the good question to ask yourself is: "How easy would it be for me, if I was dumb and using this tool for the first time ever, to program with this tool and correct my own errors?".

### Give more arguments to the agent

To pass some additional objects to your agent beyond the simple string describing the task, you can use the `additional_args` argument to pass any type of object:

```py
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

agent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), add_base_tools=True)

agent.run(
    "Why does Mike not know many people in New York?",
    additional_args={"mp3_sound_file_url":'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3'}
)
```
For instance, you can use this `additional_args` argument to pass images or strings that you want your agent to leverage.



## How to debug your agent

### 1. Use a stronger LLM

In an agentic workflows, some of the errors are actual errors, some other are the fault of your LLM engine not reasoning properly.
For instance, consider this trace for an `CodeAgent` that I asked to create a car picture:
```
==================================================================================================== New task ====================================================================================================
Make me a cool car picture
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
image_generator(prompt="A cool, futuristic sports car with LED headlights, aerodynamic design, and vibrant color, high-res, photorealistic")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Step 1:

- Time taken: 16.35 seconds
- Input tokens: 1,383
- Output tokens: 77
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final_answer("/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Print outputs:

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Final answer:
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
```
The user sees, instead of an image being returned, a path being returned to them.
It could look like a bug from the system, but actually the agentic system didn't cause the error: it's just that the LLM brain did the mistake of not saving the image output into a variable.
Thus it cannot access the image again except by leveraging the path that was logged while saving the image, so it returns the path instead of an image.

The first step to debugging your agent is thus "Use a more powerful LLM". Alternatives like `Qwen2/5-72B-Instruct` wouldn't have made that mistake.

### 2. Provide more information or specific instructions

You can also use less powerful models, provided you guide them more effectively.

Put yourself in the shoes of your model: if you were the model solving the task, would you struggle with the information available to you (from the system prompt + task formulation + tool description) ?

Would you need detailed instructions?

- If the instruction is to always be given to the agent (as we generally understand a system prompt to work): you can pass it as a string under argument `instructions` upon agent initialization. *(Note: instructions are appended to the system prompt, not replacing it.)*
- If it's about a specific task to solve: add all these details to the task. The task could be very long, like dozens of pages.
- If it's about how to use specific tools: include it in the `description` attribute of these tools.


### 3. Change the prompt templates (generally not advised)

If above clarifications are not sufficient, you can change the agent's prompt templates.

Let's see how it works. For example, let us check the default prompt templates for the [`CodeAgent`] (below version is shortened by skipping zero-shot examples).

```python
print(agent.prompt_templates["system_prompt"])
```
Here is what you get:
```text
You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
To solve the task, you must plan forward to proceed in a series of steps, in a cycle of Thought, Code, and Observation sequences.

At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
Then in the Code sequence you should write the code in simple Python. The code sequence must be opened with '{{code_block_opening_tag}}', and closed with '{{code_block_closing_tag}}'.
During each intermediate step, you can use 'print()' to save whatever important information you will then need.
These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
In the end you have to return a final answer using the `final_answer` tool.

Here are a few examples using notional tools:
---
Task: "Generate an image of the oldest person in this document."

Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.
{{code_block_opening_tag}}
answer = document_qa(document=document, question="Who is the oldest person mentioned?")
print(answer)
{{code_block_closing_tag}}
Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

Thought: I will now generate an image showcasing the oldest person.
{{code_block_opening_tag}}
image = image_generator("A portrait of John Doe, a 55-year-old man living in Canada.")
final_answer(image)
{{code_block_closing_tag}}

---
Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

Thought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool
{{code_block_opening_tag}}
result = 5 + 3 + 1294.678
final_answer(result)
{{code_block_closing_tag}}

---
Task:
"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.
You have been provided with these additional arguments, that you can access using the keys as variables in your python code:
{'question': 'Quel est l'animal sur l'image?', 'image': 'path/to/image.jpg'}"

Thought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.
{{code_block_opening_tag}}
translated_question = translator(question=question, src_lang="French", tgt_lang="English")
print(f"The translated question is {translated_question}.")
answer = image_qa(image=image, question=translated_question)
final_answer(f"The answer is {answer}")
{{code_block_closing_tag}}

---
Task:
In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.
What does he say was the consequence of Einstein learning too much math on his creativity, in one word?

Thought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.
{{code_block_opening_tag}}
pages = web_search(query="1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein")
print(pages)
{{code_block_closing_tag}}
Observation:
No result found for query "1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein".

Thought: The query was maybe too restrictive and did not find any results. Let's try again with a broader query.
{{code_block_opening_tag}}
pages = web_search(query="1979 interview Stanislaus Ulam")
print(pages)
{{code_block_closing_tag}}
Observation:
Found 6 pages:
[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)

[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)

(truncated)

Thought: I will read the first 2 pages to know more.
{{code_block_opening_tag}}
for url in ["https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/", "https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/"]:
    whole_page = visit_webpage(url)
    print(whole_page)
    print("\n" + "="*80 + "\n")  # Print separator between pages
{{code_block_closing_tag}}
Observation:
Manhattan Project Locations:
Los Alamos, NM
Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at
(truncated)

Thought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: "He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity." Let's answer in one word.
{{code_block_opening_tag}}
final_answer("diminished")
{{code_block_closing_tag}}

---
Task: "Which city has the highest population: Guangzhou or Shanghai?"

Thought: I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.
{{code_block_opening_tag}}
for city in ["Guangzhou", "Shanghai"]:
    print(f"Population {city}:", web_search(f"{city} population")
{{code_block_closing_tag}}
Observation:
Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
Population Shanghai: '26 million (2019)'

Thought: Now I know that Shanghai has the highest population.
{{code_block_opening_tag}}
final_answer("Shanghai")
{{code_block_closing_tag}}

---
Task: "What is the current age of the pope, raised to the power 0.36?"

Thought: I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.
{{code_block_opening_tag}}
pope_age_wiki = wikipedia_search(query="current pope age")
print("Pope age as per wikipedia:", pope_age_wiki)
pope_age_search = web_search(query="current pope age")
print("Pope age as per google search:", pope_age_search)
{{code_block_closing_tag}}
Observation:
Pope age: "The pope Francis is currently 88 years old."

Thought: I know that the pope is 88 years old. Let's compute the result using python code.
{{code_block_opening_tag}}
pope_current_age = 88 ** 0.36
final_answer(pope_current_age)
{{code_block_closing_tag}}

Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:
{{code_block_opening_tag}}
{%- for tool in tools.values() %}
{{ tool.to_code_prompt() }}
{% endfor %}
{{code_block_closing_tag}}

{%- if managed_agents and managed_agents.values() | list %}
You can also give tasks to team members.
Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
You can also include any relevant variables or context using the 'additional_args' argument.
Here is a list of the team members that you can call:
{{code_block_opening_tag}}
{%- for agent in managed_agents.values() %}
def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
    """{{ agent.description }}

    Args:
        task: Long detailed description of the task.
        additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
    """
{% endfor %}
{{code_block_closing_tag}}
{%- endif %}

Here are the rules you should always follow to solve your task:
1. Always provide a 'Thought:' sequence, and a '{{code_block_opening_tag}}' sequence ending with '{{code_block_closing_tag}}', else you will fail.
2. Use only variables that you have defined!
3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = wikipedia_search(query="What is the place where James Bond lives?")'.
4. For tools WITHOUT JSON output schema: Take care to not chain too many sequential tool calls in the same code block, as their output format is unpredictable. For instance, a call to wikipedia_search without a JSON output schema has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.
5. For tools WITH JSON output schema: You can confidently chain multiple tool calls and directly access structured output fields in the same code block! When a tool has a JSON output schema, you know exactly what fields and data types to expect, allowing you to write robust code that directly accesses the structured response (e.g., result['field_name']) without needing intermediate print() statements.
6. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.
7. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.
8. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.
9. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}
10. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.
11. Don't give up! You're in charge of solving the task, not providing directions to solve it.

{%- if custom_instructions %}
{{custom_instructions}}
{%- endif %}

Now Begin!
```

As you can see, there are placeholders like `"{{ tool.description }}"`: these will be used upon agent initialization to insert certain automatically generated descriptions of tools or managed agents.

So while you can overwrite this system prompt template by passing your custom prompt as an argument to the `system_prompt` parameter, your new system prompt can contain the following placeholders:
- To insert tool descriptions:
  ```
  {%- for tool in tools.values() %}
  - {{ tool.to_tool_calling_prompt() }}
  {%- endfor %}
  ```
- To insert the descriptions for managed agents if there are any:
  ```
  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  {%- for agent in managed_agents.values() %}
  - {{ agent.name }}: {{ agent.description }}
  {%- endfor %}
  {%- endif %}
  ```
- For `CodeAgent` only, to insert the list of authorized imports: `"{{authorized_imports}}"`

Then you can change the system prompt as follows:

```py
agent.prompt_templates["system_prompt"] = agent.prompt_templates["system_prompt"] + "\nHere you go!"
```

This also works with the [`ToolCallingAgent`].

But generally it's just simpler to pass argument `instructions` upon agent initalization, like:
```py
agent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), instructions="Always talk like a 5 year old.")
```

Note that `instructions` are appended to the system prompt, not replacing it.


### 4. Extra planning

We provide a model for a supplementary planning step, that an agent can run regularly in-between normal action steps. In this step, there is no tool call, the LLM is simply asked to update a list of facts it knows and to reflect on what steps it should take next based on those facts.

```py
from smolagents import load_tool, CodeAgent, InferenceClientModel, WebSearchTool
from dotenv import load_dotenv

load_dotenv()

# Import tool from Hub
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

search_tool = WebSearchTool()

agent = CodeAgent(
    tools=[search_tool, image_generation_tool],
    model=InferenceClientModel(model_id="Qwen/Qwen2.5-72B-Instruct"),
    planning_interval=3 # This is where you activate planning!
)

# Run it!
result = agent.run(
    "How long would a cheetah at full speed take to run the length of Pont Alexandre III?",
)
```



================================================
FILE: docs/source/en/tutorials/inspect_runs.md
================================================
# Inspecting runs with OpenTelemetry

[[open-in-colab]]

> [!TIP]
> If you're new to building agents, make sure to first read the [intro to agents](../conceptual_guides/intro_agents) and the [guided tour of smolagents](../guided_tour).

## Why log your agent runs?

Agent runs are complicated to debug.

Validating that a run went properly is hard, since agent workflows are [unpredictable by design](../conceptual_guides/intro_agents) (if they were predictable, you'd just be using good old code). 

And inspecting a run is hard as well: multi-step agents tend to quickly fill a console with logs, and most of the errors are just "LLM dumb" kind of errors, from which the LLM auto-corrects in the next step by writing better code or tool calls.

So using instrumentation to record agent runs is necessary in production for later inspection and monitoring!

We've adopted the [OpenTelemetry](https://opentelemetry.io/) standard for instrumenting agent runs.

This means that you can just run some instrumentation code, then run your agents normally, and everything gets logged into your platform. Below are some examples of how to do this with different OpenTelemetry backends.

Here's how it then looks like on the platform:

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/inspect_run_phoenix.gif"/>
</div>


## Setting up telemetry with Arize AI Phoenix
First install the required packages. Here we install [Phoenix by Arize AI](https://github.com/Arize-ai/phoenix) because that's a good solution to collect and inspect the logs, but there are other OpenTelemetry-compatible platforms that you could use for this collection & inspection part.

```shell
pip install 'smolagents[telemetry,toolkit]'
```

Then run the collector in the background.

```shell
python -m phoenix.server.main serve
```

Finally, set up `SmolagentsInstrumentor` to trace your agents and send the traces to Phoenix default endpoint.

```python
from phoenix.otel import register
from openinference.instrumentation.smolagents import SmolagentsInstrumentor

register()
SmolagentsInstrumentor().instrument()
```
Then you can run your agents!

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    VisitWebpageTool,
    InferenceClientModel,
)

model = InferenceClientModel()

search_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="search_agent",
    description="This is an agent that can do web search.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[search_agent],
)
manager_agent.run(
    "If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?"
)
```
VoilÃ !
You can then navigate to `http://0.0.0.0:6006/projects/` to inspect your run!

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/inspect_run_phoenix.png">

You can see that the CodeAgent called its managed ToolCallingAgent (by the way, the managed agent could have been a CodeAgent as well) to ask it to run the web search for the U.S. 2024 growth rate. Then the managed agent returned its report and the manager agent acted upon it to calculate the economy doubling time! Sweet, isn't it?

## Setting up telemetry with ðŸª¢ Langfuse

This part shows how to monitor and debug your Hugging Face **smolagents** with **Langfuse** using the `SmolagentsInstrumentor`.

> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open-source platform for LLM engineering. It provides tracing and monitoring capabilities for AI agents, helping developers debug, analyze, and optimize their products. Langfuse integrates with various tools and frameworks via native integrations, OpenTelemetry, and SDKs.

### Step 1: Install Dependencies

```python
%pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents
```

### Step 2: Set Up Environment Variables

Set your Langfuse API keys and configure the OpenTelemetry endpoint to send traces to Langfuse. Get your Langfuse API keys by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting).

Also, add your [Hugging Face token](https://huggingface.co/settings/tokens) (`HF_TOKEN`) as an environment variable.

```python
import os
# Get keys for your project from the project settings page: https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..." 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º EU region
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ US region
 
# your Hugging Face token
os.environ["HF_TOKEN"] = "hf_..."
```

With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.

```python
from langfuse import get_client
 
langfuse = get_client()
 
# Verify connection
if langfuse.auth_check():
    print("Langfuse client is authenticated and ready!")
else:
    print("Authentication failed. Please check your credentials and host.")
```

### Step 3: Initialize the `SmolagentsInstrumentor`

Initialize the `SmolagentsInstrumentor` before your application code. 


```python
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 
SmolagentsInstrumentor().instrument()
```

### Step 4: Run your smolagent

```python
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    VisitWebpageTool,
    InferenceClientModel,
)

model = InferenceClientModel(
    model_id="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
)

search_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="search_agent",
    description="This is an agent that can do web search.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[search_agent],
)
manager_agent.run(
    "How can Langfuse be used to monitor and improve the reasoning and decision-making of smolagents when they execute multi-step tasks, like dynamically adjusting a recipe based on user feedback or available ingredients?"
)
```

### Step 5: View Traces in Langfuse

After running the agent, you can view the traces generated by your smolagents application in [Langfuse](https://cloud.langfuse.com). You should see detailed steps of the LLM interactions, which can help you debug and optimize your AI agent.

![smolagents example trace](https://langfuse.com/images/cookbook/integration-smolagents/smolagent_example_trace.png)

_[Public example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/ce5160f9bfd5a6cd63b07d2bfcec6f54?timestamp=2025-02-11T09%3A25%3A45.163Z&display=details)_



================================================
FILE: docs/source/en/tutorials/memory.md
================================================
# ðŸ“š Manage your agent's memory

[[open-in-colab]]

In the end, an agent can be defined by simple components: it has tools, prompts.
And most importantly, it has a memory of past steps, drawing a history of planning, execution, and errors.

### Replay your agent's memory

We propose several features to inspect a past agent run.

You can instrument the agent's run to display it in a great UI that lets you zoom in/out on specific steps, as highlighted in the [instrumentation guide](./inspect_runs).

You can also use `agent.replay()`, as follows:

After the agent has run:
```py
from smolagents import InferenceClientModel, CodeAgent

agent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=0)

result = agent.run("What's the 20th Fibonacci number?")
```

If you want to replay this last run, just use:
```py
agent.replay()
```

### Dynamically change the agent's memory

Many advanced use cases require dynamic modification of the agent's memory.

You can access the agent's memory using:

```py
from smolagents import ActionStep

system_prompt_step = agent.memory.system_prompt
print("The system prompt given to the agent was:")
print(system_prompt_step.system_prompt)

task_step = agent.memory.steps[0]
print("\n\nThe first task step was:")
print(task_step.task)

for step in agent.memory.steps:
    if isinstance(step, ActionStep):
        if step.error is not None:
            print(f"\nStep {step.step_number} got this error:\n{step.error}\n")
        else:
            print(f"\nStep {step.step_number} got these observations:\n{step.observations}\n")
```

Use `agent.memory.get_full_steps()` to get full steps as dictionaries.

You can also use step callbacks to dynamically change the agent's memory.

Step callbacks can access the `agent` itself in their arguments, so they can access any memory step as highlighted above, and change it if needed. For instance, let's say you are observing screenshots of each step performed by a web browser agent. You want to log the newest screenshot, and remove the images from ancient steps to save on token costs.

You could run something like the following.
_Note: this code is incomplete, some imports and object definitions have been removed for the sake of concision, visit [the original script](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py) to get the full working code._

```py
import helium
from PIL import Image
from io import BytesIO
from time import sleep

def update_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    driver = helium.get_driver()
    latest_step = memory_step.step_number
    for previous_memory_step in agent.memory.steps:  # Remove previous screenshots from logs for lean processing
        if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= latest_step - 2:
            previous_memory_step.observations_images = None
    png_bytes = driver.get_screenshot_as_png()
    image = Image.open(BytesIO(png_bytes))
    memory_step.observations_images = [image.copy()]
```

Then you should pass this function in the `step_callbacks` argument upon initialization of your agent:

```py
CodeAgent(
    tools=[WebSearchTool(), go_back, close_popups, search_item_ctrl_f],
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[update_screenshot],
    max_steps=20,
    verbosity_level=2,
)
```

Head to our [vision web browser code](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py) to see the full working example.

### Run agents one step at a time

This can be useful in case you have tool calls that take days: you can just run your agents step by step.
This will also let you update the memory on each step.

```py
from smolagents import InferenceClientModel, CodeAgent, ActionStep, TaskStep

agent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=1)
agent.python_executor.send_tools({**agent.tools})
print(agent.memory.system_prompt)

task = "What is the 20th Fibonacci number?"

# You could modify the memory as needed here by inputting the memory of another agent.
# agent.memory.steps = previous_agent.memory.steps

# Let's start a new task!
agent.memory.steps.append(TaskStep(task=task, task_images=[]))

final_answer = None
step_number = 1
while final_answer is None and step_number <= 10:
    memory_step = ActionStep(
        step_number=step_number,
        observations_images=[],
    )
    # Run one step.
    final_answer = agent.step(memory_step)
    agent.memory.steps.append(memory_step)
    step_number += 1

    # Change the memory as you please!
    # For instance to update the latest step:
    # agent.memory.steps[-1] = ...

print("The final answer is:", final_answer)
```



================================================
FILE: docs/source/en/tutorials/secure_code_execution.md
================================================
# Secure code execution

[[open-in-colab]]

> [!TIP]
> If you're new to building agents, make sure to first read the [intro to agents](../conceptual_guides/intro_agents) and the [guided tour of smolagents](../guided_tour).

### Code agents

[Multiple](https://huggingface.co/papers/2402.01030) [research](https://huggingface.co/papers/2411.01747) [papers](https://huggingface.co/papers/2401.00812) have shown that having the LLM write its actions (the tool calls) in code is much better than the current standard format for tool calling, which is across the industry different shades of "writing actions as a JSON of tools names and arguments to use".

Why is code better? Well, because we crafted our code languages specifically to be great at expressing actions performed by a computer. If JSON snippets were a better way, this package would have been written in JSON snippets and the devil would be laughing at us.

Code is just a better way to express actions on a computer. It has better:
- **Composability:** could you nest JSON actions within each other, or define a set of JSON actions to re-use later, the same way you could just define a python function?
- **Object management:** how do you store the output of an action like `generate_image` in JSON?
- **Generality:** code is built to express simply anything you can have a computer do.
- **Representation in LLM training corpus:** why not leverage this benediction of the sky that plenty of quality actions have already been included in LLM training corpus?

This is illustrated on the figure below, taken from [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030).

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png">

This is why we put emphasis on proposing code agents, in this case python agents, which meant putting higher effort on building secure python interpreters.

### Local code execution??

By default, the `CodeAgent` runs LLM-generated code in your environment.

This is inherently risky, LLM-generated code could be harmful to your environment.

Malicious code execution can occur in several ways:
- **Plain LLM error:** LLMs are still far from perfect and may unintentionally generate harmful commands while attempting to be helpful. While this risk is low, instances have been observed where an LLM attempted to execute potentially dangerous code.  
- **Supply chain attack:** Running an untrusted or compromised LLM could expose a system to harmful code generation. While this risk is extremely low when using well-known models on secure inference infrastructure, it remains a theoretical possibility.  
- **Prompt injection:** an agent browsing the web could arrive on a malicious website that contains harmful instructions, thus injecting an attack into the agent's memory
- **Exploitation of publicly accessible agents:** Agents exposed to the public can be misused by malicious actors to execute harmful code. Attackers may craft adversarial inputs to exploit the agent's execution capabilities, leading to unintended consequences.
Once malicious code is executed, whether accidentally or intentionally, it can damage the file system, exploit local or cloud-based resources, abuse API services, and even compromise network security.

One could argue that on the [spectrum of agency](../conceptual_guides/intro_agents), code agents give much higher agency to the LLM on your system than other less agentic setups: this goes hand-in-hand with higher risk.

So you need to be very mindful of security.

To improve safety, we propose a range of measures that propose elevated levels of security, at a higher setup cost.

We advise you to keep in mind that no solution will be 100% safe.

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/code_execution_safety_diagram.png">

### Our local Python executor

To add a first layer of security, code execution in `smolagents` is not performed by the vanilla Python interpreter.
We have re-built a more secure `LocalPythonExecutor` from the ground up.

To be precise, this interpreter works by loading the Abstract Syntax Tree (AST) from your Code and executes it operation by operation, making sure to always follow certain rules:
- By default, imports are disallowed unless they have been explicitly added to an authorization list by the user.
- Furthermore, access to submodules is disabled by default, and each must be explicitly authorized in the import list as well, or you can pass for instance `numpy.*` to allow both `numpy` and all its subpackags, like `numpy.random` or `numpy.a.b`.
   - Note that some seemingly innocuous packages like `random` can give access to potentially harmful submodules, as in `random._os`.
- The total count of elementary operations processed is capped to prevent infinite loops and resource bloating.
- Any operation that has not been explicitly defined in our custom interpreter will raise an error.

You could try these safeguards as follows:

```py
from smolagents.local_python_executor import LocalPythonExecutor

# Set up custom executor, authorize package "numpy"
custom_executor = LocalPythonExecutor(["numpy"])

# Utility for pretty printing errors
def run_capture_exception(command: str):
    try:
        custom_executor(harmful_command)
    except Exception as e:
        print("ERROR:\n", e)

# Undefined command just do not work
harmful_command="!echo Bad command"
run_capture_exception(harmful_command)
# >>> ERROR: invalid syntax (<unknown>, line 1)


# Imports like os will not be performed unless explicitly added to `additional_authorized_imports`
harmful_command="import os; exit_code = os.system('echo Bad command')"
run_capture_exception(harmful_command)
# >>> ERROR: Code execution failed at line 'import os' due to: InterpreterError: Import of os is not allowed. Authorized imports are: ['statistics', 'numpy', 'itertools', 'time', 'queue', 'collections', 'math', 'random', 're', 'datetime', 'stat', 'unicodedata']

# Even in authorized imports, potentially harmful packages will not be imported
harmful_command="import random; random._os.system('echo Bad command')"
run_capture_exception(harmful_command)
# >>> ERROR: Code execution failed at line 'random._os.system('echo Bad command')' due to: InterpreterError: Forbidden access to module: os

# Infinite loop are interrupted after N operations
harmful_command="""
while True:
    pass
"""
run_capture_exception(harmful_command)
# >>> ERROR: Code execution failed at line 'while True: pass' due to: InterpreterError: Maximum number of 1000000 iterations in While loop exceeded
```

These safeguards make out interpreter is safer.
We have used it on a diversity of use cases, without ever observing any damage to the environment.

> [!WARNING]
> It's important to understand that no local python sandbox can ever be completely secure. While our interpreter provides significant safety improvements over the standard Python interpreter, it is still possible for a determined attacker or a fine-tuned malicious LLM to find vulnerabilities and potentially harm your environment. 
> 
> For example, if you've allowed packages like `Pillow` to process images, the LLM could generate code that creates thousands of large image files to fill your hard drive. Other advanced escape techniques might exploit deeper vulnerabilities in authorized packages.
> 
> Running LLM-generated code in your local environment always carries some inherent risk. The only way to run LLM-generated code with truly robust security isolation is to use remote execution options like E2B or Docker, as detailed below.

The risk of a malicious attack is low when using well-known LLMs from trusted inference providers, but it is not zero.
For high-security applications or when using less trusted models, you should consider using a remote execution sandbox.

## Sandbox approaches for secure code execution

When working with AI agents that execute code, security is paramount. There are two main approaches to sandboxing code execution in smolagents, each with different security properties and capabilities:


![Sandbox approaches comparison](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/sandboxed_execution.png)

1. **Running individual code snippets in a sandbox**: This approach (left side of diagram) only executes the agent-generated Python code snippets in a sandbox while keeping the rest of the agentic system in your local environment. It's simpler to set up using `executor_type="blaxel"`, `executor_type="e2b"`, `executor_type="modal"`, or
`executor_type="docker"`, but it doesn't support multi-agents and still requires passing state data between your environment and the sandbox.

2. **Running the entire agentic system in a sandbox**: This approach (right side of diagram) runs the entire agentic system, including the agent, model, and tools, within a sandbox environment. This provides better isolation but requires more manual setup and may require passing sensitive credentials (like API keys) to the sandbox environment.

This guide describes how to set up and use both types of sandbox approaches for your agent applications.

### Blaxel setup

#### Installation

1. Create a Blaxel account at [blaxel.ai](https://blaxel.ai)
2. Install the required packages:
```bash
pip install 'smolagents[blaxel]'
```

#### Running your agent with Blaxel: quick start

We provide a simple way to use a Blaxel Sandbox: simply add `executor_type="blaxel"` to the agent initialization, as follows:

```py
from smolagents import InferenceClientModel, CodeAgent

with CodeAgent(model=InferenceClientModel(), tools=[], executor_type="blaxel") as agent:
    agent.run("Can you give me the 100th Fibonacci number?")
```

> [!TIP]
> Using the agent as a context manager (with the `with` statement) ensures that the Blaxel sandbox is cleaned up immediately after the agent completes its task.
> Alternatively, you can manually call the agent's `cleanup()` method.

This solution sends the agent state to the server at the start of each `agent.run()`.
Then the models are called from the local environment, but the generated code will be sent to the sandbox for execution, and only the output will be returned.

Blaxel provides fast-launching virtual machines that start from hibernation in under 25ms and scale back to zero after inactivity while maintaining memory state, making it an excellent choice for agent applications that require quick, secure code execution.

> [!TIP]
> For even stronger security isolation, you can host your entire agent remotely on Blaxel. This provides complete sandboxing of the agent, model, and tools. See the [Blaxel agent hosting documentation](https://docs.blaxel.ai/Agents/Develop-an-agent-py) for details.

### E2B setup

#### Installation

1. Create an E2B account at [e2b.dev](https://e2b.dev)
2. Install the required packages:
```bash
pip install 'smolagents[e2b]'
```

#### Running your agent in E2B: quick start

We provide a simple way to use an E2B Sandbox: simply add `executor_type="e2b"` to the agent initialization, as follows:

```py
from smolagents import InferenceClientModel, CodeAgent

with CodeAgent(model=InferenceClientModel(), tools=[], executor_type="e2b") as agent:
    agent.run("Can you give me the 100th Fibonacci number?")
```

> [!TIP]
> Using the agent as a context manager (with the `with` statement) ensures that the E2B sandbox is cleaned up immediately after the agent completes its task.
> Alternatively, you can manually call the agent's `cleanup()` method.

This solution send the agent state to the server at the start of each `agent.run()`.
Then the models are called from the local environment, but the generated code will be sent to the sandbox for execution, and only the output will be returned.

This is illustrated in the figure below.

<p align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/sandboxed_execution.png" alt="sandboxed code execution" width=60% max-width=500px>
</p>

However, since any call to a [managed agent](../examples/multiagents) would require model calls, since we do not transfer secrets to the remote sandbox, the model call would lack credentials.
Hence this solution does not work (yet) with more complicated multi-agent setups.

#### Running your agent in E2B: multi-agents

To use multi-agents in an E2B sandbox, you need to run your agents completely from within E2B.

Here is how to do it:

```python
from e2b_code_interpreter import Sandbox
import os

# Create the sandbox
sandbox = Sandbox()

# Install required packages
sandbox.commands.run("pip install smolagents")

def run_code_raise_errors(sandbox, code: str, verbose: bool = False) -> str:
    execution = sandbox.run_code(
        code,
        envs={'HF_TOKEN': os.getenv('HF_TOKEN')}
    )
    if execution.error:
        execution_logs = "\n".join([str(log) for log in execution.logs.stdout])
        logs = execution_logs
        logs += execution.error.traceback
        raise ValueError(logs)
    return "\n".join([str(log) for log in execution.logs.stdout])

# Define your agent application
agent_code = """
import os
from smolagents import CodeAgent, InferenceClientModel

# Initialize the agents
agent = CodeAgent(
    model=InferenceClientModel(token=os.getenv("HF_TOKEN"), provider="together"),
    tools=[],
    name="coder_agent",
    description="This agent takes care of your difficult algorithmic problems using code."
)

manager_agent = CodeAgent(
    model=InferenceClientModel(token=os.getenv("HF_TOKEN"), provider="together"),
    tools=[],
    managed_agents=[agent],
)

# Run the agent
response = manager_agent.run("What's the 20th Fibonacci number?")
print(response)
"""

# Run the agent code in the sandbox
execution_logs = run_code_raise_errors(sandbox, agent_code)
print(execution_logs)
```

### Modal setup

#### Installation

1. Create a Modal account at [modal.com](https://modal.com/signup)
2. Install the required packages:
```bash
pip install 'smolagents[modal]'
```

#### Running your agent in Modal: quick start

We provide a simple way to use a Modal Sandbox: simply add `executor_type="modal"` to the agent initialization, as follows:

```py
from smolagents import InferenceClientModel, CodeAgent

with CodeAgent(model=InferenceClientModel(), tools=[], executor_type="modal") as agent:
    agent.run("What is the 42th Fibonacci number?")
```

> [!TIP]
> Using the agent as a context manager (with the `with` statement) ensures that the Modal sandbox is cleaned immediately after the agent completes its task.
> Alternatively, you can manually call the agent's `cleanup()` method.

The agent state and generated code from the `InferenceClientModel` are sent to a Modal sandbox, which can securely execute code inside them.

### Docker setup

#### Installation

1. [Install Docker on your system](https://docs.docker.com/get-started/get-docker/)
2. Install the required packages:
```bash
pip install 'smolagents[docker]'
```

#### Running your agent in Docker: quick start

Similar to the E2B Sandbox above, to quickly get started with Docker, simply add `executor_type="docker"` to the agent initialization, like:

```py
from smolagents import InferenceClientModel, CodeAgent

with CodeAgent(model=InferenceClientModel(), tools=[], executor_type="docker") as agent:
    agent.run("Can you give me the 100th Fibonacci number?")
```

> [!TIP]
> Using the agent as a context manager (with the `with` statement) ensures that the Docker container is cleaned immediately after the agent completes its task.
> Alternatively, you can manually call the agent's `cleanup()` method.

#### Advanced docker usage

If you want to run multi-agent systems in Docker, you'll need to setup a custom interpreter in a sandbox.

Here is how to setup the a Dockerfile:

```dockerfile
FROM python:3.10-bullseye

# Install build dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        build-essential \
        python3-dev && \
    pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir smolagents && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Run with limited privileges
USER nobody

# Default command
CMD ["python", "-c", "print('Container ready')"]
```

Create a sandbox manager to run code:

```python
import docker
import os
from typing import Optional

class DockerSandbox:
    def __init__(self):
        self.client = docker.from_env()
        self.container = None

    def create_container(self):
        try:
            image, build_logs = self.client.images.build(
                path=".",
                tag="agent-sandbox",
                rm=True,
                forcerm=True,
                buildargs={},
                # decode=True
            )
        except docker.errors.BuildError as e:
            print("Build error logs:")
            for log in e.build_log:
                if 'stream' in log:
                    print(log['stream'].strip())
            raise

        # Create container with security constraints and proper logging
        self.container = self.client.containers.run(
            "agent-sandbox",
            command="tail -f /dev/null",  # Keep container running
            detach=True,
            tty=True,
            mem_limit="512m",
            cpu_quota=50000,
            pids_limit=100,
            security_opt=["no-new-privileges"],
            cap_drop=["ALL"],
            environment={
                "HF_TOKEN": os.getenv("HF_TOKEN")
            },
        )

    def run_code(self, code: str) -> Optional[str]:
        if not self.container:
            self.create_container()

        # Execute code in container
        exec_result = self.container.exec_run(
            cmd=["python", "-c", code],
            user="nobody"
        )

        # Collect all output
        return exec_result.output.decode() if exec_result.output else None


    def cleanup(self):
        if self.container:
            try:
                self.container.stop()
            except docker.errors.NotFound:
                # Container already removed, this is expected
                pass
            except Exception as e:
                print(f"Error during cleanup: {e}")
            finally:
                self.container = None  # Clear the reference

# Example usage:
sandbox = DockerSandbox()

try:
    # Define your agent code
    agent_code = """
import os
from smolagents import CodeAgent, InferenceClientModel

# Initialize the agent
agent = CodeAgent(
    model=InferenceClientModel(token=os.getenv("HF_TOKEN"), provider="together"),
    tools=[]
)

# Run the agent
response = agent.run("What's the 20th Fibonacci number?")
print(response)
"""

    # Run the code in the sandbox
    output = sandbox.run_code(agent_code)
    print(output)

finally:
    sandbox.cleanup()
```

### WebAssembly setup

WebAssembly (Wasm) is a binary instruction format that allows code to be run in a safe, sandboxed environment.
It is designed to be fast, efficient, and secure, making it an excellent choice for executing potentially untrusted code.

The `WasmExecutor` uses [Pyodide](https://pyodide.org/) and [Deno](https://docs.deno.com/).

#### Installation

1. [Install Deno on your system](https://docs.deno.com/runtime/getting_started/installation/)

#### Running your agent in WebAssembly: quick start

Simply pass `executor_type="wasm"` to the agent initialization, like:
```py
from smolagents import InferenceClientModel, CodeAgent

agent = CodeAgent(model=InferenceClientModel(), tools=[], executor_type="wasm")

agent.run("Can you give me the 100th Fibonacci number?")
```

### Best practices for sandboxes

These key practices apply to Blaxel, E2B, and Docker sandboxes:

- Resource management
  - Set memory and CPU limits
  - Implement execution timeouts
  - Monitor resource usage
- Security
  - Run with minimal privileges
  - Disable unnecessary network access
  - Use environment variables for secrets
- Environment
  - Keep dependencies minimal
  - Use fixed package versions
  - If you use base images, update them regularly

- Cleanup
  - Always ensure proper cleanup of resources, especially for Docker containers, to avoid having dangling containers eating up resources.

âœ¨ By following these practices and implementing proper cleanup procedures, you can ensure your agent runs safely and efficiently in a sandboxed environment.

## Comparing security approaches

As illustrated in the diagram earlier, both sandboxing approaches have different security implications:

### Approach 1: Running just the code snippets in a sandbox
- **Pros**: 
  - Easier to set up with a simple parameter (`executor_type="blaxel"`, `executor_type="e2b"`, or `executor_type="docker"`)
  - No need to transfer API keys to the sandbox
  - Better protection for your local environment
  - Fast execution with Blaxel's hibernation technology (<25ms startup)
- **Cons**:
  - Doesn't support multi-agents (managed agents)
  - Still requires transferring state between your environment and the sandbox
  - Limited to specific code execution

### Approach 2: Running the entire agentic system in a sandbox
- **Pros**:
  - Supports multi-agents
  - Complete isolation of the entire agent system
  - More flexible for complex agent architectures
- **Cons**:
  - Requires more manual setup
  - May require transferring sensitive API keys to the sandbox
  - Potentially higher latency due to more complex operations

Choose the approach that best balances your security needs with your application's requirements. For most applications with simpler agent architectures, Approach 1 provides a good balance of security and ease of use. For more complex multi-agent systems where you need full isolation, Approach 2, while more involved to set up, offers better security guarantees.



================================================
FILE: docs/source/en/tutorials/tools.md
================================================
# Tools

[[open-in-colab]]

Here, we're going to see advanced tool usage.

> [!TIP]
> If you're new to building agents, make sure to first read the [intro to agents](../conceptual_guides/intro_agents) and the [guided tour of smolagents](../guided_tour).


### What is a tool, and how to build one?

A tool is mostly a function that an LLM can use in an agentic system.

But to use it, the LLM will need to be given an API: name, tool description, input types and descriptions, output type.

So it cannot be only a function. It should be a class.

So at core, the tool is a class that wraps a function with metadata that helps the LLM understand how to use it.

Here's how it looks:

```python
from smolagents import Tool

class HFModelDownloadsTool(Tool):
    name = "model_download_counter"
    description = """
    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
    It returns the name of the checkpoint."""
    inputs = {
        "task": {
            "type": "string",
            "description": "the task category (such as text-classification, depth-estimation, etc)",
        }
    }
    output_type = "string"

    def forward(self, task: str):
        from huggingface_hub import list_models

        model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
        return model.id

model_downloads_tool = HFModelDownloadsTool()
```

The custom tool subclasses [`Tool`] to inherit useful methods. The child class also defines:
- An attribute `name`, which corresponds to the name of the tool itself. The name usually describes what the tool does. Since the code returns the model with the most downloads for a task, let's name it `model_download_counter`.
- An attribute `description` is used to populate the agent's system prompt.
- An `inputs` attribute, which is a dictionary with keys `"type"` and `"description"`. It contains information that helps the Python interpreter make educated choices about the input.
- An `output_type` attribute, which specifies the output type. The types for both `inputs` and `output_type` should be [Pydantic formats](https://docs.pydantic.dev/latest/concepts/json_schema/#generating-json-schema), they can be either of these: `["string", "boolean","integer", "number", "image", "audio", "array", "object", "any", "null"]`.
- A `forward` method which contains the inference code to be executed.

And that's all it needs to be used in an agent!

There's another way to build a tool. In the [guided_tour](../guided_tour), we implemented a tool using the `@tool` decorator. The [`tool`] decorator is the recommended way to define simple tools, but sometimes you need more than this: using several methods in a class for more clarity, or using additional class attributes.

In this case, you can build your tool by subclassing [`Tool`] as described above.

### Share your tool to the Hub

You can share your custom tool to the Hub as a Space repository by calling [`~Tool.push_to_hub`] on the tool. Make sure you've created a repository for it on the Hub and are using a token with read access.

```python
model_downloads_tool.push_to_hub("{your_username}/hf-model-downloads", token="<YOUR_HUGGINGFACEHUB_API_TOKEN>")
```

For the push to Hub to work, your tool will need to respect some rules:
- All methods are self-contained, e.g. use variables that come either from their args.
- As per the above point, **all imports should be defined directly within the tool's functions**, else you will get an error when trying to call [`~Tool.save`] or [`~Tool.push_to_hub`] with your custom tool.
- If you subclass the `__init__` method, you can give it no other argument than `self`. This is because arguments set during a specific tool instance's initialization are hard to track, which prevents from sharing them properly to the hub. And anyway, the idea of making a specific class is that you can already set class attributes for anything you need to hard-code (just set `your_variable=(...)` directly under the `class YourTool(Tool):` line). And of course you can still create a class attribute anywhere in your code by assigning stuff to `self.your_variable`.


Once your tool is pushed to Hub, you can visualize it. [Here](https://huggingface.co/spaces/m-ric/hf-model-downloads) is the `model_downloads_tool` that I've pushed. It has a nice gradio interface.

When diving into the tool files, you can find that all the tool's logic is under [tool.py](https://huggingface.co/spaces/m-ric/hf-model-downloads/blob/main/tool.py). That is where you can inspect a tool shared by someone else.

Then you can load the tool with [`load_tool`] or create it with [`~Tool.from_hub`] and pass it to the `tools` parameter in your agent.
Since running tools means running custom code, you need to make sure you trust the repository, thus we require to pass `trust_remote_code=True` to load a tool from the Hub.

```python
from smolagents import load_tool, CodeAgent

model_download_tool = load_tool(
    "{your_username}/hf-model-downloads",
    trust_remote_code=True
)
```

### Use tools from an MCP server

Our `MCPClient` allows you to load tools from an MCP server, and gives you full control over the connection and tool management:

For stdio-based MCP servers:
```python
from smolagents import MCPClient, CodeAgent
from mcp import StdioServerParameters
import os

server_parameters = StdioServerParameters(
    command="uvx",  # Using uvx ensures dependencies are available
    args=["--quiet", "pubmedmcp@0.1.3"],
    env={"UV_PYTHON": "3.12", **os.environ},
)

with MCPClient(server_parameters) as tools:
    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)
    agent.run("Please find the latest research on COVID-19 treatment.")
```

For Streamable HTTP-based MCP servers:
```python
from smolagents import MCPClient, CodeAgent

with MCPClient({"url": "http://127.0.0.1:8000/mcp", "transport": "streamable-http"}) as tools:
    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)
    agent.run("Please find a remedy for hangover.")
```

You can also manually manage the connection lifecycle with the try...finally pattern:

```python
from smolagents import MCPClient, CodeAgent
from mcp import StdioServerParameters
import os

# Initialize server parameters
server_parameters = StdioServerParameters(
    command="uvx",
    args=["--quiet", "pubmedmcp@0.1.3"],
    env={"UV_PYTHON": "3.12", **os.environ},
)

# Manually manage the connection
try:
    mcp_client = MCPClient(server_parameters)
    tools = mcp_client.get_tools()

    # Use the tools with your agent
    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)
    result = agent.run("What are the recent therapeutic approaches for Alzheimer's disease?")

    # Process the result as needed
    print(f"Agent response: {result}")
finally:
    # Always ensure the connection is properly closed
    mcp_client.disconnect()
```

You can also connect to multiple MCP servers at once by passing a list of server parameters:
```python
from smolagents import MCPClient, CodeAgent
from mcp import StdioServerParameters
import os

server_params1 = StdioServerParameters(
    command="uvx",
    args=["--quiet", "pubmedmcp@0.1.3"],
    env={"UV_PYTHON": "3.12", **os.environ},
)

server_params2 = {"url": "http://127.0.0.1:8000/sse"}

with MCPClient([server_params1, server_params2]) as tools:
    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)
    agent.run("Please analyze the latest research and suggest remedies for headaches.")
```

> [!WARNING]
> **Security Warning:** Always verify the source and integrity of any MCP server before connecting to it, especially for production environments.
> Using MCP servers comes with security risks:
> - **Trust is essential:** Only use MCP servers from trusted sources. Malicious servers can execute harmful code on your machine.
> - **Stdio-based MCP servers** will always execute code on your machine (that's their intended functionality).
> - **Streamable HTTP-based MCP servers:** While remote MCP servers will not execute code on your machine, still proceed with caution.

#### Structured Output and Output Schema Support

The latest [MCP specifications (2025-06-18+)](https://modelcontextprotocol.io/specification/2025-06-18/server/tools#structured-content) include support for `outputSchema`, which enables tools to return structured data with defined schemas. `smolagents` takes advantage of these structured output capabilities, allowing agents to work with tools that return complex data structures, JSON objects, and other structured formats. With this feature, the agent's LLMs can "see" the structure of the tool output before calling a tool, enabling more intelligent and context-aware interactions.

To enable structured output support, pass `structured_output=True` when initializing the `MCPClient`:

```python
from smolagents import MCPClient, CodeAgent

# Enable structured output support
with MCPClient(server_parameters, structured_output=True) as tools:
    agent = CodeAgent(tools=tools, model=model, add_base_tools=True)
    agent.run("Get weather information for Paris")
```

When `structured_output=True`, the following features are enabled:
- **Output Schema Support**: Tools can define JSON schemas for their outputs
- **Structured Content Handling**: Support for `structuredContent` in MCP responses
- **JSON Parsing**: Automatic parsing of structured data from tool responses

Here's an example using a weather MCP server with structured output:

```python
# demo/weather.py - Example MCP server with structured output
from pydantic import BaseModel, Field
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather Service")

class WeatherInfo(BaseModel):
    location: str = Field(description="The location name")
    temperature: float = Field(description="Temperature in Celsius")
    conditions: str = Field(description="Weather conditions")
    humidity: int = Field(description="Humidity percentage", ge=0, le=100)

@mcp.tool(
    name="get_weather_info",
    description="Get weather information for a location as structured data.",
    # structured_output=True is enabled by default in FastMCP
)
def get_weather_info(city: str) -> WeatherInfo:
    """Get weather information for a city."""
    return WeatherInfo(
        location=city,
        temperature=22.5,
        conditions="partly cloudy",
        humidity=65
    )
```

Agent using output schema and structured output:

```python
from smolagents import MCPClient, CodeAgent

# Using the weather server with structured output
from mcp import StdioServerParameters

server_parameters = StdioServerParameters(
    command="python",
    args=["demo/weather.py"]
)

with MCPClient(server_parameters, structured_output=True) as tools:
    agent = CodeAgent(tools=tools, model=model)
    result = agent.run("What is the temperature in Tokyo in Fahrenheit?")
    print(result)
```

When structured output is enabled, the `CodeAgent` system prompt is enhanced to include JSON schema information for tools, helping the agent understand the expected structure of tool outputs and access the data appropriately.

**Backwards Compatibility**: The `structured_output` parameter currently defaults to `False` to maintain backwards compatibility. Existing code will continue to work without changes, receiving simple text outputs as before.

**Future Change**: In a future release, the default value of `structured_output` will change from `False` to `True`. It is recommended to explicitly set `structured_output=True` to opt into the enhanced functionality, which provides better tool output handling and improved agent performance. Use `structured_output=False` only if you specifically need to maintain the current text-only behavior.

### Import a Space as a tool

You can directly import a Gradio Space from the Hub as a tool using the [`Tool.from_space`] method!

You only need to provide the id of the Space on the Hub, its name, and a description that will help your agent understand what the tool does. Under the hood, this will use [`gradio-client`](https://pypi.org/project/gradio-client/) library to call the Space.

For instance, let's import the [FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) Space from the Hub and use it to generate an image.

```python
image_generation_tool = Tool.from_space(
    "black-forest-labs/FLUX.1-schnell",
    name="image_generator",
    description="Generate an image from a prompt"
)

image_generation_tool("A sunny beach")
```
And voilÃ , here's your image! ðŸ–ï¸

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sunny_beach.webp">

Then you can use this tool just like any other tool.  For example, let's improve the prompt `a rabbit wearing a space suit` and generate an image of it. This example also shows how you can pass additional arguments to the agent.

```python
from smolagents import CodeAgent, InferenceClientModel

model = InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking")
agent = CodeAgent(tools=[image_generation_tool], model=model)

agent.run(
    "Improve this prompt, then generate an image of it.", additional_args={'user_prompt': 'A rabbit wearing a space suit'}
)
```

```text
=== Agent thoughts:
improved_prompt could be "A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background"

Now that I have improved the prompt, I can use the image generator tool to generate an image based on this prompt.
>>> Agent is executing the code below:
image = image_generator(prompt="A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background")
final_answer(image)
```

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit_spacesuit_flux.webp">

How cool is this? ðŸ¤©

### Use LangChain tools

We love Langchain and think it has a very compelling suite of tools.
To import a tool from LangChain, use the `from_langchain()` method.

Here is how you can use it to recreate the intro's search result using a LangChain web search tool.
This tool will need `pip install langchain google-search-results -q` to work properly.
```python
from langchain.agents import load_tools

search_tool = Tool.from_langchain(load_tools(["serpapi"])[0])

agent = CodeAgent(tools=[search_tool], model=model)

agent.run("How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?")
```

### Manage your agent's toolbox

You can manage an agent's toolbox by adding or replacing a tool in attribute `agent.tools`, since it is a standard dictionary.

Let's add the `model_download_tool` to an existing agent initialized with only the default toolbox.

```python
from smolagents import InferenceClientModel

model = InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking")

agent = CodeAgent(tools=[], model=model, add_base_tools=True)
agent.tools[model_download_tool.name] = model_download_tool
```
Now we can leverage the new tool:

```python
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?"
)
```


> [!TIP]
> Beware of not adding too many tools to an agent: this can overwhelm weaker LLM engines.


### Use a collection of tools

You can leverage tool collections by using [`ToolCollection`]. It supports loading either a collection from the Hub or an MCP server tools.


#### Tool Collection from any MCP server

Leverage tools from the hundreds of MCP servers available on [glama.ai](https://glama.ai/mcp/servers) or [smithery.ai](https://smithery.ai/).

The MCP servers tools can be loaded with [`ToolCollection.from_mcp`].

> [!WARNING]
> **Security Warning:** Always verify the source and integrity of any MCP server before connecting to it, especially for production environments.
> Using MCP servers comes with security risks:
> - **Trust is essential:** Only use MCP servers from trusted sources. Malicious servers can execute harmful code on your machine.
> - **Stdio-based MCP servers** will always execute code on your machine (that's their intended functionality).
> - **Streamable HTTP-based MCP servers:** While remote MCP servers will not execute code on your machine, still proceed with caution.

For stdio-based MCP servers, pass the server parameters as an instance of `mcp.StdioServerParameters`:
```py
from smolagents import ToolCollection, CodeAgent
from mcp import StdioServerParameters

server_parameters = StdioServerParameters(
    command="uvx",
    args=["--quiet", "pubmedmcp@0.1.3"],
    env={"UV_PYTHON": "3.12", **os.environ},
)

with ToolCollection.from_mcp(server_parameters, trust_remote_code=True) as tool_collection:
    agent = CodeAgent(tools=[*tool_collection.tools], model=model, add_base_tools=True)
    agent.run("Please find a remedy for hangover.")
```

To enable structured output support with ToolCollection, add the `structured_output=True` parameter:
```py
with ToolCollection.from_mcp(server_parameters, trust_remote_code=True, structured_output=True) as tool_collection:
    agent = CodeAgent(tools=[*tool_collection.tools], model=model, add_base_tools=True)
    agent.run("Please find a remedy for hangover.")
```

For Streamable HTTP-based MCP servers, simply pass a dict with parameters to `mcp.client.streamable_http.streamablehttp_client` and add the key `transport` with the value `"streamable-http"`:
```py
from smolagents import ToolCollection, CodeAgent

with ToolCollection.from_mcp({"url": "http://127.0.0.1:8000/mcp", "transport": "streamable-http"}, trust_remote_code=True) as tool_collection:
    agent = CodeAgent(tools=[*tool_collection.tools], add_base_tools=True)
    agent.run("Please find a remedy for hangover.")
```

#### Tool Collection from a collection in the Hub

You can leverage it with the slug of the collection you want to use.
Then pass them as a list to initialize your agent, and start using them!

```py
from smolagents import ToolCollection, CodeAgent

image_tool_collection = ToolCollection.from_hub(
    collection_slug="huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f",
    token="<YOUR_HUGGINGFACEHUB_API_TOKEN>"
)
agent = CodeAgent(tools=[*image_tool_collection.tools], model=model, add_base_tools=True)

agent.run("Please draw me a picture of rivers and lakes.")
```

To speed up the start, tools are loaded only if called by the agent.




================================================
FILE: docs/source/es/_config.py
================================================
# docstyle-ignore
INSTALL_CONTENT = """
# Installation
! pip install smolagents
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/smolagents.git
"""

notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
black_avoid_patterns = {
    "{processor_class}": "FakeProcessorClass",
    "{model_class}": "FakeModelClass",
    "{object_class}": "FakeObjectClass",
}



================================================
FILE: docs/source/es/_toctree.yml
================================================
- title: Primeros Pasos
  sections:
  - local: index
    title: IntroducciÃ³n
  - local: installation
    title: Opciones de instalaciÃ³n
#   - local: guided_tour
#     title: Guided tour
# - title: Tutorials
#   sections:
#   - local: tutorials/building_good_agents
#     title: âœ¨ Building good agents
#   - local: tutorials/inspect_runs
#     title: ðŸ“Š Inspect your agent runs using telemetry
#   - local: tutorials/tools
#     title: ðŸ› ï¸ Tools - in-depth guide
#   - local: tutorials/secure_code_execution
#     title: ðŸ›¡ï¸ Secure code execution
#   - local: tutorials/memory
#     title: ðŸ“š Manage your agent's memory
# - title: Conceptual guides
#   sections:
#   - local: conceptual_guides/intro_agents
#     title: ðŸ¤– What are agents?
#   - local: conceptual_guides/react
#     title: ðŸ¤” How do Multi-step agents work?
# - title: Examples
#   sections:
#   - local: examples/text_to_sql
#     title: Self-correcting Text-to-SQL
#   - local: examples/rag
#     title: Master your knowledge base with agentic RAG
#   - local: examples/multiagents
#     title: Orchestrate a multi-agent system
#   - local: examples/web_browser
#     title: Build a web browser agent using vision models
#   - local: examples/using_different_models
#     title: Using different models
#   - local: examples/plan_customization
#     title: "Human-in-the-Loop: Customize agent plan interactively"
#   - local: examples/async_agent
#     title: Async Applications with Agents
# - title: Reference
#   sections:
#   - local: reference/agents
#     title: Agent-related objects
#   - local: reference/models
#     title: Model-related objects
#   - title: Tools
#     sections:
#     - title: Tool-related objects
#       local: reference/tools
#     - title: Built-in Tools
#       local: reference/default_tools



================================================
FILE: docs/source/es/index.md
================================================
# `smolagents`

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/license_to_call.png" style="max-width:700px"/>
</div>

## Â¿QuÃ© es smolagents?

`smolagents` es una biblioteca de cÃ³digo abierto en Python, diseÃ±ada para facilitar al mÃ¡ximo la construcciÃ³n y ejecuciÃ³n de agentes con solo unas pocas lÃ­neas de cÃ³digo.

Algunos aspectos clave de `smolagents` incluyen:

âœ¨ **Simplicidad**: La lÃ³gica de los agentes se implementa en aproximadamente unas mil lÃ­neas de cÃ³digo. Â¡Lo hemos mantenido simple, sin agregar complejidad innecesaria!

ðŸ§‘â€ðŸ’» **Soporte avanzado para Agentes de CÃ³digo**: [`CodeAgent`](reference/agents#smolagents.CodeAgent) ejecuta acciones directamente en cÃ³digo (en lugar de que los agentes generen cÃ³digo), lo que permite usar varias herramientas o realizar cÃ¡lculos de manera flexible. Esto hace posible combinar de manera sencilla funciones anidadas, bucles, condicionales y mucho mÃ¡s. Para garantizar la seguridad, el agente puede [ejecutarse en un entorno aislado](tutorials/secure_code_execution) usando [E2B](https://e2b.dev/) o Docker.

ðŸ“¡ **IntegraciÃ³n nativa con agentes de herramientas**: ademÃ¡s de los CodeAgent,  [`ToolCallingAgent`](reference/agents#smolagents.ToolCallingAgent) es compatible con el esquema tradicional basado en JSON/texto para casos en los que se prefiera este formato.

ðŸ¤— **Integraciones con el Hub**: mediante Gradio Spaces es posible compartir y cargar mÃºltiples agentes junto con herramientas desde o hacia el Hub de manera sencilla.

ðŸŒ **Independencia respecto al modelo**: integra fÃ¡cilmente grandes modelos de lenguaje (LLM) alojados en el Hub mediante los [proveedores de inferencia](https://huggingface.co/docs/inference-providers/index), APIs externas como OpenAI, Anthropic y muchos otros a travÃ©s de la integraciÃ³n con LiteLLM. AdemÃ¡s, es posible ejecutar localmente estos sistemas utilizando Transformers u Ollama. Es sencillo y flexible potenciar un agente con tu LLM preferido.

ðŸ‘ï¸ **Independencia respecto a la modalidad**: los agentes pueden procesar diferentes tipos de entrada (_inputs_) como texto, visiÃ³n, video y audio, ampliando considerablemente el rango de aplicaciones posibles. Consulta este [tutorial](https://huggingface.co/docs/smolagents/v1.21.0/en/examples/web_browser) sobre el Ã¡rea de visiÃ³n.

ðŸ› ï¸ **Independencia respecto a las herramientas**: existe una gran variedad de herramientas en cualquier [Servidor MCP](reference/tools#smolagents.ToolCollection.from_mcp), marcos de orquestaciÃ³n como [LangChain](reference/tools#smolagents.Tool.from_langchain) e incluso existe la posibilidad de usar el [Hub Space](reference/tools#smolagents.Tool.from_space) como herramienta.

ðŸ’» **Herramientas de CLI**: incluye utilidades en lÃ­nea de comandos (smolagent, webagent) para ejecutar agentes rÃ¡pidamente sin cÃ³digo repetitivo.

## Inicio RÃ¡pido

[[open-in-colab]]

Â¡Comienza a usar smolagents en solo unos minutos! Esta guÃ­a te mostrarÃ¡ cÃ³mo crear y ejecutar tu primer agente.

### InstalaciÃ³n

Instala smolagents usando pip:

```bash
pip install smolagents[toolkit]  # Incluye herramientas bÃ¡sicas como bÃºsqueda web.
```

### Crea tu Primer Agente

A continuaciÃ³n se detalla un ejemplo bÃ¡sico para crear y ejecutar un agente:


```python
from smolagents import CodeAgent, InferenceClientModel

# Iniciar el modelo (utilizando la API de Hugging Face Inference)
model = InferenceClientModel()  # Utiliza el modelo por defecto

# Crear un agente sin herramientas
agent = CodeAgent(tools=[], model=model)

# Ejecuta el agente con una tarea especÃ­fica
result = agent.run("Calculate the sum of numbers from 1 to 10")
print(result)
```
Â¡Eso es todo! El agente usarÃ¡ Python para completar la tarea y entregar el resultado.

### Agregar Herramientas

Mejoremos las capacidades de nuestro agente aÃ±adiendo algunas herramientas:

```python
from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool

model = InferenceClientModel()
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
)

# Â¡Ahora el agente puede buscar informaciÃ³n en Internet!
result = agent.run("What is the current weather in Paris?")
print(result)
```

### Usar Modelos Diferentes

Puedes usar diferentes modelos con los agentes:

```python
# Usar un modelo especÃ­fico de Hugging Face
model = InferenceClientModel(model_id="meta-llama/Llama-2-70b-chat-hf")

# Usar la API de OpenAI/Anthropic (requiere smolagents[litellm])
from smolagents import LiteLLMModel
model = LiteLLMModel(model_id="gpt-4")

# Utilizar modelos locales (requiere smolagents[transformers])
from smolagents import TransformersModel
model = TransformersModel(model_id="meta-llama/Llama-2-7b-chat-hf")
```

## PrÃ³ximos Pasos

- Aprende a configurar smolagents con diferentes modelos y herramientas en la [GuÃ­a de InstalaciÃ³n](installation).
- Revisa el [Tutorial Guiado](guided_tour) y aprende a usar funciones mÃ¡s avanzadas.
- Aprende a construir [herramientas personalizadas](tutorials/tools).
- Conoce mÃ¡s sobre la [ejecuciÃ³n segura de cÃ³digo](tutorials/secure_code_execution).
- Explora el desarrollo de [sistemas multiagente](tutorials/building_good_agents).

<div class="mt-10">
  <div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5">
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./guided_tour"
      ><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Tutorial Guiado</div>
      <p class="text-gray-700">Domina los conceptos bÃ¡sicos y aprende a manejar agentes. Empieza aquÃ­ si nunca los has utilizado.</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./examples/text_to_sql"
      ><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">GuÃ­as prÃ¡cticas</div>
      <p class="text-gray-700">Ejemplos prÃ¡cticos para guiarte en diferentes proyectos. Â¡Desarrolla un agente que genere y valide consultas SQL!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual_guides/intro_agents"
      ><div class="w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">GuÃ­as Conceptuales</div>
      <p class="text-gray-700">Conceptos avanzados para profundizar en la comprensiÃ³n de temas clave.</p>
   </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/building_good_agents"
      ><div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">Tutoriales</div>
      <p class="text-gray-700">Tutoriales completos que cubren aspectos clave para el desarrollo de agentes.</p>
    </a>
  </div>
</div>



================================================
FILE: docs/source/es/installation.md
================================================
# Opciones de instalaciÃ³n

La biblioteca `smolagents` se puede instalar usando pip. Existen varias formas y opciones disponibles para realizar la instalaciÃ³n.

## Requisitos Previos
- Python 3.10 o una versiÃ³n mÃ¡s reciente
- Gestor de paquetes para Python: [`pip`](https://pip.pypa.io/en/stable/) o [`uv`](https://docs.astral.sh/uv/)

## Entorno Virtual

Instalar `smolagents` en un entorno virtual de Python es altamente recomendable. Los entornos virtuales permiten mantener las dependencias 
de tu proyecto aisladas tanto de otros proyectos como de Python en el sistema, evitando conflictos de versiones y simplificando la administraciÃ³n de paquetes.

<hfoptions id="virtual-environment">
<hfoption id="venv">
Usando [`venv`](https://docs.python.org/3/library/venv.html):
```bash
python -m venv .venv
source .venv/bin/activate
```
</hfoption>
<hfoption id="uv">

Usando [`uv`](https://docs.astral.sh/uv/):
```bash
uv venv .venv
source .venv/bin/activate
```
</hfoption>
</hfoptions>

## InstalaciÃ³n BÃ¡sica

Para instalar la biblioteca principal (core) de smolagents, usa:

<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install smolagents
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install smolagents
```
</hfoption>
</hfoptions>

## InstalaciÃ³n con Complementos

Existen dependencias adicionales (extras) en `smolagents` que puedes instalar conforme a tus necesidades.
La instalaciÃ³n de estos extras se realiza con la siguiente sintaxis:

<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[extra1,extra2]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[extra1,extra2]"
```
</hfoption>
</hfoptions>

### Herramientas

Estos complementos incluyen diversas herramientas e integraciones:

<hfoptions id="installation">
<hfoption id="pip">
- **toolkit**: Instala un paquete estÃ¡ndar de herramientas para tareas habituales.
  ```bash
  pip install "smolagents[toolkit]"
  ```
- **mcp**: Incorpora el Protocolo de Contexto de Modelo (MCP) para facilitar la integraciÃ³n de herramientas y servicios externos.
  ```bash
  pip install "smolagents[mcp]"
  ```
</hfoption>
<hfoption id="uv">
- **toolkit**: Instala un paquete estÃ¡ndar de herramientas para tareas habituales.
  ```bash
  uv pip install "smolagents[toolkit]"
  ```
- **mcp**: Incorpora el Protocolo de Contexto de Modelo (MCP) para facilitar la integraciÃ³n de herramientas y servicios externos.
  ```bash
  uv pip install "smolagents[mcp]"
  ```
</hfoption>
</hfoptions>

### IntegraciÃ³n de Modelos

Las funcionalidades adicionales facilitan la conexiÃ³n con diversos modelos y frameworks de inteligencia artificial.

<hfoptions id="installation">
<hfoption id="pip">
- **openai**: IntegraciÃ³n para los modelos de OpenAI a travÃ©s de API.
  ```bash
  pip install "smolagents[openai]"
  ```
- **transformers**: Permite el uso de modelos Transformers de Hugging Face.
  ```bash
  pip install "smolagents[transformers]"
  ```
- **vllm**: Agrega compatibilidad con vLLM para una inferencia de modelos mÃ¡s eficiente.
  ```bash
  pip install "smolagents[vllm]"
  ```
- **mlx-lm**: Incorpora funcionalidades especÃ­ficas para MLX-LM.
  ```bash
  pip install "smolagents[mlx-lm]"
  ```
- **litellm**: Habilita el uso de LiteLLM en tareas de inferencia con modelos optimizados.
  ```bash
  pip install "smolagents[litellm]"
  ```
- **bedrock**:  AmplÃ­a la compatibilidad con servicios de modelos alojados en AWS Bedrock.
  ```bash
  pip install "smolagents[bedrock]"
  ```
</hfoption>
<hfoption id="uv">
- **openai**: IntegraciÃ³n para los modelos de OpenAI a travÃ©s de API.
  ```bash
  uv pip install "smolagents[openai]"
  ```
- **transformers**: Permite el uso de modelos Transformers de Hugging Face.
  ```bash
  uv pip install "smolagents[transformers]"
  ```
- **vllm**: Agrega compatibilidad con vLLM para una inferencia de modelos mÃ¡s eficiente.
  ```bash
  uv pip install "smolagents[vllm]"
  ```
- **mlx-lm**: Incorpora funcionalidades especÃ­ficas para MLX-LM.
  ```bash
  uv pip install "smolagents[mlx-lm]"
  ```
- **litellm**: Habilita el uso de LiteLLM en tareas de inferencia con modelos optimizados.
  ```bash
  uv pip install "smolagents[litellm]"
  ```
- **bedrock**: AmplÃ­a la compatibilidad con servicios de modelos alojados en AWS Bedrock.
  ```bash
  uv pip install "smolagents[bedrock]"
  ```
</hfoption>
</hfoptions>

### Capacidades Multimodales

Funciones adicionales para procesar varios tipos de datos:

<hfoptions id="installation">
<hfoption id="pip">
- **vision**: Despliega funciones avanzadas para el procesamiento de imÃ¡genes y visiÃ³n por computadora.
  ```bash
  pip install "smolagents[vision]"
  ```
- **audio**: Incorpora soporte para tareas de procesamiento de audio.
  ```bash
  pip install "smolagents[audio]"
  ```
</hfoption>
<hfoption id="uv">
- **vision**: Despliega funciones avanzadas para el procesamiento de imÃ¡genes y visiÃ³n por computadora.
  ```bash
  uv pip install "smolagents[vision]"
  ```
- **audio**: Incorpora soporte para tareas de procesamiento de audio.
  ```bash
  uv pip install "smolagents[audio]"
  ```
</hfoption>
</hfoptions>

### EjecuciÃ³n Remota

Extensiones para ejecutar cÃ³digo a distancia:

<hfoptions id="installation">
<hfoption id="pip">
- **docker**: Funcionalidad para ejecutar scripts en entornos Docker.
  ```bash
  pip install "smolagents[docker]"
  ```
- **e2b**: Facilita la ejecuciÃ³n remota mediante soporte E2B.
  ```bash
  pip install "smolagents[e2b]"
  ```
</hfoption>
<hfoption id="uv">
- **docker**: Funcionalidad para ejecutar scripts en entornos Docker.
  ```bash
  uv pip install "smolagents[docker]"
  ```
- **e2b**: Facilita la ejecuciÃ³n remota mediante soporte E2B.
  ```bash
  uv pip install "smolagents[e2b]"
  ```
</hfoption>
</hfoptions>

### TelemetrÃ­a e Interfaz de Usuario

MÃ³dulos complementarios para telemetrÃ­a, monitoreo y diseÃ±o de interfaz:

<hfoptions id="installation">
<hfoption id="pip">
- **telemetry**: Agrega funcionalidades para actividades de monitoreo y trazabilidad.
  ```bash
  pip install "smolagents[telemetry]"
  ```
- **gradio**: Permite la utilizaciÃ³n de componentes interactivos en Gradio UI.
  ```bash
  pip install "smolagents[gradio]"
  ```
</hfoption>
<hfoption id="uv">
- **telemetry**: Agrega funcionalidades para actividades de monitoreo y trazabilidad.
  ```bash
  uv pip install "smolagents[telemetry]"
  ```
- **gradio**: Permite la utilizaciÃ³n de componentes interactivos en Gradio UI.
  ```bash
  uv pip install "smolagents[gradio]"
  ```
</hfoption>
</hfoptions>

### InstalaciÃ³n Completa

Para instalar todos los complementos disponibles, puedes usar:

<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[all]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[all]"
```
</hfoption>
</hfoptions>

## VerificaciÃ³n de la InstalaciÃ³n

DespuÃ©s de la instalaciÃ³n, puedes verificar que `smolagents` estÃ© instalado correctamente ejecutando:

```python
import smolagents
print(smolagents.__version__)
```

## PrÃ³ximos Pasos

Una vez que `smolagents` estÃ© instalado correctamente, puedes:

- Aprende los conceptos bÃ¡sicos revisando el [Tutorial Guiado](guided_tour).
- Explora los ejemplos prÃ¡cticos y aplicaciones en las [GuÃ­as PrÃ¡cticas](examples/text_to_sql).
- Profundiza en los conceptos avanzados mediante las [GuÃ­as Conceptuales](conceptual_guides/intro_agents).
- Revisa los [Tutoriales](tutorials/building_good_agents) para el desarrollo de agentes.
- Consulta la [DocumentaciÃ³n API](./reference/index) para obtener informaciÃ³n detallada sobre clases y funciones.



================================================
FILE: docs/source/hi/_config.py
================================================
# docstyle-ignore
INSTALL_CONTENT = """
# Installation
! pip install smolagents
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/smolagents.git
"""

notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
black_avoid_patterns = {
    "{processor_class}": "FakeProcessorClass",
    "{model_class}": "FakeModelClass",
    "{object_class}": "FakeObjectClass",
}



================================================
FILE: docs/source/hi/_toctree.yml
================================================
[Binary file]


================================================
FILE: docs/source/hi/guided_tour.md
================================================
[Binary file]


================================================
FILE: docs/source/hi/index.md
================================================
# `smolagents`

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/license_to_call.png" width=100%/>
</div>

à¤¯à¤¹ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤ªà¤¾à¤µà¤°à¤«à¥à¤² à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¬à¤¸à¥‡ à¤¸à¤°à¤² à¤«à¥à¤°à¥‡à¤®à¤µà¤°à¥à¤• à¤¹à¥ˆ! à¤µà¥ˆà¤¸à¥‡, "à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸" à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¤¾? à¤¹à¤® à¤…à¤ªà¤¨à¥€ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¾ [à¤‡à¤¸ à¤ªà¥‡à¤œ à¤ªà¤°](conceptual_guides/intro_agents) à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¤¹à¤¾à¤ à¤†à¤ªà¤•à¥‹ à¤¯à¤¹ à¤­à¥€ à¤ªà¤¤à¤¾ à¤šà¤²à¥‡à¤—à¤¾ à¤•à¤¿ à¤‡à¤¨à¥à¤¹à¥‡à¤‚ à¤•à¤¬ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚ à¤¯à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚ (à¤¸à¥à¤ªà¥‰à¤‡à¤²à¤°: à¤†à¤ª à¤…à¤•à¥à¤¸à¤° à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‡ à¤¬à¤¿à¤¨à¤¾ à¤¬à¥‡à¤¹à¤¤à¤° à¤•à¤¾à¤® à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚)à¥¤

à¤¯à¤¹ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆ:

âœ¨ **à¤¸à¤°à¤²à¤¤à¤¾**: Agents à¤•à¤¾ à¤²à¥‰à¤œà¤¿à¤• à¤²à¤—à¤­à¤— à¤à¤• à¤¹à¤œà¤¾à¤° à¤²à¤¾à¤‡à¤¨à¥à¤¸ à¤‘à¤«à¤¼ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤¸à¤®à¤¾à¤¹à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤¹à¤®à¤¨à¥‡ à¤°à¥‰ à¤•à¥‹à¤¡ à¤•à¥‡ à¤Šà¤ªà¤° à¤à¤¬à¥à¤¸à¥à¤Ÿà¥à¤°à¥ˆà¤•à¥à¤¶à¤¨ à¤•à¥‹ à¤¨à¥à¤¯à¥‚à¤¨à¤¤à¤® à¤†à¤•à¤¾à¤° à¤®à¥‡à¤‚ à¤°à¤–à¤¾ à¤¹à¥ˆ!

ðŸŒ **à¤¸à¤­à¥€ LLM à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤ªà¥‹à¤°à¥à¤Ÿ**: à¤¯à¤¹ à¤¹à¤¬ à¤ªà¤° à¤¹à¥‹à¤¸à¥à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤®à¥‰à¤¡à¤²à¥à¤¸ à¤•à¥‹ à¤‰à¤¨à¤•à¥‡ `transformers` à¤µà¤°à¥à¤œà¤¨ à¤®à¥‡à¤‚ à¤¯à¤¾ à¤¹à¤®à¤¾à¤°à¥‡ à¤‡à¤¨à¥à¤«à¤°à¥‡à¤‚à¤¸ API à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤¸à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤¸à¤¾à¤¥ à¤¹à¥€ OpenAI, Anthropic à¤¸à¥‡ à¤­à¥€... à¤•à¤¿à¤¸à¥€ à¤­à¥€ LLM à¤¸à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤ªà¤¾à¤µà¤° à¤•à¤°à¤¨à¤¾ à¤µà¤¾à¤¸à¥à¤¤à¤µ à¤®à¥‡à¤‚ à¤†à¤¸à¤¾à¤¨ à¤¹à¥ˆà¥¤

ðŸ§‘â€ðŸ’» **à¤•à¥‹à¤¡ Agents à¤•à¥‡ à¤²à¤¿à¤ à¤«à¤°à¥à¤¸à¥à¤Ÿ-à¤•à¥à¤²à¤¾à¤¸ à¤¸à¤ªà¥‹à¤°à¥à¤Ÿ**, à¤¯à¤¾à¤¨à¥€ à¤à¤¸à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤œà¥‹ à¤…à¤ªà¤¨à¥€ à¤à¤•à¥à¤¶à¤¨à¥à¤¸ à¤•à¥‹ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤²à¤¿à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚ (à¤•à¥‹à¤¡ à¤²à¤¿à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤ à¤œà¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‡ à¤µà¤¿à¤ªà¤°à¥€à¤¤), [à¤¯à¤¹à¤¾à¤ à¤”à¤° à¤ªà¤¢à¤¼à¥‡à¤‚](tutorials/secure_code_execution)à¥¤

ðŸ¤— **à¤¹à¤¬ à¤‡à¤‚à¤Ÿà¥€à¤—à¥à¤°à¥‡à¤¶à¤¨**: à¤†à¤ª à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤¹à¤¬ à¤ªà¤° à¤¶à¥‡à¤¯à¤° à¤”à¤° à¤²à¥‹à¤¡ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤”à¤° à¤†à¤—à¥‡ à¤”à¤° à¤­à¥€ à¤¬à¤¹à¥à¤¤ à¤•à¥à¤› à¤†à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¹à¥ˆ!
!

<div class="mt-10">
  <div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5">
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./guided_tour"
      ><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">à¤—à¤¾à¤‡à¤¡à¥‡à¤¡ à¤Ÿà¥‚à¤°</div>
      <p class="text-gray-700">à¤¬à¥‡à¤¸à¤¿à¤•à¥à¤¸ à¤¸à¥€à¤–à¥‡à¤‚ à¤”à¤° à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤ªà¤°à¤¿à¤šà¤¿à¤¤ à¤¹à¥‹à¤‚à¥¤ à¤¯à¤¦à¤¿ à¤†à¤ª à¤ªà¤¹à¤²à¥€ à¤¬à¤¾à¤° à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤¤à¥‹ à¤¯à¤¹à¤¾à¤ à¤¸à¥‡ à¤¶à¥à¤°à¥‚ à¤•à¤°à¥‡à¤‚!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./examples/text_to_sql"
      ><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">à¤¹à¤¾à¤‰-à¤Ÿà¥‚ à¤—à¤¾à¤‡à¤¡à¥à¤¸</div>
      <p class="text-gray-700">à¤à¤• à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤²à¤•à¥à¤·à¥à¤¯ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¥‡ à¤²à¤¿à¤ à¤—à¤¾à¤‡à¤¡: SQL à¤•à¥à¤µà¥‡à¤°à¥€ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤”à¤° à¤Ÿà¥‡à¤¸à¥à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¬à¤¨à¤¾à¤à¤‚!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual_guides/intro_agents"
      ><div class="w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">à¤•à¥‰à¤¨à¥à¤¸à¥‡à¤ªà¥à¤šà¥à¤…à¤² à¤—à¤¾à¤‡à¤¡à¥à¤¸</div>
      <p class="text-gray-700">à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤µà¤¿à¤·à¤¯à¥‹à¤‚ à¤•à¥€ à¤¬à¥‡à¤¹à¤¤à¤° à¤¸à¤®à¤ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤šà¥à¤š-à¤¸à¥à¤¤à¤°à¥€à¤¯ à¤µà¥à¤¯à¤¾à¤–à¥à¤¯à¤¾à¤à¤‚à¥¤</p>
   </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/building_good_agents"
      ><div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">à¤Ÿà¥à¤¯à¥‚à¤Ÿà¥‹à¤°à¤¿à¤¯à¤²à¥à¤¸</div>
      <p class="text-gray-700">à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤ªà¤¹à¤²à¥à¤“à¤‚ à¤•à¥‹ à¤•à¤µà¤° à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤•à¥à¤Ÿà¥à¤¯à¥‚à¤Ÿà¥‹à¤°à¤¿à¤¯à¤²à¥à¤¸à¥¤</p>
    </a>
  </div>
</div>


================================================
FILE: docs/source/hi/conceptual_guides/intro_agents.md
================================================
# Agents à¤•à¤¾ à¤ªà¤°à¤¿à¤šà¤¯

## ðŸ¤” Agents à¤•à¥à¤¯à¤¾ à¤¹à¥ˆà¤‚?

AI à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤•à¥à¤¶à¤² à¤ªà¥à¤°à¤£à¤¾à¤²à¥€ à¤•à¥‹ LLM à¤•à¥‹ à¤µà¤¾à¤¸à¥à¤¤à¤µà¤¿à¤• à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤¤à¤• à¤•à¤¿à¤¸à¥€ à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥€ à¤ªà¤¹à¥à¤‚à¤š à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€: à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤ à¤¬à¤¾à¤¹à¤°à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤–à¥‹à¤œ à¤Ÿà¥‚à¤² à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤¸à¤‚à¤­à¤¾à¤µà¤¨à¤¾, à¤¯à¤¾ à¤•à¤¿à¤¸à¥€ à¤•à¤¾à¤°à¥à¤¯ à¤•à¥‹ à¤¹à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤› à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤ªà¤° à¤•à¤¾à¤°à¥à¤¯ à¤•à¤°à¤¨à¥‡ à¤•à¥€à¥¤ à¤¦à¥‚à¤¸à¤°à¥‡ à¤¶à¤¬à¥à¤¦à¥‹à¤‚ à¤®à¥‡à¤‚, LLM à¤®à¥‡à¤‚ ***agency*** à¤¹à¥‹à¤¨à¥€ à¤šà¤¾à¤¹à¤¿à¤à¥¤ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® LLM à¤•à¥‡ à¤²à¤¿à¤ à¤¬à¤¾à¤¹à¤°à¥€ à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤•à¤¾ à¤ªà¥à¤°à¤µà¥‡à¤¶ à¤¦à¥à¤µà¤¾à¤° à¤¹à¥ˆà¤‚à¥¤

> [!TIP]
> AI Agents à¤µà¥‡ **à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤¹à¥ˆà¤‚ à¤œà¤¹à¤¾à¤‚ LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤•à¥‹ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚**à¥¤

LLM à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤µà¤¾à¤²à¥€ à¤•à¥‹à¤ˆ à¤­à¥€ à¤ªà¥à¤°à¤£à¤¾à¤²à¥€ LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¥‹ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤à¤•à¥€à¤•à¥ƒà¤¤ à¤•à¤°à¥‡à¤—à¥€à¥¤ à¤•à¥‹à¤¡ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤ªà¤° LLM à¤•à¥‡ à¤‡à¤¨à¤ªà¥à¤Ÿ à¤•à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤®à¥‡à¤‚ LLM à¤•à¥€ à¤à¤œà¥‡à¤‚à¤¸à¥€ à¤•à¤¾ à¤¸à¥à¤¤à¤° à¤¹à¥ˆà¥¤

à¤§à¥à¤¯à¤¾à¤¨ à¤¦à¥‡à¤‚ à¤•à¤¿ à¤‡à¤¸ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¾ à¤•à¥‡ à¤¸à¤¾à¤¥, "agent" à¤à¤• à¤…à¤²à¤—, 0 à¤¯à¤¾ 1 à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ: à¤‡à¤¸à¤•à¥‡ à¤¬à¤œà¤¾à¤¯, "agency" à¤à¤• à¤¨à¤¿à¤°à¤‚à¤¤à¤° à¤¸à¥à¤ªà¥‡à¤•à¥à¤Ÿà¥à¤°à¤® à¤ªà¤° à¤µà¤¿à¤•à¤¸à¤¿à¤¤ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ, à¤œà¥ˆà¤¸à¥‡-à¤œà¥ˆà¤¸à¥‡ à¤†à¤ª à¤…à¤ªà¤¨à¥‡ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤ªà¤° LLM à¤•à¥‹ à¤…à¤§à¤¿à¤• à¤¯à¤¾ à¤•à¤® à¤¶à¤•à¥à¤¤à¤¿ à¤¦à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤¨à¥€à¤šà¥‡ à¤¦à¥€ à¤—à¤ˆ à¤¤à¤¾à¤²à¤¿à¤•à¤¾ à¤®à¥‡à¤‚ à¤¦à¥‡à¤–à¥‡à¤‚ à¤•à¤¿ à¤•à¥ˆà¤¸à¥‡ à¤à¤œà¥‡à¤‚à¤¸à¥€ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤ªà¥à¤°à¤£à¤¾à¤²à¤¿à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ à¤­à¤¿à¤¨à¥à¤¨ à¤¹à¥‹ à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆ:

| à¤à¤œà¥‡à¤‚à¤¸à¥€ à¤¸à¥à¤¤à¤° | à¤µà¤¿à¤µà¤°à¤£ | à¤‡à¤¸à¥‡ à¤•à¥à¤¯à¤¾ à¤•à¤¹à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ | à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤ªà¥ˆà¤Ÿà¤°à¥à¤¨ |
|------------|---------|-------------------|----------------|
| â˜†â˜†â˜† | LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¤¾ à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤ªà¥à¤°à¤µà¤¾à¤¹ à¤ªà¤° à¤•à¥‹à¤ˆ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¨à¤¹à¥€à¤‚ | à¤¸à¤°à¤² à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤° | `process_llm_output(llm_response)` |
| â˜…â˜†â˜† | LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ if/else à¤¸à¥à¤µà¤¿à¤š à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ | à¤°à¤¾à¤‰à¤Ÿà¤° | `if llm_decision(): path_a() else: path_b()` |
| â˜…â˜…â˜† | LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤«à¤‚à¤•à¥à¤¶à¤¨ à¤à¤•à¥à¤œà¥€à¤•à¥à¤¯à¥‚à¤¶à¤¨ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ | à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¤° | `run_function(llm_chosen_tool, llm_chosen_args)` |
| â˜…â˜…â˜… | LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤ªà¥à¤¨à¤°à¤¾à¤µà¥ƒà¤¤à¥à¤¤à¤¿ à¤”à¤° à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤•à¥€ à¤¨à¤¿à¤°à¤‚à¤¤à¤°à¤¤à¤¾ à¤•à¥‹ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ | à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ | `while llm_should_continue(): execute_next_step()` |
| â˜…â˜…â˜… | à¤à¤• à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¦à¥‚à¤¸à¤°à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤•à¥‹ à¤¶à¥à¤°à¥‚ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ | à¤®à¤²à¥à¤Ÿà¥€-à¤à¤œà¥‡à¤‚à¤Ÿ | `if llm_trigger(): execute_agent()` |

à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª agent à¤•à¥€ à¤¯à¤¹ à¤•à¥‹à¤¡ à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤¹à¥ˆ:

```python
memory = [user_defined_task]
while llm_should_continue(memory): # à¤¯à¤¹ à¤²à¥‚à¤ª à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª à¤­à¤¾à¤— à¤¹à¥ˆ
    action = llm_get_next_action(memory) # à¤¯à¤¹ à¤Ÿà¥‚à¤²-à¤•à¥‰à¤²à¤¿à¤‚à¤— à¤­à¤¾à¤— à¤¹à¥ˆ
    observations = execute_action(action)
    memory += [action, observations]
```

à¤¯à¤¹ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤à¤• à¤²à¥‚à¤ª à¤®à¥‡à¤‚ à¤šà¤²à¤¤à¤¾ à¤¹à¥ˆ, à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤šà¤°à¤£ à¤®à¥‡à¤‚ à¤à¤• à¤¨à¤ˆ à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤•à¥‹ à¤¶à¥à¤°à¥‚ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ (à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤®à¥‡à¤‚ à¤•à¥à¤› à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ *tools* à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¤¾ à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤•à¥‡à¤µà¤² à¤«à¤‚à¤•à¥à¤¶à¤‚à¤¸ à¤¹à¥ˆà¤‚), à¤œà¤¬ à¤¤à¤• à¤•à¤¿ à¤‰à¤¸à¤•à¥‡ à¤…à¤µà¤²à¥‹à¤•à¤¨ à¤¸à¥‡ à¤¯à¤¹ à¤¸à¥à¤ªà¤·à¥à¤Ÿ à¤¨ à¤¹à¥‹ à¤œà¤¾à¤ à¤•à¤¿ à¤¦à¤¿à¤ à¤—à¤ à¤•à¤¾à¤°à¥à¤¯ à¤•à¥‹ à¤¹à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤¸à¤‚à¤¤à¥‹à¤·à¤œà¤¨à¤• à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤° à¤²à¥€ à¤—à¤ˆ à¤¹à¥ˆà¥¤

## âœ… Agents à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¬ à¤•à¤°à¥‡à¤‚ / â›” à¤•à¤¬ à¤‰à¤¨à¤¸à¥‡ à¤¬à¤šà¥‡à¤‚

Agents à¤¤à¤¬ à¤‰à¤ªà¤¯à¥‹à¤—à¥€ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¬ à¤†à¤ªà¤•à¥‹ à¤•à¤¿à¤¸à¥€ à¤à¤ª à¤•à¥‡ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤•à¥‹ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ LLM à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤²à¥‡à¤•à¤¿à¤¨ à¤µà¥‡ à¤…à¤•à¥à¤¸à¤° à¤œà¤°à¥‚à¤°à¤¤ à¤¸à¥‡ à¤œà¥à¤¯à¤¾à¤¦à¤¾ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤¸à¤µà¤¾à¤² à¤¯à¤¹ à¤¹à¥ˆ à¤•à¤¿, à¤•à¥à¤¯à¤¾ à¤®à¥à¤à¥‡ à¤µà¤¾à¤¸à¥à¤¤à¤µ à¤®à¥‡à¤‚ à¤¦à¤¿à¤ à¤—à¤ à¤•à¤¾à¤°à¥à¤¯ à¤•à¥‹ à¤•à¥à¤¶à¤²à¤¤à¤¾à¤ªà¥‚à¤°à¥à¤µà¤• à¤¹à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤®à¥‡à¤‚ à¤²à¤šà¥€à¤²à¥‡à¤ªà¤¨ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆ?
à¤¯à¤¦à¤¿ à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¬à¤¹à¥à¤¤ à¤¬à¤¾à¤° à¤µà¤¿à¤«à¤² à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ, à¤¤à¥‹ à¤‡à¤¸à¤•à¤¾ à¤®à¤¤à¤²à¤¬ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ªà¤•à¥‹ à¤…à¤§à¤¿à¤• à¤²à¤šà¥€à¤²à¥‡à¤ªà¤¨ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤†à¤‡à¤ à¤à¤• à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤²à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚: à¤®à¤¾à¤¨ à¤²à¥€à¤œà¤¿à¤ à¤†à¤ª à¤à¤• à¤à¤ª à¤¬à¤¨à¤¾ à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤à¤• à¤¸à¤°à¥à¤«à¤¿à¤‚à¤— à¤Ÿà¥à¤°à¤¿à¤ª à¤µà¥‡à¤¬à¤¸à¤¾à¤‡à¤Ÿ à¤ªà¤° à¤—à¥à¤°à¤¾à¤¹à¤• à¤…à¤¨à¥à¤°à¥‹à¤§à¥‹à¤‚ à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤²à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤†à¤ª à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤œà¤¾à¤¨ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤…à¤¨à¥à¤°à¥‹à¤§ 2 à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¤¿à¤¸à¥€ à¤à¤• à¤¶à¥à¤°à¥‡à¤£à¥€ à¤®à¥‡à¤‚ à¤†à¤à¤‚à¤—à¥‡ (à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤•à¥€ à¤ªà¤¸à¤‚à¤¦ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤°), à¤”à¤° à¤†à¤ªà¤•à¥‡ à¤ªà¤¾à¤¸ à¤‡à¤¨ 2 à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¹à¥ˆà¥¤

1. à¤Ÿà¥à¤°à¤¿à¤ª à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤•à¥à¤› à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤šà¤¾à¤¹à¤¿à¤? â‡’ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤…à¤ªà¤¨à¥‡ à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤®à¥‡à¤‚ à¤–à¥‹à¤œ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤¸à¤°à¥à¤š à¤¬à¤¾à¤° à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤š à¤¦à¥‡à¤‚
2. à¤¸à¥‡à¤²à¥à¤¸ à¤Ÿà¥€à¤® à¤¸à¥‡ à¤¬à¤¾à¤¤ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚? â‡’ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤à¤• à¤¸à¤‚à¤ªà¤°à¥à¤• à¤«à¥‰à¤°à¥à¤® à¤®à¥‡à¤‚ à¤Ÿà¤¾à¤‡à¤ª à¤•à¤°à¤¨à¥‡ à¤¦à¥‡à¤‚à¥¤

à¤¯à¤¦à¤¿ à¤µà¤¹ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤£à¤¾à¤¤à¥à¤®à¤• à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¸à¤­à¥€ à¤ªà¥à¤°à¤¶à¥à¤¨à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤«à¤¿à¤Ÿ à¤¬à¥ˆà¤ à¤¤à¤¾ à¤¹à¥ˆ, à¤¤à¥‹ à¤¬à¥‡à¤¶à¤• à¤¬à¤¸ à¤¸à¤¬ à¤•à¥à¤› à¤•à¥‹à¤¡ à¤•à¤°à¥‡à¤‚! à¤¯à¤¹ à¤†à¤ªà¤•à¥‹ à¤à¤• 100% à¤µà¤¿à¤¶à¥à¤µà¤¸à¤¨à¥€à¤¯ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤¦à¥‡à¤—à¤¾ à¤”à¤° à¤à¤²à¤à¤²à¤à¤® à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤…à¤¨à¤ªà¥‡à¤•à¥à¤·à¤¿à¤¤ à¤•à¤¾à¤°à¥à¤¯à¤ªà¥à¤°à¤µà¤¾à¤¹ à¤®à¥‡à¤‚ à¤¹à¤¸à¥à¤¤à¤•à¥à¤·à¥‡à¤ª à¤•à¤°à¤¨à¥‡ à¤¸à¥‡ à¤¤à¥à¤°à¥à¤Ÿà¤¿à¤¯à¥‹à¤‚ à¤•à¤¾ à¤•à¥‹à¤ˆ à¤œà¥‹à¤–à¤¿à¤® à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤—à¤¾à¥¤ à¤¸à¤¾à¤§à¤¾à¤°à¤£à¤¤à¤¾ à¤”à¤° à¤®à¤œà¤¬à¥‚à¤¤à¥€ à¤•à¥‡ à¤²à¤¿à¤, à¤¸à¤²à¤¾à¤¹ à¤¦à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆ à¤•à¤¿ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤¨ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤à¥¤

à¤²à¥‡à¤•à¤¿à¤¨ à¤•à¥à¤¯à¤¾ à¤¹à¥‹à¤—à¤¾ à¤…à¤—à¤° à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤•à¥‹ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤‡à¤¤à¤¨à¥€ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤¨à¤¹à¥€à¤‚ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¤¤à¤¾?

à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤, à¤à¤• à¤‰à¤ªà¤¯à¥‹à¤—à¤•à¤°à¥à¤¤à¤¾ à¤ªà¥‚à¤›à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¤¾ à¤¹à¥ˆ: `"à¤®à¥ˆà¤‚ à¤¸à¥‹à¤®à¤µà¤¾à¤° à¤•à¥‹ à¤† à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤‚, à¤²à¥‡à¤•à¤¿à¤¨ à¤®à¥ˆà¤‚ à¤…à¤ªà¤¨à¤¾ à¤ªà¤¾à¤¸à¤ªà¥‹à¤°à¥à¤Ÿ à¤­à¥‚à¤² à¤—à¤¯à¤¾ à¤œà¤¿à¤¸à¤¸à¥‡ à¤®à¥à¤à¥‡ à¤¬à¥à¤§à¤µà¤¾à¤° à¤¤à¤• à¤¦à¥‡à¤° à¤¹à¥‹ à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆ, à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤®à¥à¤à¥‡ à¤”à¤° à¤®à¥‡à¤°à¥€ à¤šà¥€à¤œà¥‹à¤‚ à¤•à¥‹ à¤®à¤‚à¤—à¤²à¤µà¤¾à¤° à¤¸à¥à¤¬à¤¹ à¤¸à¤°à¥à¤« à¤•à¤°à¤¨à¥‡ à¤²à¥‡ à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤•à¥à¤¯à¤¾ à¤®à¥à¤à¥‡ à¤•à¥ˆà¤‚à¤¸à¤²à¥‡à¤¶à¤¨ à¤‡à¤‚à¤¶à¥à¤¯à¥‹à¤°à¥‡à¤‚à¤¸ à¤®à¤¿à¤² à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ?"` à¤¯à¤¹ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¤ˆ à¤•à¤¾à¤°à¤•à¥‹à¤‚ à¤ªà¤° à¤¨à¤¿à¤°à¥à¤­à¤° à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¶à¤¾à¤¯à¤¦ à¤Šà¤ªà¤° à¤¦à¤¿à¤ à¤—à¤ à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤®à¤¾à¤¨à¤¦à¤‚à¤¡à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ à¤‡à¤¸ à¤…à¤¨à¥à¤°à¥‹à¤§ à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¤°à¥à¤¯à¤¾à¤ªà¥à¤¤ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤—à¤¾à¥¤

à¤¯à¤¦à¤¿ à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¬à¤¹à¥à¤¤ à¤¬à¤¾à¤° à¤µà¤¿à¤«à¤² à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ, à¤¤à¥‹ à¤‡à¤¸à¤•à¤¾ à¤®à¤¤à¤²à¤¬ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ªà¤•à¥‹ à¤…à¤§à¤¿à¤• à¤²à¤šà¥€à¤²à¥‡à¤ªà¤¨ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¯à¤¹à¥€à¤‚ à¤ªà¤° à¤à¤• à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¥‡à¤Ÿà¤…à¤ª à¤®à¤¦à¤¦ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤Šà¤ªà¤° à¤¦à¤¿à¤ à¤—à¤ à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤®à¥‡à¤‚, à¤†à¤ª à¤¬à¤¸ à¤à¤• à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª agent à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸à¤•à¥‡ à¤ªà¤¾à¤¸ à¤®à¥Œà¤¸à¤® à¤ªà¥‚à¤°à¥à¤µà¤¾à¤¨à¥à¤®à¤¾à¤¨ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤®à¥Œà¤¸à¤® API, à¤¯à¤¾à¤¤à¥à¤°à¤¾ à¤•à¥€ à¤¦à¥‚à¤°à¥€ à¤œà¤¾à¤¨à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥‡ à¤²à¤¿à¤ Google Maps API, à¤à¤• à¤•à¤°à¥à¤®à¤šà¤¾à¤°à¥€ à¤‰à¤ªà¤²à¤¬à¥à¤§à¤¤à¤¾ à¤¡à¥ˆà¤¶à¤¬à¥‹à¤°à¥à¤¡ à¤”à¤° à¤†à¤ªà¤•à¥‡ à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤ªà¤° à¤à¤• RAG à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤š à¤¹à¥ˆà¥¤

à¤¹à¤¾à¤² à¤¹à¥€ à¤¤à¤•, à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤¤à¤• à¤¸à¥€à¤®à¤¿à¤¤ à¤¥à¥‡, if/else à¤¸à¥à¤µà¤¿à¤š à¤•à¤¾
à¤¢à¥‡à¤° à¤²à¤—à¤¾à¤•à¤¾à¤° à¤œà¤Ÿà¤¿à¤²à¤¤à¤¾ à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤²à¤¨à¥‡ à¤•à¤¾ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤•à¤° à¤°à¤¹à¥‡ à¤¥à¥‡à¥¤ à¤µà¥‡ à¤¬à¥‡à¤¹à¤¦ à¤¸à¤‚à¤•à¥€à¤°à¥à¤£ à¤•à¤¾à¤°à¥à¤¯à¥‹à¤‚ à¤ªà¤° à¤•à¥‡à¤‚à¤¦à¥à¤°à¤¿à¤¤ à¤¥à¥‡, à¤œà¥ˆà¤¸à¥‡ "à¤‡à¤¨ à¤¸à¤‚à¤–à¥à¤¯à¤¾à¤“à¤‚ à¤•à¤¾ à¤¯à¥‹à¤— à¤¨à¤¿à¤•à¤¾à¤²à¥‡à¤‚" à¤¯à¤¾ "à¤‡à¤¸ à¤—à¥à¤°à¤¾à¤«à¤¼ à¤®à¥‡à¤‚ à¤¸à¤¬à¤¸à¥‡ à¤›à¥‹à¤Ÿà¤¾ à¤°à¤¾à¤¸à¥à¤¤à¤¾ à¤–à¥‹à¤œà¥‡à¤‚"à¥¤ à¤²à¥‡à¤•à¤¿à¤¨ à¤µà¤¾à¤¸à¥à¤¤à¤µ à¤®à¥‡à¤‚, à¤…à¤§à¤¿à¤•à¤¾à¤‚à¤¶ à¤µà¤¾à¤¸à¥à¤¤à¤µà¤¿à¤• à¤œà¥€à¤µà¤¨ à¤•à¥‡ à¤•à¤¾à¤°à¥à¤¯, à¤œà¥ˆà¤¸à¥‡ à¤Šà¤ªà¤° à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¤®à¤¾à¤°à¤¾ à¤¯à¤¾à¤¤à¥à¤°à¤¾ à¤‰à¤¦à¤¾à¤¹à¤°à¤£, à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ à¤®à¥‡à¤‚ à¤«à¤¿à¤Ÿ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤® à¤•à¥‡ à¤²à¤¿à¤ à¤µà¤¾à¤¸à¥à¤¤à¤µà¤¿à¤• à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤•à¥‡ à¤•à¤¾à¤°à¥à¤¯à¥‹à¤‚ à¤•à¥€ à¤µà¤¿à¤¶à¤¾à¤² à¤¦à¥à¤¨à¤¿à¤¯à¤¾ à¤–à¥‹à¤²à¤¤à¥‡ à¤¹à¥ˆà¤‚!

## à¤•à¥à¤¯à¥‹à¤‚ `smolagents`?

à¤•à¥à¤› à¤²à¥‹-à¤²à¥‡à¤µà¤² à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤®à¤¾à¤®à¤²à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤, à¤œà¥ˆà¤¸à¥‡ à¤šà¥‡à¤¨ à¤¯à¤¾ à¤°à¤¾à¤‰à¤Ÿà¤°, à¤†à¤ª à¤¸à¤­à¥€ à¤•à¥‹à¤¡ à¤–à¥à¤¦ à¤²à¤¿à¤– à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤†à¤ª à¤‡à¤¸ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤¬à¤¹à¥à¤¤ à¤¬à¥‡à¤¹à¤¤à¤° à¤¹à¥‹à¤‚à¤—à¥‡, à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¯à¤¹ à¤†à¤ªà¤•à¥‹ à¤…à¤ªà¤¨à¥‡ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥‹ à¤¬à¥‡à¤¹à¤¤à¤° à¤¢à¤‚à¤— à¤¸à¥‡ à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤”à¤° à¤¸à¤®à¤à¤¨à¥‡ à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤¦à¥‡à¤—à¤¾à¥¤

à¤²à¥‡à¤•à¤¿à¤¨ à¤œà¥ˆà¤¸à¥‡ à¤¹à¥€ à¤†à¤ª à¤…à¤§à¤¿à¤• à¤œà¤Ÿà¤¿à¤² à¤µà¥à¤¯à¤µà¤¹à¤¾à¤°à¥‹à¤‚ à¤•à¥€ à¤“à¤° à¤¬à¤¢à¤¼à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¿ LLM à¤•à¥‹ à¤à¤• à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¦à¥‡à¤¨à¤¾ (à¤¯à¤¹ "tool calling" à¤¹à¥ˆ) à¤¯à¤¾ LLM à¤•à¥‹ à¤à¤• while à¤²à¥‚à¤ª à¤šà¤²à¤¾à¤¨à¥‡ à¤¦à¥‡à¤¨à¤¾ ("multi-step agent"), à¤•à¥à¤› à¤à¤¬à¥à¤¸à¤Ÿà¥à¤°à¥ˆà¤•à¥à¤¶à¤¨à¥à¤¸ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ:
- à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¤¿à¤‚à¤— à¤•à¥‡ à¤²à¤¿à¤, à¤†à¤ªà¤•à¥‹ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¥‹ à¤ªà¤¾à¤°à¥à¤¸ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ, à¤‡à¤¸à¤²à¤¿à¤ à¤‡à¤¸ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¥‹ à¤à¤• à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤ªà¥à¤°à¤¾à¤°à¥‚à¤ª à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ à¤œà¥ˆà¤¸à¥‡ "à¤µà¤¿à¤šà¤¾à¤°: à¤®à¥à¤à¥‡ 'get_weather' à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤ à¤•à¥à¤°à¤¿à¤¯à¤¾: get_weather(Paris)à¥¤", à¤œà¤¿à¤¸à¥‡ à¤†à¤ª à¤à¤• à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¤¾à¤°à¥à¤¸ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤”à¤° LLM à¤•à¥‹ à¤¦à¤¿à¤ à¤—à¤ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ à¤•à¥‹ à¤‡à¤¸ à¤ªà¥à¤°à¤¾à¤°à¥‚à¤ª à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¸à¥‚à¤šà¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤
- à¤à¤• à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤²à¤¿à¤ à¤œà¤¹à¤¾à¤‚ LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤²à¥‚à¤ª à¤•à¥‹ à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤†à¤ªà¤•à¥‹ à¤ªà¤¿à¤›à¤²à¥‡ à¤²à¥‚à¤ª à¤‡à¤Ÿà¤°à¥‡à¤¶à¤¨ à¤®à¥‡à¤‚ à¤•à¥à¤¯à¤¾ à¤¹à¥à¤† à¤‡à¤¸à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° LLM à¤•à¥‹ à¤à¤• à¤…à¤²à¤— à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ à¤¦à¥‡à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ: à¤‡à¤¸à¤²à¤¿à¤ à¤†à¤ªà¤•à¥‹ à¤•à¤¿à¤¸à¥€ à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥€ à¤®à¥‡à¤®à¥‹à¤°à¥€ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤

à¤‡à¤¨ à¤¦à¥‹ à¤‰à¤¦à¤¾à¤¹à¤°à¤£à¥‹à¤‚ à¤•à¥‡ à¤¸à¤¾à¤¥, à¤¹à¤®à¤¨à¥‡ à¤ªà¤¹à¤²à¥‡ à¤¹à¥€ à¤•à¥à¤› à¤šà¥€à¤œà¥‹à¤‚ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤•à¤¾ à¤ªà¤¤à¤¾ à¤²à¤—à¤¾ à¤²à¤¿à¤¯à¤¾:

- à¤¬à¥‡à¤¶à¤•, à¤à¤• LLM à¤œà¥‹ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥‹ à¤ªà¤¾à¤µà¤° à¤¦à¥‡à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤‡à¤‚à¤œà¤¨ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤•à¤¾à¤°à¥à¤¯ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ
- à¤à¤œà¥‡à¤‚à¤Ÿ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤à¤•à¥à¤¸à¥‡à¤¸ à¤•à¤¿à¤ à¤œà¤¾ à¤¸à¤•à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥€ à¤à¤• à¤¸à¥‚à¤šà¥€
- à¤à¤• à¤ªà¤¾à¤°à¥à¤¸à¤° à¤œà¥‹ LLM à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤¸à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤•à¥‹ à¤¨à¤¿à¤•à¤¾à¤²à¤¤à¤¾ à¤¹à¥ˆ
- à¤à¤• à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤ªà¥à¤°à¥‹à¤®à¥à¤ªà¥à¤Ÿ à¤œà¥‹ à¤ªà¤¾à¤°à¥à¤¸à¤° à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¸à¤¿à¤‚à¤•à¥à¤°à¤¨à¤¾à¤‡à¤œà¤¼ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ
- à¤à¤• à¤®à¥‡à¤®à¥‹à¤°à¥€

à¤²à¥‡à¤•à¤¿à¤¨ à¤°à¥à¤•à¤¿à¤, à¤šà¥‚à¤‚à¤•à¤¿ à¤¹à¤® à¤¨à¤¿à¤°à¥à¤£à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ LLM à¤•à¥‹ à¤œà¤—à¤¹ à¤¦à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤µà¥‡ à¤—à¤²à¤¤à¤¿à¤¯à¤¾à¤‚ à¤•à¤°à¥‡à¤‚à¤—à¥‡: à¤‡à¤¸à¤²à¤¿à¤ à¤¹à¤®à¥‡à¤‚ à¤à¤°à¤° à¤²à¥‰à¤—à¤¿à¤‚à¤— à¤”à¤° à¤ªà¥à¤¨à¤ƒ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤¤à¤‚à¤¤à¥à¤° à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¯à¥‡ à¤¸à¤­à¥€ à¤¤à¤¤à¥à¤µ à¤à¤• à¤…à¤šà¥à¤›à¥‡ à¤•à¤¾à¤®à¤•à¤¾à¤œà¥€ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤•-à¤¦à¥‚à¤¸à¤°à¥‡ à¤¸à¥‡ à¤˜à¤¨à¤¿à¤·à¥à¤  à¤°à¥‚à¤ª à¤¸à¥‡ à¤œà¥à¤¡à¤¼à¥‡ à¤¹à¥à¤ à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¹à¥€ à¤•à¤¾à¤°à¤£ à¤¹à¥ˆ à¤•à¤¿ à¤¹à¤®à¤¨à¥‡ à¤¤à¤¯ à¤•à¤¿à¤¯à¤¾ à¤•à¤¿ à¤‡à¤¨ à¤¸à¤­à¥€ à¤šà¥€à¤œà¥‹à¤‚ à¤•à¥‹ à¤à¤• à¤¸à¤¾à¤¥ à¤•à¤¾à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¬à¥à¤¨à¤¿à¤¯à¤¾à¤¦à¥€ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤¬à¥à¤²à¥‰à¤•à¥à¤¸ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

## à¤•à¥‹à¤¡ Agents

à¤à¤• à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚, à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤šà¤°à¤£ à¤ªà¤°, LLM à¤¬à¤¾à¤¹à¤°à¥€ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤•à¥à¤› à¤•à¥‰à¤² à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤à¤• à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤²à¤¿à¤– à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¨ à¤•à¥à¤°à¤¿à¤¯à¤¾à¤“à¤‚ à¤•à¥‹ à¤²à¤¿à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤¸à¥à¤µà¤°à¥‚à¤ª (Anthropic, OpenAI à¤”à¤° à¤•à¤ˆ à¤…à¤¨à¥à¤¯ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ) à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° "à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‡ à¤¨à¤¾à¤® à¤”à¤° à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¤à¤°à¥à¤•à¥‹à¤‚ à¤•à¥‡ JSON à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤•à¥à¤°à¤¿à¤¯à¤¾à¤à¤‚ à¤²à¤¿à¤–à¤¨à¥‡" à¤•à¥‡ à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤°à¥‚à¤ª à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚ à¤†à¤ª à¤«à¤¿à¤° à¤ªà¤¾à¤°à¥à¤¸ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤¯à¤¹ à¤œà¤¾à¤¨à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿ à¤•à¥Œà¤¨ à¤¸à¤¾ à¤Ÿà¥‚à¤² à¤•à¤¿à¤¨ à¤¤à¤°à¥à¤•à¥‹à¤‚ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¨à¤¿à¤·à¥à¤ªà¤¾à¤¦à¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤¹à¥ˆ"à¥¤

[à¤•à¤ˆ](https://huggingface.co/papers/2402.01030) [à¤¶à¥‹à¤§](https://huggingface.co/papers/2411.01747) [à¤ªà¤¤à¥à¤°à¥‹à¤‚](https://huggingface.co/papers/2401.00812) à¤¨à¥‡ à¤¦à¤¿à¤–à¤¾à¤¯à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¤¿à¤‚à¤— LLM à¤•à¤¾ à¤¹à¥‹à¤¨à¤¾ à¤¬à¤¹à¥à¤¤ à¤¬à¥‡à¤¹à¤¤à¤° à¤¹à¥ˆà¥¤

à¤‡à¤¸à¤•à¤¾ à¤•à¤¾à¤°à¤£ à¤¬à¤¸ à¤¯à¤¹ à¤¹à¥ˆ à¤•à¤¿ *à¤¹à¤®à¤¨à¥‡ à¤…à¤ªà¤¨à¥€ à¤•à¥‹à¤¡ à¤­à¤¾à¤·à¤¾à¤“à¤‚ à¤•à¥‹ à¤µà¤¿à¤¶à¥‡à¤· à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤•à¤¿à¤ à¤—à¤ à¤•à¤¾à¤°à¥à¤¯à¥‹à¤‚ à¤•à¥‹ à¤µà¥à¤¯à¤•à¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤¸à¤°à¥à¤µà¥‹à¤¤à¥à¤¤à¤® à¤¸à¤‚à¤­à¤µ à¤¤à¤°à¥€à¤•à¤¾ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¤¿à¤¯à¤¾*à¥¤ à¤¯à¤¦à¤¿ JSON à¤¸à¥à¤¨à¤¿à¤ªà¥‡à¤Ÿà¥à¤¸ à¤¬à¥‡à¤¹à¤¤à¤° à¤…à¤­à¤¿à¤µà¥à¤¯à¤•à¥à¤¤à¤¿ à¤¹à¥‹à¤¤à¥‡, à¤¤à¥‹ JSON à¤¶à¥€à¤°à¥à¤· à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤— à¤­à¤¾à¤·à¤¾ à¤¹à¥‹à¤¤à¥€ à¤”à¤° à¤ªà¥à¤°à¥‹à¤—à¥à¤°à¤¾à¤®à¤¿à¤‚à¤— à¤¨à¤°à¤• à¤®à¥‡à¤‚ à¤¹à¥‹à¤¤à¥€à¥¤

à¤¨à¥€à¤šà¥‡ à¤¦à¥€ à¤—à¤ˆ à¤›à¤µà¤¿, [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030) à¤¸à¥‡ à¤²à¥€ à¤—à¤ˆ à¤¹à¥ˆ, à¤œà¥‹ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤•à¥à¤°à¤¿à¤¯à¤¾à¤à¤‚ à¤²à¤¿à¤–à¤¨à¥‡ à¤•à¥‡ à¤•à¥à¤› à¤«à¤¾à¤¯à¤¦à¥‡ à¤¦à¤°à¥à¤¶à¤¾à¤¤à¥€ à¤¹à¥ˆ:

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png">

JSON à¤œà¥ˆà¤¸à¥‡ à¤¸à¥à¤¨à¤¿à¤ªà¥‡à¤Ÿà¥à¤¸ à¤•à¥€ à¤¬à¤œà¤¾à¤¯ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤•à¥à¤°à¤¿à¤¯à¤¾à¤à¤‚ à¤²à¤¿à¤–à¤¨à¥‡ à¤¸à¥‡ à¤¬à¥‡à¤¹à¤¤à¤° à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ:

- **à¤•à¤®à¥à¤ªà¥‹à¤œà¥‡à¤¬à¤¿à¤²à¤¿à¤Ÿà¥€:** à¤•à¥à¤¯à¤¾ à¤†à¤ª JSON à¤•à¥à¤°à¤¿à¤¯à¤¾à¤“à¤‚ à¤•à¥‹ à¤à¤•-à¤¦à¥‚à¤¸à¤°à¥‡ à¤•à¥‡ à¤­à¥€à¤¤à¤° à¤¨à¥‡à¤¸à¥à¤Ÿ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¯à¤¾ à¤¬à¤¾à¤¦ à¤®à¥‡à¤‚ à¤ªà¥à¤¨: à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ JSON à¤•à¥à¤°à¤¿à¤¯à¤¾à¤“à¤‚ à¤•à¤¾ à¤à¤• à¤¸à¥‡à¤Ÿ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤‰à¤¸à¥€ à¤¤à¤°à¤¹ à¤œà¥ˆà¤¸à¥‡ à¤†à¤ª à¤¬à¤¸ à¤à¤• à¤ªà¤¾à¤¯à¤¥à¤¨ à¤«à¤‚à¤•à¥à¤¶à¤¨ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚?
- **à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤ªà¥à¤°à¤¬à¤‚à¤§à¤¨:** à¤†à¤ª `generate_image` à¤œà¥ˆà¤¸à¥€ à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤•à¥‡ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¥‹ JSON à¤®à¥‡à¤‚ à¤•à¥ˆà¤¸à¥‡ à¤¸à¥à¤Ÿà¥‹à¤° à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚?
- **à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯à¤¤à¤¾:** à¤•à¥‹à¤¡ à¤•à¥‹ à¤¸à¤°à¤² à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¥à¤› à¤­à¥€ à¤µà¥à¤¯à¤•à¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤†à¤ª à¤•à¤‚à¤ªà¥à¤¯à¥‚à¤Ÿà¤° à¤¸à¥‡ à¤•à¤°à¤µà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
- **LLM à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤¡à¥‡à¤Ÿà¤¾ à¤®à¥‡à¤‚ à¤ªà¥à¤°à¤¤à¤¿à¤¨à¤¿à¤§à¤¿à¤¤à¥à¤µ:** à¤¬à¤¹à¥à¤¤ à¤¸à¤¾à¤°à¥€ à¤—à¥à¤£à¤µà¤¤à¥à¤¤à¤¾à¤ªà¥‚à¤°à¥à¤£ à¤•à¥‹à¤¡ à¤•à¥à¤°à¤¿à¤¯à¤¾à¤à¤‚ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤¹à¥€ LLM à¤•à¥‡ à¤Ÿà¥à¤°à¥‡à¤¨à¤¿à¤‚à¤— à¤¡à¥‡à¤Ÿà¤¾ à¤®à¥‡à¤‚ à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸à¤•à¤¾ à¤®à¤¤à¤²à¤¬ à¤¹à¥ˆ à¤•à¤¿ à¤µà¥‡ à¤‡à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤¹à¥€ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤¹à¥ˆà¤‚!


================================================
FILE: docs/source/hi/conceptual_guides/react.md
================================================
[Binary file]


================================================
FILE: docs/source/hi/examples/multiagents.md
================================================
# à¤®à¤²à¥à¤Ÿà¥€-à¤à¤œà¥‡à¤‚à¤Ÿ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¤¾ à¤†à¤¯à¥‹à¤œà¤¨ à¤•à¤°à¥‡à¤‚ ðŸ¤–ðŸ¤ðŸ¤–

[[open-in-colab]]

à¤‡à¤¸ à¤¨à¥‹à¤Ÿà¤¬à¥à¤• à¤®à¥‡à¤‚ à¤¹à¤® à¤à¤• **à¤®à¤²à¥à¤Ÿà¥€-à¤à¤œà¥‡à¤‚à¤Ÿ à¤µà¥‡à¤¬ à¤¬à¥à¤°à¤¾à¤‰à¤œà¤¼à¤° à¤¬à¤¨à¤¾à¤à¤‚à¤—à¥‡: à¤à¤• à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤•à¤ˆ à¤à¤œà¥‡à¤‚à¤Ÿ à¤µà¥‡à¤¬ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤¸à¤®à¤¸à¥à¤¯à¤¾à¤“à¤‚ à¤•à¥‹ à¤¹à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¹à¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚!**

à¤¯à¤¹ à¤à¤• à¤¸à¤°à¤² à¤¸à¤‚à¤°à¤šà¤¨à¤¾ à¤¹à¥‹à¤—à¥€, à¤œà¥‹ à¤ªà¥à¤°à¤¬à¤‚à¤§à¤¿à¤¤ à¤µà¥‡à¤¬ à¤–à¥‹à¤œ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤°à¥ˆà¤ª à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `ManagedAgent` à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ:

```
              +----------------+
              | Manager agent  |
              +----------------+
                       |
        _______________|______________
       |                              |
  Code interpreter   +--------------------------------+
       tool          |         Managed agent          |
                     |      +------------------+      |
                     |      | Web Search agent |      |
                     |      +------------------+      |
                     |         |            |         |
                     |  Web Search tool     |         |
                     |             Visit webpage tool |
                     +--------------------------------+
```
à¤†à¤‡à¤ à¤‡à¤¸ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥‹ à¤¸à¥‡à¤Ÿ à¤•à¤°à¥‡à¤‚à¥¤

à¤†à¤µà¤¶à¥à¤¯à¤• à¤¡à¤¿à¤ªà¥‡à¤‚à¤¡à¥‡à¤‚à¤¸à¥€ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¥€à¤šà¥‡ à¤¦à¥€ à¤—à¤ˆ à¤²à¤¾à¤‡à¤¨ à¤šà¤²à¤¾à¤à¤‚:

```
!pip install 'smolagents[toolkit]' --upgrade -q
```

HF Inference API à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤²à¥‰à¤—à¤¿à¤¨ à¤•à¤°à¥‡à¤‚:

```
from huggingface_hub import login

login()
```

âš¡ï¸ à¤¹à¤®à¤¾à¤°à¤¾ à¤à¤œà¥‡à¤‚à¤Ÿ [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking) à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤¸à¤‚à¤šà¤¾à¤²à¤¿à¤¤ à¤¹à¥‹à¤—à¤¾ à¤œà¥‹ `InferenceClientModel` à¤•à¥à¤²à¤¾à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥‹ HF à¤•à¥‡ Inference API à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ: Inference API à¤•à¤¿à¤¸à¥€ à¤­à¥€ OS à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤œà¤²à¥à¤¦à¥€ à¤”à¤° à¤†à¤¸à¤¾à¤¨à¥€ à¤¸à¥‡ à¤šà¤²à¤¾à¤¨à¥‡ à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤

_à¤¨à¥‹à¤Ÿ:_ The Inference API à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤®à¤¾à¤¨à¤¦à¤‚à¤¡à¥‹à¤‚ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤®à¥‰à¤¡à¤² à¤¹à¥‹à¤¸à¥à¤Ÿ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¡à¤¿à¤ªà¥à¤²à¥‰à¤¯ à¤•à¤¿à¤ à¤—à¤ à¤®à¥‰à¤¡à¤² à¤¬à¤¿à¤¨à¤¾ à¤ªà¥‚à¤°à¥à¤µ à¤¸à¥‚à¤šà¤¨à¤¾ à¤•à¥‡ à¤…à¤ªà¤¡à¥‡à¤Ÿ à¤¯à¤¾ à¤¬à¤¦à¤²à¥‡ à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤‡à¤¸à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤…à¤§à¤¿à¤• à¤œà¤¾à¤¨à¥‡à¤‚ [à¤¯à¤¹à¤¾à¤‚](https://huggingface.co/docs/api-inference/supported-models)à¥¤

```py
model_id = "Qwen/Qwen3-Next-80B-A3B-Thinking"
```

## ðŸ” à¤à¤• à¤µà¥‡à¤¬ à¤¸à¤°à¥à¤š à¤Ÿà¥‚à¤² à¤¬à¤¨à¤¾à¤à¤‚

à¤µà¥‡à¤¬ à¤¬à¥à¤°à¤¾à¤‰à¤œà¤¼à¤¿à¤‚à¤— à¤•à¥‡ à¤²à¤¿à¤, à¤¹à¤® à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤®à¥Œà¤œà¥‚à¤¦ [`WebSearchTool`] à¤Ÿà¥‚à¤² à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥‹ Google search à¤•à¥‡ à¤¸à¤®à¤¾à¤¨ à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤²à¥‡à¤•à¤¿à¤¨ à¤«à¤¿à¤° à¤¹à¤®à¥‡à¤‚ `WebSearchTool` à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤–à¥‹à¤œà¥‡ à¤—à¤ à¤ªà¥‡à¤œ à¤•à¥‹ à¤¦à¥‡à¤–à¤¨à¥‡ à¤®à¥‡à¤‚ à¤­à¥€ à¤¸à¤•à¥à¤·à¤® à¤¹à¥‹à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€à¥¤
à¤à¤¸à¤¾ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤¹à¤® à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¥‡ à¤¬à¤¿à¤²à¥à¤Ÿ-à¤‡à¤¨ `VisitWebpageTool` à¤•à¥‹ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤²à¥‡à¤•à¤¿à¤¨ à¤¹à¤® à¤‡à¤¸à¥‡ à¤«à¤¿à¤° à¤¸à¥‡ à¤¬à¤¨à¤¾à¤à¤‚à¤—à¥‡ à¤¯à¤¹ à¤¦à¥‡à¤–à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¿ à¤¯à¤¹ à¤•à¥ˆà¤¸à¥‡ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¤à¥‹ à¤†à¤‡à¤ `markdownify` à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤¶à¥à¤°à¥‚ à¤¸à¥‡ à¤…à¤ªà¤¨à¤¾ `VisitWebpageTool` à¤Ÿà¥‚à¤² à¤¬à¤¨à¤¾à¤à¤‚à¥¤

```py
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool


@tool
def visit_webpage(url: str) -> str:
    """Visits a webpage at the given URL and returns its content as a markdown string.

    Args:
        url: The URL of the webpage to visit.

    Returns:
        The content of the webpage converted to Markdown, or an error message if the request fails.
    """
    try:
        # Send a GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes

        # Convert the HTML content to Markdown
        markdown_content = markdownify(response.text).strip()

        # Remove multiple line breaks
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

à¤ à¥€à¤• à¤¹à¥ˆ, à¤…à¤¬ à¤šà¤²à¤¿à¤ à¤¹à¤®à¤¾à¤°à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‹ à¤Ÿà¥‡à¤¸à¥à¤Ÿ à¤•à¤°à¥‡à¤‚!

```py
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## à¤¹à¤®à¤¾à¤°à¥€ à¤®à¤²à¥à¤Ÿà¥€-à¤à¤œà¥‡à¤‚à¤Ÿ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¤¾ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤•à¤°à¥‡à¤‚ ðŸ¤–ðŸ¤ðŸ¤–

à¤…à¤¬ à¤œà¤¬ à¤¹à¤®à¤¾à¤°à¥‡ à¤ªà¤¾à¤¸ à¤¸à¤­à¥€ à¤Ÿà¥‚à¤²à¥à¤¸ `search` à¤”à¤° `visit_webpage` à¤¹à¥ˆà¤‚, à¤¹à¤® à¤‰à¤¨à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤µà¥‡à¤¬ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤‡à¤¸ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥Œà¤¨ à¤¸à¤¾ à¤•à¥‰à¤¨à¥à¤«à¤¼à¤¿à¤—à¤°à¥‡à¤¶à¤¨ à¤šà¥à¤¨à¥‡à¤‚?
- à¤µà¥‡à¤¬ à¤¬à¥à¤°à¤¾à¤‰à¤œà¤¼à¤¿à¤‚à¤— à¤à¤• à¤¸à¤¿à¤‚à¤—à¤²-à¤Ÿà¤¾à¤‡à¤®à¤²à¤¾à¤‡à¤¨ à¤Ÿà¤¾à¤¸à¥à¤• à¤¹à¥ˆ à¤œà¤¿à¤¸à¥‡ à¤¸à¤®à¤¾à¤¨à¤¾à¤‚à¤¤à¤° à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ, à¤‡à¤¸à¤²à¤¿à¤ JSON à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¤¿à¤‚à¤— à¤‡à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤…à¤šà¥à¤›à¥€ à¤¤à¤°à¤¹ à¤•à¤¾à¤® à¤•à¤°à¤¤à¥€ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤²à¤¿à¤ à¤¹à¤® `ToolCallingAgent` à¤šà¥à¤¨à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
- à¤¸à¤¾à¤¥ à¤¹à¥€, à¤šà¥‚à¤‚à¤•à¤¿ à¤•à¤­à¥€-à¤•à¤­à¥€ à¤µà¥‡à¤¬ à¤¸à¤°à¥à¤š à¤®à¥‡à¤‚ à¤¸à¤¹à¥€ à¤‰à¤¤à¥à¤¤à¤° à¤–à¥‹à¤œà¤¨à¥‡ à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤•à¤ˆ à¤ªà¥‡à¤œà¥‹à¤‚ à¤•à¥€ à¤¸à¤°à¥à¤š à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ, à¤¹à¤® `max_steps` à¤•à¥‹ à¤¬à¤¢à¤¼à¤¾à¤•à¤° 10 à¤•à¤°à¤¨à¤¾ à¤ªà¤¸à¤‚à¤¦ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    InferenceClientModel,
    ManagedAgent,
    WebSearchTool,
)

model = InferenceClientModel(model_id=model_id)

web_agent = ToolCallingAgent(
    tools=[WebSearchTool(), visit_webpage],
    model=model,
    max_steps=10,
)
```

à¤«à¤¿à¤° à¤¹à¤® à¤‡à¤¸ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤à¤• `ManagedAgent` à¤®à¥‡à¤‚ à¤°à¥ˆà¤ª à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤‡à¤¸à¥‡ à¤‡à¤¸à¤•à¥‡ à¤®à¥ˆà¤¨à¥‡à¤œà¤° à¤à¤œà¥‡à¤‚à¤Ÿ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¯à¥‹à¤—à¥à¤¯ à¤¬à¤¨à¤¾à¤à¤—à¤¾à¥¤

```py
managed_web_agent = ManagedAgent(
    agent=web_agent,
    name="search",
    description="Runs web searches for you. Give it your query as an argument.",
)
```

à¤…à¤‚à¤¤ à¤®à¥‡à¤‚ à¤¹à¤® à¤à¤• à¤®à¥ˆà¤¨à¥‡à¤œà¤° à¤à¤œà¥‡à¤‚à¤Ÿ à¤¬à¤¨à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤”à¤° à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° à¤¹à¤® à¤…à¤ªà¤¨à¥‡ à¤®à¥ˆà¤¨à¥‡à¤œà¥à¤¡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤‡à¤¸à¤•à¥‡ `managed_agents` à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¸ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤šà¥‚à¤‚à¤•à¤¿ à¤¯à¤¹ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¯à¥‹à¤œà¤¨à¤¾ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤”à¤° à¤¸à¥‹à¤šà¤¨à¥‡ à¤•à¤¾ à¤•à¤¾à¤® à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤‰à¤¨à¥à¤¨à¤¤ à¤¤à¤°à¥à¤• à¤²à¤¾à¤­à¤¦à¤¾à¤¯à¤• à¤¹à¥‹à¤—à¤¾, à¤‡à¤¸à¤²à¤¿à¤ `CodeAgent` à¤¸à¤¬à¤¸à¥‡ à¤…à¤šà¥à¤›à¤¾ à¤µà¤¿à¤•à¤²à¥à¤ª à¤¹à¥‹à¤—à¤¾à¥¤

à¤¸à¤¾à¤¥ à¤¹à¥€, à¤¹à¤® à¤à¤• à¤à¤¸à¤¾ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤ªà¥‚à¤›à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤µà¤°à¥à¤· à¤”à¤° à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤ à¤¡à¥‡à¤Ÿà¤¾ à¤—à¤£à¤¨à¤¾ à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥ˆ: à¤‡à¤¸à¤²à¤¿à¤ à¤†à¤‡à¤ `additional_authorized_imports=["time", "numpy", "pandas"]` à¤œà¥‹à¤¡à¤¼à¥‡à¤‚, à¤¯à¤¦à¤¿ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤‡à¤¨ à¤ªà¥ˆà¤•à¥‡à¤œà¥‹à¤‚ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¥¤

```py
manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[managed_web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

à¤¬à¤¸ à¤‡à¤¤à¤¨à¤¾ à¤¹à¥€! à¤…à¤¬ à¤šà¤²à¤¿à¤ à¤¹à¤®à¤¾à¤°à¥‡ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥‹ à¤šà¤²à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚! à¤¹à¤® à¤à¤• à¤à¤¸à¤¾ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤šà¥à¤¨à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤—à¤£à¤¨à¤¾ à¤”à¤° à¤¶à¥‹à¤§ à¤¦à¥‹à¤¨à¥‹à¤‚ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

```py
answer = manager_agent.run("If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.")
```

We get this report as the answer:
```
Based on current growth projections and energy consumption estimates, if LLM trainings continue to scale up at the 
current rhythm until 2030:

1. The electric power required to power the biggest training runs by 2030 would be approximately 303.74 GW, which 
translates to about 2,660,762 GWh/year.

2. Comparing this to countries' electricity consumption:
   - It would be equivalent to about 34% of China's total electricity consumption.
   - It would exceed the total electricity consumption of India (184%), Russia (267%), and Japan (291%).
   - It would be nearly 9 times the electricity consumption of countries like Italy or Mexico.

3. Source of numbers:
   - The initial estimate of 5 GW for future LLM training comes from AWS CEO Matt Garman.
   - The growth projection used a CAGR of 79.80% from market research by Springs.
   - Country electricity consumption data is from the U.S. Energy Information Administration, primarily for the year 
2021.
```

à¤²à¤—à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤¯à¤¦à¤¿ [à¤¸à¥à¤•à¥‡à¤²à¤¿à¤‚à¤— à¤¹à¤¾à¤‡à¤ªà¥‹à¤¥à¤¿à¤¸à¤¿à¤¸](https://gwern.net/scaling-hypothesis) à¤¸à¤¤à¥à¤¯ à¤¬à¤¨à¥€ à¤°à¤¹à¤¤à¥€ à¤¹à¥ˆ à¤¤à¥‹ à¤¹à¤®à¥‡à¤‚ à¤•à¥à¤› à¤¬à¤¡à¤¼à¥‡ à¤ªà¤¾à¤µà¤°à¤ªà¥à¤²à¤¾à¤‚à¤Ÿà¥à¤¸ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€à¥¤

à¤¹à¤®à¤¾à¤°à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¨à¥‡ à¤•à¤¾à¤°à¥à¤¯ à¤•à¥‹ à¤¹à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤¶à¤²à¤¤à¤¾à¤ªà¥‚à¤°à¥à¤µà¤• à¤¸à¤¹à¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾! âœ…

ðŸ’¡ à¤†à¤ª à¤‡à¤¸ à¤‘à¤°à¥à¤•à¥‡à¤¸à¥à¤Ÿà¥à¤°à¥‡à¤¶à¤¨ à¤•à¥‹ à¤†à¤¸à¤¾à¤¨à¥€ à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤®à¥‡à¤‚ à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤°à¤¿à¤¤ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚: à¤à¤• à¤•à¥‹à¤¡ à¤à¤•à¥à¤œà¥€à¤•à¥à¤¯à¥‚à¤¶à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤à¤• à¤µà¥‡à¤¬ à¤¸à¤°à¥à¤š à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤à¤• à¤«à¤¾à¤‡à¤² à¤²à¥‹à¤¡à¤¿à¤‚à¤— à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤²à¤¤à¤¾ à¤¹à¥ˆà¥¤



================================================
FILE: docs/source/hi/examples/rag.md
================================================
# à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• RAG

[[open-in-colab]]

à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤²-à¤‘à¤—à¤®à¥‡à¤‚à¤Ÿà¥‡à¤¡-à¤œà¤¨à¤°à¥‡à¤¶à¤¨ (RAG) à¤¹à¥ˆ "à¤à¤• à¤¯à¥‚à¤œà¤° à¤•à¥‡ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¤¾ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ LLM à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾, à¤²à¥‡à¤•à¤¿à¤¨ à¤‰à¤¤à¥à¤¤à¤° à¤•à¥‹ à¤à¤• à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤¸à¥‡ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤ªà¤° à¤†à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¨à¤¾"à¥¤ à¤‡à¤¸à¤®à¥‡à¤‚ à¤µà¥ˆà¤¨à¤¿à¤²à¤¾ à¤¯à¤¾ à¤«à¤¾à¤‡à¤¨-à¤Ÿà¥à¤¯à¥‚à¤¨à¥à¤¡ LLM à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤¤à¥à¤²à¤¨à¤¾ à¤®à¥‡à¤‚ à¤•à¤ˆ à¤«à¤¾à¤¯à¤¦à¥‡ à¤¹à¥ˆà¤‚: à¤•à¥à¤› à¤¨à¤¾à¤® à¤²à¥‡à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤¯à¤¹ à¤‰à¤¤à¥à¤¤à¤° à¤•à¥‹ à¤¸à¤¤à¥à¤¯ à¤¤à¤¥à¥à¤¯à¥‹à¤‚ à¤ªà¤° à¤†à¤§à¤¾à¤°à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤”à¤° à¤•à¤¾à¤²à¥à¤ªà¤¨à¤¿à¤• à¤¬à¤¾à¤¤à¥‹à¤‚ à¤•à¥‹ à¤•à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ, à¤¯à¤¹ LLM à¤•à¥‹ à¤¡à¥‹à¤®à¥‡à¤¨-à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤œà¥à¤žà¤¾à¤¨ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¯à¤¹ à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤¸à¥‡ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤š à¤•à¤¾ à¤¸à¥‚à¤•à¥à¤·à¥à¤® à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤£ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤²à¥‡à¤•à¤¿à¤¨ à¤µà¥ˆà¤¨à¤¿à¤²à¤¾ RAG à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤à¤‚ à¤¹à¥ˆà¤‚, à¤¸à¤¬à¤¸à¥‡ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¯à¥‡ à¤¦à¥‹:
- à¤¯à¤¹ à¤•à¥‡à¤µà¤² à¤à¤• à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤² à¤¸à¥à¤Ÿà¥‡à¤ª à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ: à¤¯à¤¦à¤¿ à¤ªà¤°à¤¿à¤£à¤¾à¤® à¤–à¤°à¤¾à¤¬ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤œà¤¨à¤°à¥‡à¤¶à¤¨ à¤­à¥€ à¤¬à¤¦à¤²à¥‡ à¤®à¥‡à¤‚ à¤–à¤°à¤¾à¤¬ à¤¹à¥‹à¤—à¤¾à¥¤
- à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤®à¤¾à¤¨à¤¤à¤¾ à¤•à¥€ à¤—à¤£à¤¨à¤¾ à¤¯à¥‚à¤œà¤° à¤•à¥‡ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¥‹ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤•à¤°à¤•à¥‡ à¤•à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆ, à¤œà¥‹ à¤…à¤¨à¥à¤•à¥‚à¤² à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤¸à¤•à¤¤à¥€: à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤, à¤¯à¥‚à¤œà¤° à¤•à¤¾ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤…à¤•à¥à¤¸à¤° à¤à¤• à¤¸à¤µà¤¾à¤² à¤¹à¥‹à¤—à¤¾, à¤œà¤¬à¤•à¤¿ à¤¸à¤¹à¥€ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤¡à¥‰à¤•à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿ à¤¸à¤•à¤¾à¤°à¤¾à¤¤à¥à¤®à¤• à¤¸à¥à¤µà¤° à¤®à¥‡à¤‚ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤‡à¤¸à¤•à¤¾ à¤¸à¤®à¤¾à¤¨à¤¤à¤¾ à¤¸à¥à¤•à¥‹à¤° à¤…à¤¨à¥à¤¯ à¤¸à¥à¤°à¥‹à¤¤ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼à¥‹à¤‚ à¤•à¥€ à¤¤à¥à¤²à¤¨à¤¾ à¤®à¥‡à¤‚ à¤•à¤® à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥‹ à¤ªà¥à¤°à¤¶à¥à¤¨à¤µà¤¾à¤šà¤• à¤¸à¥à¤µà¤° à¤®à¥‡à¤‚ à¤¹à¥‹ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤‡à¤¸à¤¸à¥‡ à¤¸à¤‚à¤¬à¤‚à¤§à¤¿à¤¤ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤•à¥‹ à¤šà¥‚à¤•à¤¨à¥‡ à¤•à¤¾ à¤œà¥‹à¤–à¤¿à¤® à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¹à¤® à¤à¤• RAG à¤à¤œà¥‡à¤‚à¤Ÿ à¤¬à¤¨à¤¾à¤•à¤° à¤‡à¤¨ à¤¸à¤®à¤¸à¥à¤¯à¤¾à¤“à¤‚ à¤•à¥‹ à¤•à¤® à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚: à¤¬à¤¹à¥à¤¤ à¤¸à¤°à¤² à¤¤à¤°à¥€à¤•à¥‡ à¤¸à¥‡, à¤à¤• à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤° à¤Ÿà¥‚à¤² à¤¸à¥‡ à¤²à¥ˆà¤¸ à¤à¤œà¥‡à¤‚à¤Ÿ!

à¤¯à¤¹ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¤°à¥‡à¤—à¤¾: âœ… à¤¸à¥à¤µà¤¯à¤‚ à¤•à¥à¤µà¥‡à¤°à¥€ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¤°à¥‡à¤—à¤¾ à¤”à¤° âœ… à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤ªà¤¡à¤¼à¤¨à¥‡ à¤ªà¤° à¤ªà¥à¤¨à¤ƒ-à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤à¤¿ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤®à¥€à¤•à¥à¤·à¤¾ à¤•à¤°à¥‡à¤—à¤¾à¥¤

à¤‡à¤¸à¤²à¤¿à¤ à¤¯à¤¹ à¤¸à¤¹à¤œ à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¥à¤› à¤‰à¤¨à¥à¤¨à¤¤ RAG à¤¤à¤•à¤¨à¥€à¤•à¥‹à¤‚ à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤° à¤²à¥‡à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤!
- à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤–à¥‹à¤œ à¤®à¥‡à¤‚ à¤¸à¥€à¤§à¥‡ à¤¯à¥‚à¤œà¤° à¤•à¥à¤µà¥‡à¤°à¥€ à¤•à¤¾ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤¬à¤œà¤¾à¤¯, à¤à¤œà¥‡à¤‚à¤Ÿ à¤¸à¥à¤µà¤¯à¤‚ à¤à¤• à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤µà¤¾à¤•à¥à¤¯ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤²à¤•à¥à¤·à¤¿à¤¤ à¤¡à¥‰à¤•à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‡ à¤•à¤°à¥€à¤¬ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥ˆà¤¸à¤¾ à¤•à¤¿ [HyDE](https://huggingface.co/papers/2212.10496) à¤®à¥‡à¤‚ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤
à¤à¤œà¥‡à¤‚à¤Ÿ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤¸à¥à¤¨à¤¿à¤ªà¥‡à¤Ÿà¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤ªà¤¡à¤¼à¤¨à¥‡ à¤ªà¤° à¤ªà¥à¤¨à¤ƒ-à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤à¤¿ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥ˆà¤¸à¤¾ à¤•à¤¿ [Self-Query](https://docs.llamaindex.ai/en/stable/examples/evaluation/RetryQuery/) à¤®à¥‡à¤‚ à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤

à¤šà¤²à¤¿à¤ à¤‡à¤¸ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤•à¥‹ à¤¬à¤¨à¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ ðŸ› ï¸

à¤†à¤µà¤¶à¥à¤¯à¤• à¤¡à¤¿à¤ªà¥‡à¤‚à¤¡à¥‡à¤‚à¤¸à¥€ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¥€à¤šà¥‡ à¤¦à¥€ à¤—à¤ˆ à¤²à¤¾à¤‡à¤¨ à¤šà¤²à¤¾à¤à¤‚à¥¤
```bash
!pip install smolagents pandas langchain langchain-community sentence-transformers rank_bm25 --upgrade -q
```
HF Inference API à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤†à¤ªà¤•à¥‹ à¤…à¤ªà¤¨à¥‡ à¤à¤¨à¤µà¤¾à¤¯à¤°à¤¨à¤®à¥‡à¤‚à¤Ÿ à¤µà¥‡à¤°à¤¿à¤à¤¬à¤² `HF_TOKEN` à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤à¤• à¤µà¥ˆà¤§ à¤Ÿà¥‹à¤•à¤¨ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€à¥¤
à¤¹à¤® à¤‡à¤¸à¥‡ à¤²à¥‹à¤¡ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ python-dotenv à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
```py
from dotenv import load_dotenv
load_dotenv()
```

à¤¹à¤® à¤ªà¤¹à¤²à¥‡ à¤à¤• à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤²à¥‹à¤¡ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸ à¤ªà¤° à¤¹à¤® RAG à¤•à¥‹ à¤²à¤¾à¤—à¥‚ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚: à¤¯à¤¹ à¤¡à¥‡à¤Ÿà¤¾ à¤¸à¥‡à¤Ÿ Hugging Face à¤•à¥‡ à¤•à¤ˆ à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¥‡ à¤¡à¥‰à¤•à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿ à¤ªà¥ƒà¤·à¥à¤ à¥‹à¤‚ à¤•à¤¾ à¤¸à¤‚à¤•à¤²à¤¨ à¤¹à¥ˆ, à¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚ Markdown à¤®à¥‡à¤‚ à¤¸à¥à¤Ÿà¥‹à¤° à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤¹à¤® à¤•à¥‡à¤µà¤² `transformers` à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¥‡ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼à¥‹à¤‚ à¤•à¥‹ à¤°à¤–à¥‡à¤‚à¤—à¥‡à¥¤

à¤«à¤¿à¤° à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤•à¥‹ à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸ à¤•à¤°à¤•à¥‡ à¤”à¤° à¤‡à¤¸à¥‡ à¤à¤• à¤µà¥‡à¤•à¥à¤Ÿà¤° à¤¡à¥‡à¤Ÿà¤¾à¤¬à¥‡à¤¸ à¤®à¥‡à¤‚ à¤¸à¥à¤Ÿà¥‹à¤° à¤•à¤°à¤•à¥‡ à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¤°à¥‡à¤‚ à¤œà¤¿à¤¸à¥‡ à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤° à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤à¤—à¤¾à¥¤

à¤¹à¤® [LangChain](https://python.langchain.com/docs/introduction/) à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤‡à¤¸à¤®à¥‡à¤‚ à¤‰à¤¤à¥à¤•à¥ƒà¤·à¥à¤Ÿ à¤µà¥‡à¤•à¥à¤Ÿà¤° à¤¡à¥‡à¤Ÿà¤¾à¤¬à¥‡à¤¸ à¤‰à¤ªà¤¯à¥‹à¤—à¤¿à¤¤à¤¾à¤à¤‚ à¤¹à¥ˆà¤‚à¥¤

```py
import datasets
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever

knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))

source_docs = [
    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]})
    for doc in knowledge_base
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    add_start_index=True,
    strip_whitespace=True,
    separators=["\n\n", "\n", ".", " ", ""],
)
docs_processed = text_splitter.split_documents(source_docs)
```

à¤…à¤¬ à¤¡à¥‰à¤•à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤¤à¥ˆà¤¯à¤¾à¤° à¤¹à¥ˆà¤‚à¥¤

à¤¤à¥‹ à¤šà¤²à¤¿à¤ à¤…à¤ªà¤¨à¤¾ à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• RAG à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤¬à¤¨à¤¾à¤à¤‚!

ðŸ‘‰ à¤¹à¤®à¥‡à¤‚ à¤•à¥‡à¤µà¤² à¤à¤• RetrieverTool à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤œà¤¿à¤¸à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤¹à¤®à¤¾à¤°à¤¾ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¨à¥‰à¤²à¥‡à¤œ à¤¬à¥‡à¤¸ à¤¸à¥‡ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤šà¥‚à¤‚à¤•à¤¿ à¤¹à¤®à¥‡à¤‚ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤à¤Ÿà¥à¤°à¥€à¤¬à¥à¤¯à¥‚à¤Ÿ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤à¤• vectordb à¤œà¥‹à¤¡à¤¼à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤¹à¤® à¤¸à¤°à¤² à¤Ÿà¥‚à¤² à¤•à¤‚à¤¸à¥à¤Ÿà¥à¤°à¤•à¥à¤Ÿà¤° à¤•à¥‹ `@tool` à¤¡à¥‡à¤•à¥‹à¤°à¥‡à¤Ÿà¤° à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¸à¥€à¤§à¥‡ à¤‰à¤ªà¤¯à¥‹à¤— à¤¨à¤¹à¥€à¤‚ à¤•à¤° à¤¸à¤•à¤¤à¥‡: à¤‡à¤¸à¤²à¤¿à¤ à¤¹à¤® [tools tutorial](../tutorials/tools) à¤®à¥‡à¤‚ à¤¹à¤¾à¤‡à¤²à¤¾à¤‡à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤¸à¥‡à¤Ÿà¤…à¤ª à¤•à¤¾ à¤ªà¤¾à¤²à¤¨ à¤•à¤°à¥‡à¤‚à¤—à¥‡à¥¤

```py
from smolagents import Tool

class RetrieverTool(Tool):
    name = "retriever"
    description = "Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    inputs = {
        "query": {
            "type": "string",
            "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
        }
    }
    output_type = "string"

    def __init__(self, docs, **kwargs):
        super().__init__(**kwargs)
        self.retriever = BM25Retriever.from_documents(
            docs, k=10
        )

    def forward(self, query: str) -> str:
        assert isinstance(query, str), "Your search query must be a string"

        docs = self.retriever.invoke(
            query,
        )
        return "\nRetrieved documents:\n" + "".join(
            [
                f"\n\n===== Document {str(i)} =====\n" + doc.page_content
                for i, doc in enumerate(docs)
            ]
        )

retriever_tool = RetrieverTool(docs_processed)
```
à¤¹à¤®à¤¨à¥‡ BM25 à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆ, à¤œà¥‹ à¤à¤• à¤•à¥à¤²à¤¾à¤¸à¤¿à¤• à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤² à¤µà¤¿à¤§à¤¿ à¤¹à¥ˆ,  à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤‡à¤¸à¥‡ à¤¸à¥‡à¤Ÿà¤…à¤ª à¤•à¤°à¤¨à¤¾ à¤¬à¤¹à¥à¤¤ à¤†à¤¸à¤¾à¤¨ à¤¹à¥ˆà¥¤
à¤°à¤¿à¤Ÿà¥à¤°à¥€à¤µà¤² à¤¸à¤Ÿà¥€à¤•à¤¤à¤¾ à¤®à¥‡à¤‚ à¤¸à¥à¤§à¤¾à¤° à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤†à¤ª BM25 à¤•à¥‹ à¤¡à¥‰à¤•à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‡ à¤²à¤¿à¤ à¤µà¥‡à¤•à¥à¤Ÿà¤° à¤ªà¥à¤°à¤¤à¤¿à¤¨à¤¿à¤§à¤¿à¤¤à¥à¤µ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤–à¥‹à¤œ à¤¸à¥‡ à¤¬à¤¦à¤² à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚: à¤‡à¤¸ à¤ªà¥à¤°à¤•à¤¾à¤° à¤†à¤ª à¤à¤• à¤…à¤šà¥à¤›à¤¾ à¤à¤®à¥à¤¬à¥‡à¤¡à¤¿à¤‚à¤— à¤®à¥‰à¤¡à¤² à¤šà¥à¤¨à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) à¤ªà¤° à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤…à¤¬ à¤¯à¤¹ à¤¸à¥€à¤§à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤à¤• à¤à¤œà¥‡à¤‚à¤Ÿ à¤¬à¤¨à¤¾à¤¯à¤¾ à¤œà¤¾à¤ à¤œà¥‹ à¤‡à¤¸ `retriever_tool` à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤—à¤¾!


à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° à¤‡à¤¨ à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€:
- `tools`: à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥€ à¤à¤• à¤¸à¥‚à¤šà¥€ à¤œà¤¿à¤¨à¥à¤¹à¥‡à¤‚ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‰à¤² à¤•à¤° à¤¸à¤•à¥‡à¤—à¤¾à¥¤
- `model`: LLM à¤œà¥‹ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤ªà¤¾à¤µà¤° à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤
à¤¹à¤®à¤¾à¤°à¤¾ `model` à¤à¤• à¤•à¥‰à¤²à¥‡à¤¬à¤² à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤ à¤œà¥‹ à¤‡à¤¨à¤ªà¥à¤Ÿ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤¸à¤‚à¤¦à¥‡à¤¶à¥‹à¤‚ à¤•à¥€ à¤à¤• à¤¸à¥‚à¤šà¥€ à¤²à¥‡à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤²à¥Œà¤Ÿà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¥‡ à¤à¤• stop_sequences à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ à¤­à¥€ à¤¸à¥à¤µà¥€à¤•à¤¾à¤° à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤¬à¤¤à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤œà¤¨à¤°à¥‡à¤¶à¤¨ à¤•à¤¬ à¤°à¥‹à¤•à¤¨à¥€ à¤¹à¥ˆà¥¤ à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤•à¥‡ à¤²à¤¿à¤, à¤¹à¤® à¤¸à¥€à¤§à¥‡ à¤ªà¥ˆà¤•à¥‡à¤œ à¤®à¥‡à¤‚ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¥€ à¤—à¤ˆ HfEngine à¤•à¥à¤²à¤¾à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤¤à¤¾à¤•à¤¿ à¤à¤• LLM à¤‡à¤‚à¤œà¤¨ à¤®à¤¿à¤² à¤¸à¤•à¥‡ à¤œà¥‹ Hugging Face à¤•à¥‡ Inference API à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤”à¤° à¤¹à¤® [meta-llama/Llama-3.3-70B-Instruct](meta-llama/Llama-3.3-70B-Instruct) à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— llm à¤‡à¤‚à¤œà¤¨ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿:
- à¤‡à¤¸à¤®à¥‡à¤‚ à¤²à¤‚à¤¬à¤¾ 128k à¤•à¥‰à¤¨à¥à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤¹à¥ˆ, à¤œà¥‹ à¤²à¤‚à¤¬à¥‡ à¤¸à¥à¤°à¥‹à¤¤ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¥‹à¤‚ à¤•à¥‹ à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸ à¤•à¤°à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦à¤—à¤¾à¤° à¤¹à¥ˆ
- à¤¯à¤¹ à¤¹à¤° à¤¸à¤®à¤¯ HF à¤•à¥‡ Inference API à¤ªà¤° à¤®à¥à¤«à¥à¤¤ à¤®à¥‡à¤‚ à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤¹à¥ˆ!

_à¤¨à¥‹à¤Ÿ:_ Inference API à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤®à¤¾à¤¨à¤¦à¤‚à¤¡à¥‹à¤‚ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤®à¥‰à¤¡à¤² à¤¹à¥‹à¤¸à¥à¤Ÿ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¡à¤¿à¤ªà¥à¤²à¥‰à¤¯ à¤•à¤¿à¤ à¤—à¤ à¤®à¥‰à¤¡à¤² à¤¬à¤¿à¤¨à¤¾ à¤ªà¥‚à¤°à¥à¤µ à¤¸à¥‚à¤šà¤¨à¤¾ à¤•à¥‡ à¤…à¤ªà¤¡à¥‡à¤Ÿ à¤¯à¤¾ à¤¬à¤¦à¤²à¥‡ à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤‡à¤¸à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤…à¤§à¤¿à¤• à¤œà¤¾à¤¨à¥‡à¤‚ [à¤¯à¤¹à¤¾à¤‚](https://huggingface.co/docs/api-inference/supported-models) à¤ªà¤¢à¤¼à¥‡à¤‚à¥¤

```py
from smolagents import InferenceClientModel, CodeAgent

agent = CodeAgent(
    tools=[retriever_tool], model=InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct"), max_steps=4, verbosity_level=2
)
```

CodeAgent à¤•à¥‹ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œ à¤•à¤°à¤¨à¥‡ à¤ªà¤°, à¤‡à¤¸à¥‡ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤à¤• à¤¡à¤¿à¤«à¤¼à¥‰à¤²à¥à¤Ÿ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ à¤¦à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤œà¥‹ LLM à¤‡à¤‚à¤œà¤¨ à¤•à¥‹ à¤šà¤°à¤£-à¤¦à¤°-à¤šà¤°à¤£ à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸ à¤•à¤°à¤¨à¥‡ à¤”à¤° à¤•à¥‹à¤¡ à¤¸à¥à¤¨à¤¿à¤ªà¥‡à¤Ÿà¥à¤¸ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¹à¤¤à¤¾ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤†à¤ª à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾à¤¨à¥à¤¸à¤¾à¤° à¤‡à¤¸ à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ à¤Ÿà¥‡à¤®à¥à¤ªà¤²à¥‡à¤Ÿ à¤•à¥‹ à¤…à¤ªà¤¨à¥‡ à¤¸à¥‡ à¤¬à¤¦à¤² à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤œà¤¬ CodeAgent à¤•à¤¾ `.run()` à¤®à¥‡à¤¥à¤¡ à¤²à¥‰à¤¨à¥à¤š à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤¤à¥‹ à¤à¤œà¥‡à¤‚à¤Ÿ LLM à¤‡à¤‚à¤œà¤¨ à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤•à¤¾à¤°à¥à¤¯ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¥à¤¸ à¤•à¥‹ à¤¨à¤¿à¤·à¥à¤ªà¤¾à¤¦à¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤¯à¤¹ à¤¸à¤¬ à¤à¤• à¤²à¥‚à¤ª à¤®à¥‡à¤‚ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥‹ à¤¤à¤¬ à¤¤à¤• à¤šà¤²à¤¤à¤¾ à¤¹à¥ˆ à¤œà¤¬ à¤¤à¤• à¤Ÿà¥‚à¤² final_answer à¤•à¥‡ à¤¸à¤¾à¤¥ à¤…à¤‚à¤¤à¤¿à¤® à¤‰à¤¤à¥à¤¤à¤° à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤¨à¤¹à¥€à¤‚ à¤¬à¥à¤²à¤¾à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾à¥¤

```py
agent_output = agent.run("For a transformers model training, which is slower, the forward or the backward pass?")

print("Final output:")
print(agent_output)
```





================================================
FILE: docs/source/hi/examples/text_to_sql.md
================================================
[Binary file]


================================================
FILE: docs/source/hi/reference/agents.md
================================================
# Agents

<Tip warning={true}>

Smolagents à¤à¤• experimental API à¤¹à¥ˆ à¤œà¥‹ à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¸à¤®à¤¯ à¤¬à¤¦à¤² à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤²à¥Œà¤Ÿà¤¾à¤ à¤—à¤ à¤ªà¤°à¤¿à¤£à¤¾à¤® à¤­à¤¿à¤¨à¥à¤¨ à¤¹à¥‹ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ APIs à¤¯à¤¾ underlying à¤®à¥‰à¤¡à¤² à¤¬à¤¦à¤²à¤¨à¥‡ à¤•à¥€ à¤¸à¤‚à¤­à¤¾à¤µà¤¨à¤¾ à¤°à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

</Tip>

Agents à¤”à¤° tools à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤…à¤§à¤¿à¤• à¤œà¤¾à¤¨à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [introductory guide](../index) à¤ªà¤¢à¤¼à¤¨à¤¾ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤ 
à¤¯à¤¹ à¤ªà¥‡à¤œ underlying à¤•à¥à¤²à¤¾à¤¸à¥‡à¤œ à¤•à¥‡ à¤²à¤¿à¤ API docs à¤•à¥‹ à¤¶à¤¾à¤®à¤¿à¤² à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

## Agents

à¤¹à¤®à¤¾à¤°à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ [`MultiStepAgent`] à¤¸à¥‡ à¤‡à¤¨à¤¹à¥‡à¤°à¤¿à¤Ÿ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¸à¤•à¤¾ à¤…à¤°à¥à¤¥ à¤¹à¥ˆ à¤•à¤¿ à¤µà¥‡ à¤•à¤ˆ à¤šà¤°à¤£à¥‹à¤‚ à¤®à¥‡à¤‚ à¤•à¤¾à¤°à¥à¤¯ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤šà¤°à¤£ à¤®à¥‡à¤‚ à¤à¤• à¤µà¤¿à¤šà¤¾à¤°, à¤«à¤¿à¤° à¤à¤• à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤”à¤° à¤à¤•à¥à¤œà¥€à¤•à¥à¤¯à¥‚à¤¶à¤¨ à¤¶à¤¾à¤®à¤¿à¤² à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤ [à¤‡à¤¸ à¤•à¥‰à¤¨à¥à¤¸à¥‡à¤ªà¥à¤šà¥à¤…à¤² à¤—à¤¾à¤‡à¤¡](../conceptual_guides/react) à¤®à¥‡à¤‚ à¤…à¤§à¤¿à¤• à¤ªà¤¢à¤¼à¥‡à¤‚à¥¤

à¤¹à¤® à¤®à¥à¤–à¥à¤¯ [`Agent`] à¤•à¥à¤²à¤¾à¤¸ à¤ªà¤° à¤†à¤§à¤¾à¤°à¤¿à¤¤ à¤¦à¥‹ à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
  - [`CodeAgent`] à¤¡à¤¿à¤«à¤¼à¥‰à¤²à¥à¤Ÿ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¹à¥ˆ, à¤¯à¤¹ à¤…à¤ªà¤¨à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¥à¤¸ à¤•à¥‹ Python à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤²à¤¿à¤–à¤¤à¤¾ à¤¹à¥ˆà¥¤
  - [`ToolCallingAgent`] à¤…à¤ªà¤¨à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‰à¤²à¥à¤¸ à¤•à¥‹ JSON à¤®à¥‡à¤‚ à¤²à¤¿à¤–à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¦à¥‹à¤¨à¥‹à¤‚ à¤•à¥‹ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° `model` à¤”à¤° à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥€ à¤¸à¥‚à¤šà¥€ `tools` à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤

### Agents à¤•à¥€ à¤•à¥à¤²à¤¾à¤¸à¥‡à¤œ

[[autodoc]] MultiStepAgent

[[autodoc]] CodeAgent

[[autodoc]] ToolCallingAgent

### stream_to_gradio

[[autodoc]] stream_to_gradio

### GradioUI

[[autodoc]] GradioUI

## à¤®à¥‰à¤¡à¤²à¥à¤¸

à¤†à¤ª à¤¸à¥à¤µà¤¤à¤‚à¤¤à¥à¤° à¤°à¥‚à¤ª à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤¸à¥à¤µà¤¯à¤‚ à¤•à¥‡ à¤®à¥‰à¤¡à¤² à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤‰à¤¨à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤†à¤ª à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥‹à¤ˆ à¤­à¥€ `model` à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¯à¥‹à¤—à¥à¤¯ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤œà¤¬ à¤¤à¤• à¤•à¤¿:
1. à¤¯à¤¹ à¤…à¤ªà¤¨à¥‡ à¤‡à¤¨à¤ªà¥à¤Ÿ `messages` à¤•à¥‡ à¤²à¤¿à¤ [messages format](./chat_templating) (`List[Dict[str, str]]`) à¤•à¤¾ à¤ªà¤¾à¤²à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤¯à¤¹ à¤à¤• `str` à¤²à¥Œà¤Ÿà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤
2. à¤¯à¤¹ à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ `stop_sequences` à¤®à¥‡à¤‚ à¤ªà¤¾à¤¸ à¤•à¤¿à¤ à¤—à¤ à¤¸à¥€à¤•à¥à¤µà¥‡à¤‚à¤¸ à¤¸à¥‡ *à¤ªà¤¹à¤²à¥‡* à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤°à¤¨à¤¾ à¤¬à¤‚à¤¦ à¤•à¤° à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤…à¤ªà¤¨à¥‡ LLM à¤•à¥‹ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤†à¤ª à¤à¤• `custom_model` à¤®à¥‡à¤¥à¤¡ à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥‹ [messages](./chat_templating) à¤•à¥€ à¤à¤• à¤¸à¥‚à¤šà¥€ à¤¸à¥à¤µà¥€à¤•à¤¾à¤° à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤”à¤° à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤¯à¥à¤•à¥à¤¤ .content à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾ à¤µà¤¾à¤²à¤¾ à¤à¤• à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤²à¥Œà¤Ÿà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸ à¤•à¥‰à¤²à¥‡à¤¬à¤² à¤•à¥‹ à¤à¤• `stop_sequences` à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ à¤­à¥€ à¤¸à¥à¤µà¥€à¤•à¤¾à¤° à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ à¤¬à¤¤à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤•à¤¬ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤°à¤¨à¤¾ à¤”à¤° à¤¬à¤‚à¤¦ à¤•à¤°à¤¨à¤¾ à¤¹à¥ˆà¥¤

```python
from huggingface_hub import login, InferenceClient

login("<YOUR_HUGGINGFACEHUB_API_TOKEN>")

model_id = "meta-llama/Llama-3.3-70B-Instruct"

client = InferenceClient(model=model_id)

def custom_model(messages, stop_sequences=["Task"]):
    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)
    answer = response.choices[0].message
    return answer
```

à¤‡à¤¸à¤•à¥‡ à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤, `custom_model` à¤à¤• `grammar` à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ à¤­à¥€ à¤²à¥‡ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤œà¤¿à¤¸ à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤®à¥‡à¤‚ à¤†à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° à¤à¤• `grammar` à¤¨à¤¿à¤°à¥à¤¦à¤¿à¤·à¥à¤Ÿ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¯à¤¹ à¤†à¤°à¥à¤—à¥à¤®à¥‡à¤‚à¤Ÿ à¤®à¥‰à¤¡à¤² à¤•à¥‡ à¤•à¥‰à¤²à¥à¤¸ à¤•à¥‹ à¤†à¤ªà¤•à¥‡ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ `grammar` à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¤¾à¤¸ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤à¤—à¤¾, à¤¤à¤¾à¤•à¤¿ [constrained generation](https://huggingface.co/docs/text-generation-inference/conceptual/guidance) à¤•à¥€ à¤…à¤¨à¥à¤®à¤¤à¤¿ à¤®à¤¿à¤² à¤¸à¤•à¥‡ à¤œà¤¿à¤¸à¤¸à¥‡ à¤‰à¤šà¤¿à¤¤-à¤«à¥‰à¤°à¥à¤®à¥‡à¤Ÿà¥‡à¤¡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¥‹ à¤«à¥‹à¤°à¥à¤¸ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¥‡à¥¤

### TransformersModel

à¤¸à¥à¤µà¤¿à¤§à¤¾ à¤•à¥‡ à¤²à¤¿à¤, à¤¹à¤®à¤¨à¥‡ à¤à¤• `TransformersModel` à¤œà¥‹à¤¡à¤¼à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° à¤¦à¤¿à¤ à¤—à¤ model_id à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤²à¥‹à¤•à¤² `transformers` à¤ªà¤¾à¤‡à¤ªà¤²à¤¾à¤‡à¤¨ à¤¬à¤¨à¤¾à¤•à¤° à¤Šà¤ªà¤° à¤•à¥‡ à¤¬à¤¿à¤‚à¤¦à¥à¤“à¤‚ à¤•à¥‹ à¤²à¤¾à¤—à¥‚ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

```python
from smolagents import TransformersModel

model = TransformersModel(model_id="HuggingFaceTB/SmolLM-135M-Instruct")

print(model([{"role": "user", "content": "Ok!"}], stop_sequences=["great"]))
```
```text
>>> What a
```

[[autodoc]] TransformersModel

### InferenceClientModel

`InferenceClientModel` LLM à¤•à¥‡ à¤à¤•à¥à¤œà¥€à¤•à¥à¤¯à¥‚à¤¶à¤¨ à¤•à¥‡ à¤²à¤¿à¤ [HF Inference API](https://huggingface.co/docs/api-inference/index) à¤•à¥à¤²à¤¾à¤‡à¤‚à¤Ÿ à¤•à¥‹ à¤°à¥ˆà¤ª à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

```python
from smolagents import InferenceClientModel

messages = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "No need to help, take it easy."},
]

model = InferenceClientModel()
print(model(messages))
```
```text
>>> Of course! If you change your mind, feel free to reach out. Take care!
```
[[autodoc]] InferenceClientModel

### LiteLLMModel

`LiteLLMModel` à¤µà¤¿à¤­à¤¿à¤¨à¥à¤¨ à¤ªà¥à¤°à¤¦à¤¾à¤¤à¤¾à¤“à¤‚ à¤¸à¥‡ 100+ LLMs à¤•à¥‹ à¤¸à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [LiteLLM](https://www.litellm.ai/) à¤•à¤¾ à¤²à¤¾à¤­ à¤‰à¤ à¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤
à¤†à¤ª à¤®à¥‰à¤¡à¤² à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤ªà¤° kwargs à¤ªà¤¾à¤¸ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¥‹ à¤¤à¤¬ à¤®à¥‰à¤¡à¤² à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¤à¥‡ à¤¸à¤®à¤¯ à¤ªà¥à¤°à¤¯à¥‹à¤— à¤•à¤¿à¤ à¤œà¤¾à¤à¤‚à¤—à¥‡, à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤ à¤¨à¥€à¤šà¥‡ à¤¹à¤® `temperature` à¤ªà¤¾à¤¸ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```python
from smolagents import LiteLLMModel

messages = [
  {"role": "user", "content": "Hello, how are you?"},
  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
  {"role": "user", "content": "No need to help, take it easy."},
]

model = LiteLLMModel(model_id="anthropic/claude-3-5-sonnet-latest", temperature=0.2, max_tokens=10)
print(model(messages))
```

[[autodoc]] LiteLLMModel

### OpenAiModel


à¤¯à¤¹ à¤•à¥à¤²à¤¾à¤¸ à¤†à¤ªà¤•à¥‹ à¤•à¤¿à¤¸à¥€ à¤­à¥€ OpenAIServer à¤•à¤®à¥à¤ªà¥ˆà¤Ÿà¤¿à¤¬à¤² à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¦à¥‡à¤¤à¥€ à¤¹à¥ˆà¥¤
à¤¯à¤¹à¤¾à¤ à¤¬à¤¤à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª à¤‡à¤¸à¥‡ à¤•à¥ˆà¤¸à¥‡ à¤¸à¥‡à¤Ÿ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ (à¤†à¤ª à¤¦à¥‚à¤¸à¤°à¥‡ à¤¸à¤°à¥à¤µà¤° à¤•à¥‹ à¤ªà¥‰à¤‡à¤‚à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `api_base` url à¤•à¥‹ à¤•à¤¸à¥à¤Ÿà¤®à¤¾à¤‡à¤œà¤¼ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚):
```py
import os
from smolagents import OpenAIModel

model = OpenAIModel(
    model_id="gpt-4o",
    api_base="https://api.openai.com/v1",
    api_key=os.environ["OPENAI_API_KEY"],
)
```

## Prompts

[[autodoc]] smolagents.agents.PromptTemplates

[[autodoc]] smolagents.agents.PlanningPromptTemplate

[[autodoc]] smolagents.agents.ManagedAgentPromptTemplate

[[autodoc]] smolagents.agents.FinalAnswerPromptTemplate



================================================
FILE: docs/source/hi/reference/tools.md
================================================
# Tools

<Tip warning={true}>

Smolagents à¤à¤• experimental API à¤¹à¥ˆ à¤œà¥‹ à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤¸à¤®à¤¯ à¤¬à¤¦à¤² à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤²à¥Œà¤Ÿà¤¾à¤ à¤—à¤ à¤ªà¤°à¤¿à¤£à¤¾à¤® à¤­à¤¿à¤¨à¥à¤¨ à¤¹à¥‹ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ APIs à¤¯à¤¾ underlying à¤®à¥‰à¤¡à¤² à¤¬à¤¦à¤²à¤¨à¥‡ à¤•à¥€ à¤¸à¤‚à¤­à¤¾à¤µà¤¨à¤¾ à¤°à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

</Tip>

à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤”à¤° à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤…à¤§à¤¿à¤• à¤œà¤¾à¤¨à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [introductory guide](../index) à¤ªà¤¢à¤¼à¤¨à¤¾ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤ 
à¤¯à¤¹ à¤ªà¥‡à¤œ underlying à¤•à¥à¤²à¤¾à¤¸à¥‡à¤œ à¤•à¥‡ à¤²à¤¿à¤ API docs à¤•à¥‹ à¤¶à¤¾à¤®à¤¿à¤² à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

## Tools

### load_tool

[[autodoc]] load_tool

### tool

[[autodoc]] tool

### Tool

[[autodoc]] Tool

### launch_gradio_demo

[[autodoc]] launch_gradio_demo

## Default Tools

### PythonInterpreterTool

[[autodoc]] PythonInterpreterTool

### DuckDuckGoSearchTool

[[autodoc]] DuckDuckGoSearchTool

### VisitWebpageTool

[[autodoc]] VisitWebpageTool

### UserInputTool

[[autodoc]] UserInputTool

## ToolCollection

[[autodoc]] ToolCollection

## Agent à¤Ÿà¤¾à¤‡à¤ªà¥à¤¸

à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‡ à¤¬à¥€à¤š à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥€ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤² à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚; à¤Ÿà¥‚à¤²à¥à¤¸, à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤®à¤²à¥à¤Ÿà¥€à¤®à¥‹à¤¡à¤² à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤•à¤¾à¤°à¤£, à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ, à¤‡à¤®à¥‡à¤œ, à¤‘à¤¡à¤¿à¤¯à¥‹, à¤µà¥€à¤¡à¤¿à¤¯à¥‹ à¤¸à¤¹à¤¿à¤¤ à¤…à¤¨à¥à¤¯ à¤ªà¥à¤°à¤•à¤¾à¤°à¥‹à¤‚ à¤•à¥‹ à¤¸à¥à¤µà¥€à¤•à¤¾à¤° à¤”à¤° à¤°à¤¿à¤Ÿà¤°à¥à¤¨ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ 
à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‡ à¤¬à¥€à¤š à¤…à¤¨à¥à¤•à¥‚à¤²à¤¤à¤¾ à¤¬à¤¢à¤¼à¤¾à¤¨à¥‡ à¤•à¥‡ à¤¸à¤¾à¤¥-à¤¸à¤¾à¤¥ à¤‡à¤¨ à¤°à¤¿à¤Ÿà¤°à¥à¤¨à¥à¤¸ à¤•à¥‹ ipython (jupyter, colab, ipython notebooks, ...) à¤®à¥‡à¤‚ à¤¸à¤¹à¥€ à¤¢à¤‚à¤— à¤¸à¥‡ à¤°à¥‡à¤‚à¤¡à¤° à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤¹à¤® à¤‡à¤¨ à¤Ÿà¤¾à¤‡à¤ªà¥à¤¸ à¤•à¥‡ à¤†à¤¸à¤ªà¤¾à¤¸ à¤°à¥ˆà¤ªà¤° à¤•à¥à¤²à¤¾à¤¸à¥‡à¤œ à¤•à¥‹ à¤²à¤¾à¤—à¥‚ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤°à¥ˆà¤ª à¤•à¤¿à¤ à¤—à¤ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿà¥à¤¸ à¤•à¥‹ à¤ªà¥à¤°à¤¾à¤°à¤‚à¤­ à¤®à¥‡à¤‚ à¤œà¥ˆà¤¸à¤¾ à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤ à¤µà¥ˆà¤¸à¤¾ à¤¹à¥€ à¤•à¤°à¤¨à¤¾ à¤œà¤¾à¤°à¥€ à¤°à¤–à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤; à¤à¤• à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‹ à¤…à¤­à¥€ à¤­à¥€ à¤¸à¥à¤Ÿà¥à¤°à¤¿à¤‚à¤— à¤•à¥€ à¤¤à¤°à¤¹ à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤|
à¤à¤• à¤‡à¤®à¥‡à¤œ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‹ à¤…à¤­à¥€ à¤­à¥€ `PIL.Image` à¤•à¥€ à¤¤à¤°à¤¹ à¤µà¥à¤¯à¤µà¤¹à¤¾à¤° à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤à¥¤

à¤‡à¤¨ à¤Ÿà¤¾à¤‡à¤ªà¥à¤¸ à¤•à¥‡ à¤¤à¥€à¤¨ à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤‰à¤¦à¥à¤¦à¥‡à¤¶à¥à¤¯ à¤¹à¥ˆà¤‚:

- à¤Ÿà¤¾à¤‡à¤ª à¤ªà¤° `to_raw` à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¸à¥‡ à¤…à¤‚à¤¤à¤°à¥à¤¨à¤¿à¤¹à¤¿à¤¤ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤°à¤¿à¤Ÿà¤°à¥à¤¨ à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤
- à¤Ÿà¤¾à¤‡à¤ª à¤ªà¤° `to_string` à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤¸à¥‡ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‹ à¤¸à¥à¤Ÿà¥à¤°à¤¿à¤‚à¤— à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤°à¤¿à¤Ÿà¤°à¥à¤¨ à¤¹à¥‹à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤: à¤µà¤¹ `AgentText` à¤•à¥‡ à¤®à¤¾à¤®à¤²à¥‡ à¤®à¥‡à¤‚ à¤¸à¥à¤Ÿà¥à¤°à¤¿à¤‚à¤— à¤¹à¥‹ à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆ à¤²à¥‡à¤•à¤¿à¤¨ à¤…à¤¨à¥à¤¯ à¤‰à¤¦à¤¾à¤¹à¤°à¤£à¥‹à¤‚ à¤®à¥‡à¤‚ à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‡ à¤¸à¥€à¤°à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥à¤¡ à¤µà¤°à¥à¤œà¤¨ à¤•à¤¾ à¤ªà¤¾à¤¥ à¤¹à¥‹à¤—à¤¾
- à¤‡à¤¸à¥‡ à¤à¤• ipython kernel à¤®à¥‡à¤‚ à¤ªà¥à¤°à¤¦à¤°à¥à¤¶à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤ªà¤° à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‹ à¤¸à¤¹à¥€ à¤¢à¤‚à¤— à¤¸à¥‡ à¤ªà¥à¤°à¤¦à¤°à¥à¤¶à¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤

### AgentText

[[autodoc]] smolagents.agent_types.AgentText

### AgentImage

[[autodoc]] smolagents.agent_types.AgentImage

### AgentAudio

[[autodoc]] smolagents.agent_types.AgentAudio



================================================
FILE: docs/source/hi/tutorials/building_good_agents.md
================================================
[Binary file]


================================================
FILE: docs/source/hi/tutorials/inspect_runs.md
================================================
# OpenTelemetry à¤•à¥‡ à¤¸à¤¾à¤¥ runs à¤•à¤¾ à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£

[[open-in-colab]]

> [!TIP]
> à¤¯à¤¦à¤¿ à¤†à¤ª à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¨à¤ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤ªà¤¹à¤²à¥‡ [à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¤¾ à¤ªà¤°à¤¿à¤šà¤¯](../conceptual_guides/intro_agents) à¤”à¤° [smolagents à¤•à¥€ à¤—à¤¾à¤‡à¤¡à¥‡à¤¡ à¤Ÿà¥‚à¤°](../guided_tour) à¤ªà¤¢à¤¼à¤¨à¤¾ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤

### Agents runs à¤•à¥‹ à¤²à¥‰à¤— à¤•à¥à¤¯à¥‹à¤‚ à¤•à¤°à¥‡à¤‚?

Agent runs à¤•à¥‹ à¤¡à¥€à¤¬à¤— à¤•à¤°à¤¨à¤¾ à¤œà¤Ÿà¤¿à¤² à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¯à¤¹ à¤¸à¤¤à¥à¤¯à¤¾à¤ªà¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤•à¤ à¤¿à¤¨ à¤¹à¥ˆ à¤•à¤¿ à¤à¤• à¤°à¤¨ à¤ à¥€à¤• à¤¸à¥‡ à¤šà¤²à¤¾ à¤¯à¤¾ à¤¨à¤¹à¥€à¤‚, à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤à¤œà¥‡à¤‚à¤Ÿ à¤µà¤°à¥à¤•à¤«à¤¼à¥à¤²à¥‹ [à¤¡à¤¿à¤œà¤¼à¤¾à¤‡à¤¨ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤…à¤ªà¥à¤°à¤¤à¥à¤¯à¤¾à¤¶à¤¿à¤¤](../conceptual_guides/intro_agents) à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚ (à¤¯à¤¦à¤¿ à¤µà¥‡ à¤ªà¥à¤°à¤¤à¥à¤¯à¤¾à¤¶à¤¿à¤¤ à¤¹à¥‹à¤¤à¥‡, à¤¤à¥‹ à¤†à¤ª à¤ªà¥à¤°à¤¾à¤¨à¥‡ à¤…à¤šà¥à¤›à¥‡ à¤•à¥‹à¤¡ à¤•à¤¾ à¤¹à¥€ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥‹à¤¤à¥‡)à¥¤

à¤”à¤° à¤°à¤¨ à¤•à¤¾ à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤°à¤¨à¤¾ à¤­à¥€ à¤•à¤ à¤¿à¤¨ à¤¹à¥ˆ: à¤®à¤²à¥à¤Ÿà¥€-à¤¸à¥à¤Ÿà¥‡à¤ª à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤œà¤²à¥à¤¦à¥€ à¤¹à¥€ à¤•à¤‚à¤¸à¥‹à¤² à¤•à¥‹ à¤²à¥‰à¤— à¤¸à¥‡ à¤­à¤° à¤¦à¥‡à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤”à¤° à¤…à¤§à¤¿à¤•à¤¾à¤‚à¤¶ à¤¤à¥à¤°à¥à¤Ÿà¤¿à¤¯à¤¾à¤‚ à¤•à¥‡à¤µà¤² "LLM dumb" à¤ªà¥à¤°à¤•à¤¾à¤° à¤•à¥€ à¤¤à¥à¤°à¥à¤Ÿà¤¿à¤¯à¤¾à¤‚ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¨à¤¸à¥‡ LLM à¤…à¤—à¤²à¥‡ à¤šà¤°à¤£ à¤®à¥‡à¤‚ à¤¬à¥‡à¤¹à¤¤à¤° à¤•à¥‹à¤¡ à¤¯à¤¾ à¤Ÿà¥‚à¤² à¤•à¥‰à¤² à¤²à¤¿à¤–à¤•à¤° à¤¸à¥à¤µà¤¯à¤‚ à¤•à¥‹ à¤¸à¥à¤§à¤¾à¤° à¤²à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤‡à¤¸à¤²à¤¿à¤ à¤¬à¤¾à¤¦ à¤•à¥‡ à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤”à¤° à¤®à¥‰à¤¨à¤¿à¤Ÿà¤°à¤¿à¤‚à¤— à¤•à¥‡ à¤²à¤¿à¤ à¤ªà¥à¤°à¥‹à¤¡à¤•à¥à¤¶à¤¨ à¤®à¥‡à¤‚ agent runs à¤•à¥‹ à¤°à¤¿à¤•à¥‰à¤°à¥à¤¡ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‡à¤‚à¤¸à¥à¤Ÿà¥à¤°à¥à¤®à¥‡à¤‚à¤Ÿà¥‡à¤¶à¤¨ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾ à¤†à¤µà¤¶à¥à¤¯à¤• à¤¹à¥ˆ!

à¤¹à¤®à¤¨à¥‡ agent runs à¤•à¥‹ à¤‡à¤‚à¤¸à¥à¤Ÿà¥à¤°à¥à¤®à¥‡à¤‚à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [OpenTelemetry](https://opentelemetry.io/) à¤®à¤¾à¤¨à¤• à¤•à¥‹ à¤…à¤ªà¤¨à¤¾à¤¯à¤¾ à¤¹à¥ˆà¥¤

à¤‡à¤¸à¤•à¤¾ à¤®à¤¤à¤²à¤¬ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª à¤¬à¤¸ à¤•à¥à¤› à¤‡à¤‚à¤¸à¥à¤Ÿà¥à¤°à¥à¤®à¥‡à¤‚à¤Ÿà¥‡à¤¶à¤¨ à¤•à¥‹à¤¡ à¤šà¤²à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤«à¤¿à¤° à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‹ à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤°à¥‚à¤ª à¤¸à¥‡ à¤šà¤²à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤”à¤° à¤¸à¤¬ à¤•à¥à¤› à¤†à¤ªà¤•à¥‡ à¤ªà¥à¤²à¥‡à¤Ÿà¤«à¥‰à¤°à¥à¤® à¤®à¥‡à¤‚ à¤²à¥‰à¤— à¤¹à¥‹ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤¯à¤¹ à¤‡à¤¸ à¤ªà¥à¤°à¤•à¤¾à¤° à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ:
à¤ªà¤¹à¤²à¥‡ à¤†à¤µà¤¶à¥à¤¯à¤• à¤ªà¥ˆà¤•à¥‡à¤œ à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¥‡à¤‚à¥¤ à¤¯à¤¹à¤¾à¤‚ à¤¹à¤® [Phoenix by Arize AI](https://github.com/Arize-ai/phoenix) à¤‡à¤‚à¤¸à¥à¤Ÿà¥‰à¤² à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¯à¤¹ à¤²à¥‰à¤—à¥à¤¸ à¤•à¥‹ à¤à¤•à¤¤à¥à¤° à¤”à¤° à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤à¤• à¤…à¤šà¥à¤›à¤¾ à¤¸à¤®à¤¾à¤§à¤¾à¤¨ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤‡à¤¸ à¤¸à¤‚à¤—à¥à¤°à¤¹ à¤”à¤° à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤­à¤¾à¤— à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤ª à¤…à¤¨à¥à¤¯ OpenTelemetry-à¤•à¤®à¥à¤ªà¥ˆà¤Ÿà¤¿à¤¬à¤² à¤ªà¥à¤²à¥‡à¤Ÿà¤«à¥‰à¤°à¥à¤®à¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```shell
pip install smolagents
pip install arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents
```

à¤«à¤¿à¤° à¤•à¤²à¥‡à¤•à¥à¤Ÿà¤° à¤•à¥‹ à¤¬à¥ˆà¤•à¤—à¥à¤°à¤¾à¤‰à¤‚à¤¡ à¤®à¥‡à¤‚ à¤šà¤²à¤¾à¤à¤‚à¥¤

```shell
python -m phoenix.server.main serve
```

à¤…à¤‚à¤¤ à¤®à¥‡à¤‚, à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥‹ à¤Ÿà¥à¤°à¥‡à¤¸ à¤•à¤°à¤¨à¥‡ à¤”à¤° à¤Ÿà¥à¤°à¥‡à¤¸ à¤•à¥‹ à¤¨à¥€à¤šà¥‡ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤à¤‚à¤¡à¤ªà¥‰à¤‡à¤‚à¤Ÿ à¤ªà¤° Phoenix à¤•à¥‹ à¤­à¥‡à¤œà¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `SmolagentsInstrumentor` à¤•à¥‹ à¤¸à¥‡à¤Ÿ à¤•à¤°à¥‡à¤‚à¥¤

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

from openinference.instrumentation.smolagents import SmolagentsInstrumentor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

endpoint = "http://0.0.0.0:6006/v1/traces"
trace_provider = TracerProvider()
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))

SmolagentsInstrumentor().instrument(tracer_provider=trace_provider)
```
à¤¤à¤¬ à¤†à¤ª à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤šà¤²à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚!

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    VisitWebpageTool,
    InferenceClientModel,
)

model = InferenceClientModel()

managed_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="managed_agent",
    description="This is an agent that can do web search.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[managed_agent],
)
manager_agent.run(
    "If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?"
)
```
à¤”à¤° à¤«à¤¿à¤° à¤†à¤ª à¤…à¤ªà¤¨à¥‡ à¤°à¤¨ à¤•à¤¾ à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `http://0.0.0.0:6006/projects/` à¤ªà¤° à¤œà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚!

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/inspect_run_phoenix.png">

à¤†à¤ª à¤¦à¥‡à¤– à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¤¿ CodeAgent à¤¨à¥‡ à¤…à¤ªà¤¨à¥‡ à¤®à¥ˆà¤¨à¥‡à¤œà¥à¤¡ ToolCallingAgent à¤•à¥‹ (à¤µà¥ˆà¤¸à¥‡, à¤®à¥ˆà¤¨à¥‡à¤œà¥à¤¡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤à¤• CodeAgent à¤­à¥€ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¥à¤¾) U.S. 2024 à¤—à¥à¤°à¥‹à¤¥ à¤°à¥‡à¤Ÿ à¤•à¥‡ à¤²à¤¿à¤ à¤µà¥‡à¤¬ à¤¸à¤°à¥à¤š à¤šà¤²à¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥‰à¤² à¤•à¤¿à¤¯à¤¾à¥¤ à¤«à¤¿à¤° à¤®à¥ˆà¤¨à¥‡à¤œà¥à¤¡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¨à¥‡ à¤…à¤ªà¤¨à¥€ à¤°à¤¿à¤ªà¥‹à¤°à¥à¤Ÿ à¤²à¥Œà¤Ÿà¤¾à¤ˆ à¤”à¤° à¤®à¥ˆà¤¨à¥‡à¤œà¤° à¤à¤œà¥‡à¤‚à¤Ÿ à¤¨à¥‡ à¤…à¤°à¥à¤¥à¤µà¥à¤¯à¤µà¤¸à¥à¤¥à¤¾ à¤•à¥‡ à¤¦à¥‹à¤—à¥à¤¨à¤¾ à¤¹à¥‹à¤¨à¥‡ à¤•à¤¾ à¤¸à¤®à¤¯ à¤—à¤£à¤¨à¤¾ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤¸ à¤ªà¤° à¤•à¤¾à¤°à¥à¤¯ à¤•à¤¿à¤¯à¤¾! à¤…à¤šà¥à¤›à¤¾ à¤¹à¥ˆ, à¤¹à¥ˆ à¤¨à¤¾?


================================================
FILE: docs/source/hi/tutorials/secure_code_execution.md
================================================
[Binary file]


================================================
FILE: docs/source/hi/tutorials/tools.md
================================================
# Tools

[[open-in-colab]]

à¤¯à¤¹à¤¾à¤, à¤¹à¤® à¤à¤¡à¤µà¤¾à¤‚à¤¸à¥à¤¡ tools à¤‰à¤ªà¤¯à¥‹à¤— à¤¦à¥‡à¤–à¥‡à¤‚à¤—à¥‡à¥¤

> [!TIP]
> à¤¯à¤¦à¤¿ à¤†à¤ª à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤®à¥‡à¤‚ à¤¨à¤ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤¸à¤¬à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ [à¤à¤œà¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¤¾ à¤ªà¤°à¤¿à¤šà¤¯](../conceptual_guides/intro_agents) à¤”à¤° [smolagents à¤•à¥€ à¤—à¤¾à¤‡à¤¡à¥‡à¤¡ à¤Ÿà¥‚à¤°](../guided_tour) à¤ªà¤¢à¤¼à¤¨à¤¾ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¥‡à¤‚à¥¤

- [Tools](#tools)
    - [à¤Ÿà¥‚à¤² à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ, à¤”à¤° à¤‡à¤¸à¥‡ à¤•à¥ˆà¤¸à¥‡ à¤¬à¤¨à¤¾à¤à¤‚?](#à¤Ÿà¥‚à¤²-à¤•à¥à¤¯à¤¾-à¤¹à¥ˆ-à¤”à¤°-à¤‡à¤¸à¥‡-à¤•à¥ˆà¤¸à¥‡-à¤¬à¤¨à¤¾à¤à¤‚)
    - [à¤…à¤ªà¤¨à¤¾ à¤Ÿà¥‚à¤² à¤¹à¤¬ à¤ªà¤° à¤¶à¥‡à¤¯à¤° à¤•à¤°à¥‡à¤‚](#à¤…à¤ªà¤¨à¤¾-à¤Ÿà¥‚à¤²-à¤¹à¤¬-à¤ªà¤°-à¤¶à¥‡à¤¯à¤°-à¤•à¤°à¥‡à¤‚)
    - [à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‹ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¥‡à¤‚](#à¤¸à¥à¤ªà¥‡à¤¸-à¤•à¥‹-à¤Ÿà¥‚à¤²-à¤•à¥‡-à¤°à¥‚à¤ª-à¤®à¥‡à¤‚-à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ-à¤•à¤°à¥‡à¤‚)
    - [LangChain à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](#LangChain-à¤Ÿà¥‚à¤²à¥à¤¸-à¤•à¤¾-à¤‰à¤ªà¤¯à¥‹à¤—-à¤•à¤°à¥‡à¤‚)
    - [à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥‹ à¤®à¥ˆà¤¨à¥‡à¤œ à¤•à¤°à¥‡à¤‚](#à¤…à¤ªà¤¨à¥‡-à¤à¤œà¥‡à¤‚à¤Ÿ-à¤•à¥‡-à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸-à¤•à¥‹-à¤®à¥ˆà¤¨à¥‡à¤œ-à¤•à¤°à¥‡à¤‚)
    - [à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚](#à¤Ÿà¥‚à¤²à¥à¤¸-à¤•à¤¾-à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨-à¤‰à¤ªà¤¯à¥‹à¤—-à¤•à¤°à¥‡à¤‚)

### à¤Ÿà¥‚à¤² à¤•à¥à¤¯à¤¾ à¤¹à¥ˆ à¤”à¤° à¤‡à¤¸à¥‡ à¤•à¥ˆà¤¸à¥‡ à¤¬à¤¨à¤¾à¤à¤‚

à¤Ÿà¥‚à¤² à¤®à¥à¤–à¥à¤¯ à¤°à¥‚à¤ª à¤¸à¥‡ à¤à¤• à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨ à¤¹à¥ˆ à¤œà¤¿à¤¸à¥‡ à¤à¤• LLM à¤à¤œà¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤®à¥‡à¤‚ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤²à¥‡à¤•à¤¿à¤¨ à¤‡à¤¸à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, LLM à¤•à¥‹ à¤à¤• API à¤¦à¥€ à¤œà¤¾à¤à¤—à¥€: à¤¨à¤¾à¤®, à¤Ÿà¥‚à¤² à¤µà¤¿à¤µà¤°à¤£, à¤‡à¤¨à¤ªà¥à¤Ÿ à¤ªà¥à¤°à¤•à¤¾à¤° à¤”à¤° à¤µà¤¿à¤µà¤°à¤£, à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤ªà¥à¤°à¤•à¤¾à¤°à¥¤

à¤‡à¤¸à¤²à¤¿à¤ à¤¯à¤¹ à¤•à¥‡à¤µà¤² à¤à¤• à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨ à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾à¥¤ à¤¯à¤¹ à¤à¤• à¤•à¥à¤²à¤¾à¤¸ à¤¹à¥‹à¤¨à¥€ à¤šà¤¾à¤¹à¤¿à¤à¥¤

à¤¤à¥‹ à¤®à¥‚à¤² à¤°à¥‚à¤ª à¤¸à¥‡, à¤Ÿà¥‚à¤² à¤à¤• à¤•à¥à¤²à¤¾à¤¸ à¤¹à¥ˆ à¤œà¥‹ à¤à¤• à¤«à¤¼à¤‚à¤•à¥à¤¶à¤¨ à¤•à¥‹ à¤®à¥‡à¤Ÿà¤¾à¤¡à¥‡à¤Ÿà¤¾ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤°à¥ˆà¤ª à¤•à¤°à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ LLM à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆ à¤•à¤¿ à¤‡à¤¸à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥ˆà¤¸à¥‡ à¤•à¤°à¥‡à¤‚à¥¤

à¤¯à¤¹ à¤•à¥ˆà¤¸à¤¾ à¤¦à¤¿à¤–à¤¤à¤¾ à¤¹à¥ˆ:

```python
from smolagents import Tool

class HFModelDownloadsTool(Tool):
    name = "model_download_counter"
    description = """
    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
    It returns the name of the checkpoint."""
    inputs = {
        "task": {
            "type": "string",
            "description": "the task category (such as text-classification, depth-estimation, etc)",
        }
    }
    output_type = "string"

    def forward(self, task: str):
        from huggingface_hub import list_models

        model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
        return model.id

model_downloads_tool = HFModelDownloadsTool()
```

à¤•à¤¸à¥à¤Ÿà¤® à¤Ÿà¥‚à¤² `Tool` à¤•à¥‹ à¤¸à¤¬à¤•à¥à¤²à¤¾à¤¸ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤‰à¤ªà¤¯à¥‹à¤—à¥€ à¤®à¥‡à¤¥à¤¡à¥à¤¸ à¤•à¥‹ à¤‡à¤¨à¤¹à¥‡à¤°à¤¿à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤à¥¤ à¤šà¤¾à¤‡à¤²à¥à¤¡ à¤•à¥à¤²à¤¾à¤¸ à¤­à¥€ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆ:
- à¤à¤• `name` à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿ, à¤œà¥‹ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤¨à¤¾à¤® à¤¸à¥‡ à¤¸à¤‚à¤¬à¤‚à¤§à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤¨à¤¾à¤® à¤†à¤®à¤¤à¥Œà¤° à¤ªà¤° à¤¬à¤¤à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤Ÿà¥‚à¤² à¤•à¥à¤¯à¤¾ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤šà¥‚à¤‚à¤•à¤¿ à¤•à¥‹à¤¡ à¤à¤• à¤Ÿà¤¾à¤¸à¥à¤• à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¬à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤¡à¤¾à¤‰à¤¨à¤²à¥‹à¤¡ à¤µà¤¾à¤²à¥‡ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤°à¤¿à¤Ÿà¤°à¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ, à¤‡à¤¸à¤²à¤¿à¤ à¤‡à¤¸à¥‡ `model_download_counter` à¤¨à¤¾à¤® à¤¦à¥‡à¤‚à¥¤
- à¤à¤• `description` à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ à¤•à¥‹ à¤ªà¥‰à¤ªà¥à¤²à¥‡à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤
- à¤à¤• `inputs` à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿ, à¤œà¥‹ `"type"` à¤”à¤° `"description"` keys à¤µà¤¾à¤²à¤¾ à¤¡à¤¿à¤•à¥à¤¶à¤¨à¤°à¥€ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤®à¥‡à¤‚ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ à¤œà¥‹ à¤ªà¤¾à¤¯à¤¥à¤¨ à¤‡à¤‚à¤Ÿà¤°à¤ªà¥à¤°à¥‡à¤Ÿà¤° à¤•à¥‹ à¤‡à¤¨à¤ªà¥à¤Ÿ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤µà¤¿à¤•à¤²à¥à¤ª à¤šà¥à¤¨à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¤¤à¥€ à¤¹à¥ˆà¥¤
- à¤à¤• `output_type` à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿ, à¤œà¥‹ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤Ÿà¤¾à¤‡à¤ª à¤•à¥‹ à¤¨à¤¿à¤°à¥à¤¦à¤¿à¤·à¥à¤Ÿ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ `inputs` à¤”à¤° `output_type` à¤¦à¥‹à¤¨à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤Ÿà¤¾à¤‡à¤ª [Pydantic formats](https://docs.pydantic.dev/latest/concepts/json_schema/#generating-json-schema) à¤¹à¥‹à¤¨à¥‡ à¤šà¤¾à¤¹à¤¿à¤, à¤µà¥‡ à¤‡à¤¨à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥‹à¤ˆ à¤­à¥€ à¤¹à¥‹ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚: `["string", "boolean","integer", "number", "image", "audio", "array", "object", "any", "null"]`à¥¤
- à¤à¤• `forward` à¤®à¥‡à¤¥à¤¡ à¤œà¤¿à¤¸à¤®à¥‡à¤‚ à¤à¤•à¥à¤œà¥€à¤•à¥à¤¯à¥‚à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¨à¥‡ à¤µà¤¾à¤²à¤¾ à¤‡à¤¨à¥à¤«à¤°à¥‡à¤‚à¤¸ à¤•à¥‹à¤¡ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤

à¤à¤œà¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤ à¤œà¤¾à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤‡à¤¤à¤¨à¤¾ à¤¹à¥€ à¤šà¤¾à¤¹à¤¿à¤!

à¤Ÿà¥‚à¤² à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¤¾ à¤à¤• à¤”à¤° à¤¤à¤°à¥€à¤•à¤¾ à¤¹à¥ˆà¥¤ [guided_tour](../guided_tour) à¤®à¥‡à¤‚, à¤¹à¤®à¤¨à¥‡ `@tool` à¤¡à¥‡à¤•à¥‹à¤°à¥‡à¤Ÿà¤° à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤à¤• à¤Ÿà¥‚à¤² à¤•à¥‹ à¤²à¤¾à¤—à¥‚ à¤•à¤¿à¤¯à¤¾à¥¤ [`tool`] à¤¡à¥‡à¤•à¥‹à¤°à¥‡à¤Ÿà¤° à¤¸à¤°à¤² à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤…à¤¨à¥à¤¶à¤‚à¤¸à¤¿à¤¤ à¤¤à¤°à¥€à¤•à¤¾ à¤¹à¥ˆ, à¤²à¥‡à¤•à¤¿à¤¨ à¤•à¤­à¥€-à¤•à¤­à¥€ à¤†à¤ªà¤•à¥‹ à¤‡à¤¸à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ: à¤…à¤§à¤¿à¤• à¤¸à¥à¤ªà¤·à¥à¤Ÿà¤¤à¤¾ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤•à¥à¤²à¤¾à¤¸ à¤®à¥‡à¤‚ à¤•à¤ˆ à¤®à¥‡à¤¥à¤¡à¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾, à¤¯à¤¾ à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤ à¤•à¥à¤²à¤¾à¤¸ à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿà¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾à¥¤

à¤‡à¤¸ à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤®à¥‡à¤‚, à¤†à¤ª à¤Šà¤ªà¤° à¤¬à¤¤à¤¾à¤ à¤…à¤¨à¥à¤¸à¤¾à¤° [`Tool`] à¤•à¥‹ à¤¸à¤¬à¤•à¥à¤²à¤¾à¤¸ à¤•à¤°à¤•à¥‡ à¤…à¤ªà¤¨à¤¾ à¤Ÿà¥‚à¤² à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

### à¤…à¤ªà¤¨à¤¾ à¤Ÿà¥‚à¤² à¤¹à¤¬ à¤ªà¤° à¤¶à¥‡à¤¯à¤° à¤•à¤°à¥‡à¤‚

à¤†à¤ª à¤Ÿà¥‚à¤² à¤ªà¤° [`~Tool.push_to_hub`] à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤•à¥‡ à¤…à¤ªà¤¨à¤¾ à¤•à¤¸à¥à¤Ÿà¤® à¤Ÿà¥‚à¤² à¤¹à¤¬ à¤ªà¤° à¤¶à¥‡à¤¯à¤° à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¥‡à¤‚ à¤•à¤¿ à¤†à¤ªà¤¨à¥‡ à¤¹à¤¬ à¤ªà¤° à¤‡à¤¸à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤¬à¤¨à¤¾à¤ˆ à¤¹à¥ˆ à¤”à¤° à¤†à¤ª à¤°à¥€à¤¡ à¤à¤•à¥à¤¸à¥‡à¤¸ à¤µà¤¾à¤²à¤¾ à¤Ÿà¥‹à¤•à¤¨ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚à¥¤

```python
model_downloads_tool.push_to_hub("{your_username}/hf-model-downloads", token="<YOUR_HUGGINGFACEHUB_API_TOKEN>")
```

à¤¹à¤¬ à¤ªà¤° à¤ªà¥à¤¶ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤¾à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤†à¤ªà¤•à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‹ à¤•à¥à¤› à¤¨à¤¿à¤¯à¤®à¥‹à¤‚ à¤•à¤¾ à¤ªà¤¾à¤²à¤¨ à¤•à¤°à¤¨à¤¾ à¤¹à¥‹à¤—à¤¾:
- à¤¸à¤­à¥€ à¤®à¥‡à¤¥à¤¡à¥à¤¸ à¤¸à¥‡à¤²à¥à¤«-à¤•à¤‚à¤Ÿà¥‡à¤¨à¥à¤¡ à¤¹à¥ˆà¤‚, à¤¯à¤¾à¤¨à¥€ à¤‰à¤¨à¤•à¥‡ à¤†à¤°à¥à¤—à¥à¤¸ à¤¸à¥‡ à¤†à¤¨à¥‡ à¤µà¤¾à¤²à¥‡ à¤µà¥‡à¤°à¤¿à¤à¤¬à¤²à¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚à¥¤
- à¤‰à¤ªà¤°à¥‹à¤•à¥à¤¤ à¤¬à¤¿à¤‚à¤¦à¥ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤°, **à¤¸à¤­à¥€ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿà¥à¤¸ à¤•à¥‹ à¤¸à¥€à¤§à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤«à¤¼à¤‚à¤•à¥à¤¶à¤‚à¤¸ à¤•à¥‡ à¤­à¥€à¤¤à¤° à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¨à¤¾ à¤šà¤¾à¤¹à¤¿à¤**, à¤…à¤¨à¥à¤¯à¤¥à¤¾ à¤†à¤ªà¤•à¥‹ à¤…à¤ªà¤¨à¥‡ à¤•à¤¸à¥à¤Ÿà¤® à¤Ÿà¥‚à¤² à¤•à¥‡ à¤¸à¤¾à¤¥ [`~Tool.save`] à¤¯à¤¾ [`~Tool.push_to_hub`] à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤ªà¥à¤°à¤¯à¤¾à¤¸ à¤•à¤°à¤¤à¥‡ à¤¸à¤®à¤¯ à¤à¤°à¤° à¤®à¤¿à¤²à¥‡à¤—à¤¾à¥¤
- à¤¯à¤¦à¤¿ à¤†à¤ª `__init__` à¤µà¤¿à¤§à¤¿ à¤•à¥‹ à¤¸à¤¬à¤•à¥à¤²à¤¾à¤¸ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤†à¤ª à¤‡à¤¸à¥‡ `self` à¤•à¥‡ à¤…à¤²à¤¾à¤µà¤¾ à¤•à¥‹à¤ˆ à¤…à¤¨à¥à¤¯ à¤†à¤°à¥à¤—à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿ à¤¨à¤¹à¥€à¤‚ à¤¦à¥‡ à¤¸à¤•à¤¤à¥‡à¥¤ à¤à¤¸à¤¾ à¤‡à¤¸à¤²à¤¿à¤ à¤¹à¥ˆ à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤•à¤¿à¤¸à¥€ à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤Ÿà¥‚à¤² à¤‡à¤‚à¤¸à¥à¤Ÿà¥‡à¤‚à¤¸ à¤•à¥‡ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¥‡à¤¶à¤¨ à¤•à¥‡ à¤¦à¥Œà¤°à¤¾à¤¨ à¤¸à¥‡à¤Ÿ à¤•à¤¿à¤ à¤—à¤ à¤¤à¤°à¥à¤•à¥‹à¤‚ à¤•à¥‹ à¤†à¤°à¥à¤—à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¤°à¤¨à¤¾ à¤•à¤ à¤¿à¤¨ à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¥‹ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤¹à¤¬ à¤ªà¤° à¤ à¥€à¤• à¤¸à¥‡ à¤¸à¤¾à¤à¤¾ à¤•à¤°à¤¨à¥‡ à¤¸à¥‡ à¤°à¥‹à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤”à¤° à¤µà¥ˆà¤¸à¥‡ à¤­à¥€, à¤à¤• à¤µà¤¿à¤¶à¤¿à¤·à¥à¤Ÿ à¤•à¥à¤²à¤¾à¤¸ à¤¬à¤¨à¤¾à¤¨à¥‡ à¤•à¤¾ à¤µà¤¿à¤šà¤¾à¤° à¤¯à¤¹ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª à¤¹à¤¾à¤°à¥à¤¡-à¤•à¥‹à¤¡ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤µà¤¶à¥à¤¯à¤• à¤•à¤¿à¤¸à¥€ à¤­à¥€ à¤šà¥€à¤œà¤¼ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¥à¤²à¤¾à¤¸ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤ à¤ªà¤¹à¤²à¥‡ à¤¸à¥‡ à¤¹à¥€ à¤¸à¥‡à¤Ÿ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ (à¤¬à¤¸ `your_variable=(...)` à¤•à¥‹ à¤¸à¥€à¤§à¥‡ `class YourTool(Tool):` à¤ªà¤‚à¤•à¥à¤¤à¤¿ à¤•à¥‡ à¤…à¤‚à¤¤à¤°à¥à¤—à¤¤ à¤¸à¥‡à¤Ÿ à¤•à¤°à¥‡à¤‚ ). à¤”à¤° à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤†à¤ª à¤…à¤­à¥€ à¤­à¥€ `self.your_variable` à¤•à¥‹ à¤…à¤¸à¤¾à¤‡à¤¨ à¤•à¤°à¤•à¥‡ à¤…à¤ªà¤¨à¥‡ à¤•à¥‹à¤¡ à¤®à¥‡à¤‚ à¤•à¤¹à¥€à¤‚ à¤­à¥€ à¤à¤• à¤•à¥à¤²à¤¾à¤¸ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾ à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤


à¤à¤• à¤¬à¤¾à¤° à¤œà¤¬ à¤†à¤ªà¤•à¤¾ à¤Ÿà¥‚à¤² à¤¹à¤¬ à¤ªà¤° à¤ªà¥à¤¶ à¤¹à¥‹ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤¤à¥‹ à¤†à¤ª à¤‡à¤¸à¥‡ à¤µà¤¿à¤œà¤¼à¥à¤…à¤²à¤¾à¤‡à¤œà¤¼ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ [à¤¯à¤¹à¤¾à¤](https://huggingface.co/spaces/m-ric/hf-model-downloads) `model_downloads_tool` à¤¹à¥ˆ à¤œà¤¿à¤¸à¥‡ à¤®à¥ˆà¤‚à¤¨à¥‡ à¤ªà¥à¤¶ à¤•à¤¿à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤‡à¤¸à¤®à¥‡à¤‚ à¤à¤• à¤…à¤šà¥à¤›à¤¾ à¤—à¥à¤°à¥‡à¤¡à¤¿à¤¯à¥‹ à¤‡à¤‚à¤Ÿà¤°à¤«à¤¼à¥‡à¤¸ à¤¹à¥ˆà¥¤

à¤Ÿà¥‚à¤² à¤«à¤¼à¤¾à¤‡à¤²à¥‹à¤‚ à¤®à¥‡à¤‚ à¤—à¤¹à¤°à¤¾à¤ˆ à¤¸à¥‡ à¤œà¤¾à¤¨à¥‡ à¤ªà¤°, à¤†à¤ª à¤ªà¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤¸à¤¾à¤°à¥€ à¤Ÿà¥‚à¤² à¤²à¥‰à¤œà¤¿à¤• [tool.py](https://huggingface.co/spaces/m-ric/hf-model-downloads/blob/main/tool.py) à¤•à¥‡ à¤…à¤‚à¤¤à¤°à¥à¤—à¤¤ à¤¹à¥ˆà¥¤ à¤¯à¤¹à¥€à¤‚ à¤†à¤ª à¤•à¤¿à¤¸à¥€ à¤”à¤° à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤¶à¥‡à¤¯à¤° à¤•à¤¿à¤ à¤—à¤ à¤Ÿà¥‚à¤² à¤•à¤¾ à¤¨à¤¿à¤°à¥€à¤•à¥à¤·à¤£ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

à¤«à¤¿à¤° à¤†à¤ª à¤Ÿà¥‚à¤² à¤•à¥‹ [`load_tool`] à¤•à¥‡ à¤¸à¤¾à¤¥ à¤²à¥‹à¤¡ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤¯à¤¾ [`~Tool.from_hub`] à¤•à¥‡ à¤¸à¤¾à¤¥ à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤‡à¤¸à¥‡ à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚ `tools` à¤ªà¥ˆà¤°à¤¾à¤®à¥€à¤Ÿà¤° à¤®à¥‡à¤‚ à¤ªà¤¾à¤¸ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
à¤šà¥‚à¤‚à¤•à¤¿ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤šà¤²à¤¾à¤¨à¥‡ à¤•à¤¾ à¤®à¤¤à¤²à¤¬ à¤•à¤¸à¥à¤Ÿà¤® à¤•à¥‹à¤¡ à¤šà¤²à¤¾à¤¨à¤¾ à¤¹à¥ˆ, à¤†à¤ªà¤•à¥‹ à¤¯à¤¹ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤¹à¥‹à¤—à¤¾ à¤•à¤¿ à¤†à¤ª à¤°à¤¿à¤ªà¥‰à¤œà¤¿à¤Ÿà¤°à¥€ à¤ªà¤° à¤­à¤°à¥‹à¤¸à¤¾ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤‡à¤¸à¤²à¤¿à¤ à¤¹à¤® à¤¹à¤¬ à¤¸à¥‡ à¤Ÿà¥‚à¤² à¤²à¥‹à¤¡ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `trust_remote_code=True` à¤ªà¤¾à¤¸ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤°à¤–à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```python
from smolagents import load_tool, CodeAgent

model_download_tool = load_tool(
    "{your_username}/hf-model-downloads",
    trust_remote_code=True
)
```

### à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‹ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¥‡à¤‚

à¤†à¤ª [`Tool.from_space`] à¤®à¥‡à¤¥à¤¡ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤¹à¤¬ à¤¸à¥‡ à¤à¤• à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‹ à¤¸à¥€à¤§à¥‡ à¤Ÿà¥‚à¤² à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚!

à¤†à¤ªà¤•à¥‹ à¤•à¥‡à¤µà¤² à¤¹à¤¬ à¤ªà¤° à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥€ ID, à¤‡à¤¸à¤•à¤¾ à¤¨à¤¾à¤®, à¤”à¤° à¤à¤• à¤µà¤¿à¤µà¤°à¤£ à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥ˆ à¤œà¥‹ à¤†à¤ªà¤•à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤¸à¤®à¤à¤¨à¥‡ à¤®à¥‡à¤‚ à¤®à¤¦à¤¦ à¤•à¤°à¥‡à¤—à¤¾ à¤•à¤¿ à¤Ÿà¥‚à¤² à¤•à¥à¤¯à¤¾ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤ à¤…à¤‚à¤¦à¤° à¤¸à¥‡, à¤¯à¤¹ à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‹ à¤•à¥‰à¤² à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ [`gradio-client`](https://pypi.org/project/gradio-client/) à¤²à¤¾à¤‡à¤¬à¥à¤°à¥‡à¤°à¥€ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤—à¤¾à¥¤

à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤, à¤šà¤²à¤¿à¤ à¤¹à¤¬ à¤¸à¥‡ [FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) à¤¸à¥à¤ªà¥‡à¤¸ à¤•à¥‹ à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¥‡à¤‚ à¤”à¤° à¤‡à¤¸à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤à¤• à¤‡à¤®à¥‡à¤œ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤•à¤°à¥‡à¤‚à¥¤

```python
image_generation_tool = Tool.from_space(
    "black-forest-labs/FLUX.1-schnell",
    name="image_generator",
    description="Generate an image from a prompt"
)

image_generation_tool("A sunny beach")
```
à¤”à¤° à¤¦à¥‡à¤–à¥‹, à¤¯à¤¹ à¤¤à¥à¤®à¥à¤¹à¤¾à¤°à¥€ à¤›à¤µà¤¿ à¤¹à¥ˆ! ðŸ–ï¸

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sunny_beach.webp">

à¤«à¤¿à¤° à¤†à¤ª à¤‡à¤¸ à¤Ÿà¥‚à¤² à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¸à¥€ à¤…à¤¨à¥à¤¯ à¤Ÿà¥‚à¤² à¤•à¥€ à¤¤à¤°à¤¹ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤•à¥‡ à¤²à¤¿à¤, à¤šà¤²à¤¿à¤ à¤ªà¥à¤°à¥‰à¤®à¥à¤ªà¥à¤Ÿ `a rabbit wearing a space suit` à¤•à¥‹ à¤¸à¥à¤§à¤¾à¤°à¥‡à¤‚ à¤”à¤° à¤‡à¤¸à¤•à¥€ à¤à¤• à¤‡à¤®à¥‡à¤œ à¤œà¤¨à¤°à¥‡à¤Ÿ à¤•à¤°à¥‡à¤‚à¥¤ à¤¯à¤¹ à¤‰à¤¦à¤¾à¤¹à¤°à¤£ à¤¯à¤¹ à¤­à¥€ à¤¦à¤¿à¤–à¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤…à¤¤à¤¿à¤°à¤¿à¤•à¥à¤¤ à¤†à¤°à¥à¤—à¥à¤¯à¥‚à¤®à¥‡à¤‚à¤Ÿà¥à¤¸ à¤•à¥ˆà¤¸à¥‡ à¤ªà¤¾à¤¸ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```python
from smolagents import CodeAgent, InferenceClientModel

model = InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking")
agent = CodeAgent(tools=[image_generation_tool], model=model)

agent.run(
    "Improve this prompt, then generate an image of it.", additional_args={'user_prompt': 'A rabbit wearing a space suit'}
)
```

```text
=== Agent thoughts:
improved_prompt could be "A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background"

Now that I have improved the prompt, I can use the image generator tool to generate an image based on this prompt.
>>> Agent is executing the code below:
image = image_generator(prompt="A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background")
final_answer(image)
```

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit_spacesuit_flux.webp">

à¤¯à¤¹ à¤•à¤¿à¤¤à¤¨à¤¾ à¤•à¥‚à¤² à¤¹à¥ˆ? ðŸ¤©

### LangChain à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚

à¤¹à¤® LangChain à¤•à¥‹ à¤ªà¤¸à¤‚à¤¦ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤®à¤¾à¤¨à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤•à¤¿ à¤‡à¤¸à¤•à¥‡ à¤ªà¤¾à¤¸ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤à¤• à¤¬à¤¹à¥à¤¤ à¤†à¤•à¤°à¥à¤·à¤• à¤¸à¤‚à¤—à¥à¤°à¤¹ à¤¹à¥ˆà¥¤
LangChain à¤¸à¥‡ à¤à¤• à¤Ÿà¥‚à¤² à¤‡à¤®à¥à¤ªà¥‹à¤°à¥à¤Ÿ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, `from_langchain()` à¤®à¥‡à¤¥à¤¡ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚à¥¤

à¤¯à¤¹à¤¾à¤ à¤¬à¤¤à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤†à¤ª LangChain à¤µà¥‡à¤¬ à¤¸à¤°à¥à¤š à¤Ÿà¥‚à¤² à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤ªà¤°à¤¿à¤šà¤¯ à¤•à¥‡ à¤¸à¤°à¥à¤š à¤°à¤¿à¤œà¤²à¥à¤Ÿ à¤•à¥‹ à¤•à¥ˆà¤¸à¥‡ à¤«à¤¿à¤° à¤¸à¥‡ à¤¬à¤¨à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
à¤‡à¤¸ à¤Ÿà¥‚à¤² à¤•à¥‹ à¤•à¤¾à¤® à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ `pip install langchain google-search-results -q` à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤—à¥€à¥¤
```python
from langchain.agents import load_tools

search_tool = Tool.from_langchain(load_tools(["serpapi"])[0])

agent = CodeAgent(tools=[search_tool], model=model)

agent.run("How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?")
```

### à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥‹ à¤®à¥ˆà¤¨à¥‡à¤œ à¤•à¤°à¥‡à¤‚

à¤†à¤ª à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‡ à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥‹ `agent.tools` à¤à¤Ÿà¥à¤°à¤¿à¤¬à¥à¤¯à¥‚à¤Ÿ à¤®à¥‡à¤‚ à¤à¤• à¤Ÿà¥‚à¤² à¤œà¥‹à¤¡à¤¼à¤•à¤° à¤¯à¤¾ à¤¬à¤¦à¤²à¤•à¤° à¤®à¥ˆà¤¨à¥‡à¤œ à¤•à¤° à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤•à¥à¤¯à¥‹à¤‚à¤•à¤¿ à¤¯à¤¹ à¤à¤• à¤¸à¥à¤Ÿà¥ˆà¤‚à¤¡à¤°à¥à¤¡ à¤¡à¤¿à¤•à¥à¤¶à¤¨à¤°à¥€ à¤¹à¥ˆà¥¤

à¤šà¤²à¤¿à¤ à¤•à¥‡à¤µà¤² à¤¡à¤¿à¤«à¤¼à¥‰à¤²à¥à¤Ÿ à¤Ÿà¥‚à¤²à¤¬à¥‰à¤•à¥à¤¸ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¤¼ à¤•à¤¿à¤ à¤—à¤ à¤®à¥Œà¤œà¥‚à¤¦à¤¾ à¤à¤œà¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚ `model_download_tool` à¤œà¥‹à¤¡à¤¼à¥‡à¤‚à¥¤

```python
from smolagents import InferenceClientModel

model = InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking")

agent = CodeAgent(tools=[], model=model, add_base_tools=True)
agent.tools[model_download_tool.name] = model_download_tool
```
à¤…à¤¬ à¤¹à¤® à¤¨à¤ à¤Ÿà¥‚à¤² à¤•à¤¾ à¤²à¤¾à¤­ à¤‰à¤ à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

```python
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub but reverse the letters?"
)
```


> [!TIP]
> à¤à¤œà¥‡à¤‚à¤Ÿ à¤®à¥‡à¤‚ à¤¬à¤¹à¥à¤¤ à¤…à¤§à¤¿à¤• à¤Ÿà¥‚à¤²à¥à¤¸ à¤¨ à¤œà¥‹à¤¡à¤¼à¤¨à¥‡ à¤¸à¥‡ à¤¸à¤¾à¤µà¤§à¤¾à¤¨ à¤°à¤¹à¥‡à¤‚: à¤¯à¤¹ à¤•à¤®à¤œà¥‹à¤° LLM à¤‡à¤‚à¤œà¤¨ à¤•à¥‹ à¤“à¤µà¤°à¤µà¥à¤¹à¥‡à¤²à¥à¤® à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤


### à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¥‡à¤‚

à¤†à¤ª `ToolCollection` à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤Ÿà¥‚à¤² à¤•à¤²à¥‡à¤•à¥à¤¶à¤‚à¤¸ à¤•à¤¾ à¤²à¤¾à¤­ à¤‰à¤ à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¹ à¤¯à¤¾ à¤¤à¥‹ à¤¹à¤¬ à¤¸à¥‡ à¤à¤• à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤¯à¤¾ MCP à¤¸à¤°à¥à¤µà¤° à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤²à¥‹à¤¡ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

#### à¤¹à¤¬ à¤®à¥‡à¤‚ à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤¸à¥‡ à¤Ÿà¥‚à¤² à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨

à¤†à¤ª à¤‰à¤¸ à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨ à¤•à¥‡ à¤¸à¥à¤²à¤— à¤•à¥‡ à¤¸à¤¾à¤¥ à¤‡à¤¸à¤•à¤¾ à¤²à¤¾à¤­ à¤‰à¤ à¤¾ à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¿à¤¸à¤•à¤¾ à¤†à¤ª à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤
à¤«à¤¿à¤° à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤…à¤ªà¤¨à¥‡ à¤à¤œà¥‡à¤‚à¤Ÿ à¤•à¥‹ à¤‡à¤¨à¤¿à¤¶à¤¿à¤¯à¤²à¤¾à¤‡à¤œà¤¼ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤ à¤à¤• à¤²à¤¿à¤¸à¥à¤Ÿ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤ªà¤¾à¤¸ à¤•à¤°à¥‡à¤‚, à¤”à¤° à¤‰à¤¨à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤¶à¥à¤°à¥‚ à¤•à¤°à¥‡à¤‚!

```py
from smolagents import ToolCollection, CodeAgent

image_tool_collection = ToolCollection.from_hub(
    collection_slug="huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f",
    token="<YOUR_HUGGINGFACEHUB_API_TOKEN>"
)
agent = CodeAgent(tools=[*image_tool_collection.tools], model=model, add_base_tools=True)

agent.run("Please draw me a picture of rivers and lakes.")
```

à¤¸à¥à¤Ÿà¤¾à¤°à¥à¤Ÿ à¤•à¥‹ à¤¤à¥‡à¤œ à¤•à¤°à¤¨à¥‡ à¤•à¥‡ à¤²à¤¿à¤, à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‡à¤µà¤² à¤¤à¤­à¥€ à¤²à¥‹à¤¡ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤œà¤¬ à¤à¤œà¥‡à¤‚à¤Ÿ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤•à¥‰à¤² à¤•à¤¿à¤ à¤œà¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤

#### à¤•à¤¿à¤¸à¥€ à¤­à¥€ MCP à¤¸à¤°à¥à¤µà¤° à¤¸à¥‡ à¤Ÿà¥‚à¤² à¤•à¤²à¥‡à¤•à¥à¤¶à¤¨

[glama.ai](https://glama.ai/mcp/servers) à¤¯à¤¾ [smithery.ai](https://smithery.ai/) à¤ªà¤° à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤¸à¥ˆà¤•à¤¡à¤¼à¥‹à¤‚ MCP à¤¸à¤°à¥à¤µà¤°à¥à¤¸ à¤¸à¥‡ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¤¾ à¤²à¤¾à¤­ à¤‰à¤ à¤¾à¤à¤‚à¥¤

MCP à¤¸à¤°à¥à¤µà¤°à¥à¤¸ à¤Ÿà¥‚à¤²à¥à¤¸ à¤•à¥‹ à¤¨à¤¿à¤®à¥à¤¨à¤¾à¤¨à¥à¤¸à¤¾à¤° `ToolCollection` à¤‘à¤¬à¥à¤œà¥‡à¤•à¥à¤Ÿ à¤®à¥‡à¤‚ à¤²à¥‹à¤¡ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ:

```py
from smolagents import ToolCollection, CodeAgent
from mcp import StdioServerParameters

server_parameters = StdioServerParameters(
    command="uv",
    args=["--quiet", "pubmedmcp@0.1.3"],
    env={"UV_PYTHON": "3.12", **os.environ},
)

with ToolCollection.from_mcp(server_parameters, trust_remote_code=True) as tool_collection:
    agent = CodeAgent(tools=[*tool_collection.tools], add_base_tools=True)
    agent.run("Please find a remedy for hangover.")
```


================================================
FILE: docs/source/ko/_config.py
================================================
# docstyle-ignore
INSTALL_CONTENT = """
# Installation
! pip install smolagents
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/smolagents.git
"""

notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
black_avoid_patterns = {
    "{processor_class}": "FakeProcessorClass",
    "{model_class}": "FakeModelClass",
    "{object_class}": "FakeObjectClass",
}



================================================
FILE: docs/source/ko/_toctree.yml
================================================
[Binary file]


================================================
FILE: docs/source/ko/guided_tour.md
================================================
# ì—ì´ì „íŠ¸ ì•ˆë‚´ì„œ[[agents---guided-tour]]

[[open-in-colab]]

ì´ ì•ˆë‚´ì„œì—ì„œëŠ” ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•, ì‹¤í–‰í•˜ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  ì‚¬ìš© ì‚¬ë¡€ì— ë§žê²Œ ë” ìž˜ ìž‘ë™í•˜ë„ë¡ ë§žì¶¤ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ ìœ í˜• ì„ íƒ: CodeAgent ë˜ëŠ” ToolCallingAgent[[choosing-an-agent-type:-codeagent-or-toolcallingagent]]

`smolagents`ëŠ” [`CodeAgent`]ì™€ [`ToolCallingAgent`] ë‘ ê°€ì§€ ì—ì´ì „íŠ¸ í´ëž˜ìŠ¤ë¥¼ ì œê³µí•˜ëŠ”ë°, ì´ ë‘ í´ëž˜ìŠ¤ëŠ” ê°ê° ì—ì´ì „íŠ¸ê°€ ë„êµ¬ì™€ ìƒí˜¸ìž‘ìš©í•˜ëŠ” ë°©ë²•ì´ ë‹¤ë¦…ë‹ˆë‹¤.
ë‘ ë°©ì‹ì˜ í•µì‹¬ ì°¨ì´ì ì€ 'ì•¡ì…˜ì„ ì§€ì •í•˜ê³  ì‹¤í–‰'í•˜ëŠ” ë°©ì‹ì— ìžˆìŠµë‹ˆë‹¤: ì½”ë“œ ìƒì„± vs êµ¬ì¡°í™”ëœ ë„êµ¬ í˜¸ì¶œ.

- [`CodeAgent`]ëŠ” ë„êµ¬ í˜¸ì¶œì„ Python ì½”ë“œ ìŠ¤ë‹ˆíŽ«ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
  - ì½”ë“œëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ê±°ë‚˜(ìž ìž¬ì ìœ¼ë¡œ ë¶ˆì•ˆì „) ë³´ì•ˆ ìƒŒë“œë°•ìŠ¤ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
  - ë„êµ¬ëŠ” Python í•¨ìˆ˜ë¡œ ë…¸ì¶œë©ë‹ˆë‹¤(ë°”ì¸ë”©ì„ í†µí•´).
  - ë„êµ¬ í˜¸ì¶œ ì˜ˆì‹œ:
    ```py
    result = search_docs("What is the capital of France?")
    print(result)
    ```
  - ìž¥ì :
    - ë†’ì€ í‘œí˜„ë ¥: ë³µìž¡í•œ ë¡œì§ê³¼ ì œì–´ íë¦„ì„ í—ˆìš©í•˜ê³  ë„êµ¬ë¥¼ ê²°í•©í•˜ê³ , ë°˜ë³µí•˜ê³ , ë³€í™˜í•˜ê³ , ì¶”ë¡ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    - ìœ ì—°ì„±: ëª¨ë“  ê°€ëŠ¥í•œ ì•¡ì…˜ì„ ë¯¸ë¦¬ ì •ì˜í•  í•„ìš”ê°€ ì—†ìœ¼ë©°, ë™ì ìœ¼ë¡œ ìƒˆë¡œìš´ ì•¡ì…˜/ë„êµ¬ë¥¼ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    - ì°½ë°œì  ì¶”ë¡ : ë‹¤ë‹¨ê³„ ë¬¸ì œë‚˜ ë™ì  ë¡œì§ì— ì´ìƒì ìž…ë‹ˆë‹¤.
  - ì œí•œì‚¬í•­
    - ì˜¤ë¥˜ ìœ„í—˜: êµ¬ë¬¸ ì˜¤ë¥˜, ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
    - ì˜ˆì¸¡ì„± ë¶€ì¡±: ì˜ˆìƒì¹˜ ëª»í•œ ë˜ëŠ” ì•ˆì „í•˜ì§€ ì•Šì€ ì¶œë ¥ì— ë” ì·¨ì•½í•©ë‹ˆë‹¤.
    - ë³´ì•ˆ ì‹¤í–‰ í™˜ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤.

- [`ToolCallingAgent`]ëŠ” ë„êµ¬ í˜¸ì¶œì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ìž‘ì„±í•©ë‹ˆë‹¤.
  - ì´ëŠ” ë§Žì€ í”„ë ˆìž„ì›Œí¬(OpenAI API)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ì ì¸ í˜•ì‹ìœ¼ë¡œ, ì½”ë“œ ì‹¤í–‰ ì—†ì´ êµ¬ì¡°í™”ëœ ë„êµ¬ ìƒí˜¸ìž‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
  - ë„êµ¬ëŠ” JSON ìŠ¤í‚¤ë§ˆë¡œ ì •ì˜ë©ë‹ˆë‹¤: ì´ë¦„, ì„¤ëª…, ë§¤ê°œë³€ìˆ˜ íƒ€ìž… ë“±.
  - ë„êµ¬ í˜¸ì¶œ ì˜ˆì‹œ:
    ```json
    {
      "tool_call": {
        "name": "search_docs",
        "arguments": {
          "query": "What is the capital of France?"
        }
      }
    }
    ```
  - ìž¥ì :
    - ì•ˆì •ì„±: í™˜ê°ì´ ì ê³ , ì¶œë ¥ì´ êµ¬ì¡°í™”ë˜ê³  ê²€ì¦ë©ë‹ˆë‹¤.
    - ì•ˆì „ì„±: ì¸ìˆ˜ê°€ ì—„ê²©í•˜ê²Œ ê²€ì¦ë˜ê³ , ìž„ì˜ì˜ ì½”ë“œê°€ ì‹¤í–‰ë  ìœ„í—˜ì´ ì—†ìŠµë‹ˆë‹¤.
    - ìƒí˜¸ ìš´ìš©ì„±: ì™¸ë¶€ APIë‚˜ ì„œë¹„ìŠ¤ì— ì‰½ê²Œ ë§¤í•‘ë©ë‹ˆë‹¤.
  - ì œí•œì‚¬í•­:
    - ë‚®ì€ í‘œí˜„ë ¥: ê²°ê³¼ë¥¼ ë™ì ìœ¼ë¡œ ì‰½ê²Œ ê²°í•©í•˜ê±°ë‚˜ ë³€í™˜í•  ìˆ˜ ì—†ê³ , ë³µìž¡í•œ ë¡œì§ì´ë‚˜ ì œì–´ íë¦„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    - ìœ ì—°ì„± ë¶€ì¡±: ëª¨ë“  ê°€ëŠ¥í•œ ì•¡ì…˜ì„ ë¯¸ë¦¬ ì •ì˜í•´ì•¼ í•˜ê³ , ì‚¬ì „ ì •ì˜ëœ ë„êµ¬ë¡œ ì œí•œë©ë‹ˆë‹¤.
    - ì½”ë“œ í•©ì„± ì—†ìŒ: ë„êµ¬ ê¸°ëŠ¥ìœ¼ë¡œ ì œí•œë©ë‹ˆë‹¤.

ì–´ë–¤ ì—ì´ì „íŠ¸ ìœ í˜•ì„ ì‚¬ìš©í• ì§€:
- [`CodeAgent`]ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:
  - ì¶”ë¡ , ì—°ê²° ë˜ëŠ” ë™ì  êµ¬ì„±ì´ í•„ìš”í•œ ê²½ìš°.
  - ë„êµ¬ê°€ ê²°í•©í•  ìˆ˜ ìžˆëŠ” í•¨ìˆ˜ì¸ ê²½ìš°(ì˜ˆ: êµ¬ë¬¸ ë¶„ì„ + ìˆ˜í•™ + ì¿¼ë¦¬).
  - ì—ì´ì „íŠ¸ê°€ ë¬¸ì œ í•´ê²°ìž ë˜ëŠ” í”„ë¡œê·¸ëž˜ë¨¸ì¸ ê²½ìš°.

- [`ToolCallingAgent`]ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:
  - ë‹¨ìˆœí•˜ê³  ë…ë¦½ì ì¸ ë„êµ¬ê°€ ìžˆëŠ” ê²½ìš°(ì˜ˆ: API í˜¸ì¶œ, ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°).
  - ë†’ì€ ì•ˆì •ì„±ê³¼ ëª…í™•í•œ ê²€ì¦ì„ ì›í•˜ëŠ” ê²½ìš°.
  - ì—ì´ì „íŠ¸ê°€ ë””ìŠ¤íŒ¨ì²˜ë‚˜ ì»¨íŠ¸ë¡¤ëŸ¬ ê°™ì€ ì—­í• ì¸ ê²½ìš°.

## CodeAgent[[codeagent]]

[`CodeAgent`]ëŠ” ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ê³  ìž‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Python ì½”ë“œ ìŠ¤ë‹ˆíŽ«ì„ ìƒì„±í•©ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ Python ì½”ë“œ ì‹¤í–‰ì€ ë¡œì»¬ í™˜ê²½ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
ì‚¬ìš©ìžê°€ ì œê³µí•œ ë„êµ¬ë“¤(íŠ¹ížˆ Hugging Face ë„êµ¬ë§Œ ìžˆëŠ” ê²½ìš°)ê³¼ `print`ë‚˜ `math` ëª¨ë“ˆ í•¨ìˆ˜ ê°™ì€ ì‚¬ì „ ì •ì˜ëœ ì•ˆì „í•œ í•¨ìˆ˜ë“¤ë§Œ í˜¸ì¶œí•  ìˆ˜ ìžˆë„ë¡ ì œí•œë˜ì–´ ìžˆì–´ ì•ˆì „í•©ë‹ˆë‹¤.

Python ì¸í„°í”„ë¦¬í„°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì•ˆì „ ëª©ë¡ì— í¬í•¨ëœ ëª¨ë“ˆë§Œ importë¥¼ í—ˆìš©í•˜ë¯€ë¡œ, ëŒ€ë¶€ë¶„ì˜ ëª…ë°±í•œ ë³´ì•ˆ ê³µê²©ì„ ë°©ì§€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
[`CodeAgent`]ë¥¼ ì´ˆê¸°í™”í•  ë•Œ `additional_authorized_imports` ì¸ìˆ˜ì— ë¬¸ìžì—´ ëª©ë¡ìœ¼ë¡œ ìŠ¹ì¸ëœ ëª¨ë“ˆì„ ì „ë‹¬í•˜ì—¬ ì¶”ê°€ importë¥¼ ìŠ¹ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```py
model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

ë˜í•œ ì¶”ê°€ ë³´ì•ˆ ê³„ì¸µìœ¼ë¡œ, import ëª©ë¡ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ìŠ¹ì¸ë˜ì§€ ì•ŠëŠ” í•œ ì„œë¸Œëª¨ë“ˆì— ëŒ€í•œ ì ‘ê·¼ì€ ê¸°ë³¸ì ìœ¼ë¡œ ê¸ˆì§€ë©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, `numpy.random` ì„œë¸Œëª¨ë“ˆì— ì ‘ê·¼í•˜ë ¤ë©´ `additional_authorized_imports` ëª©ë¡ì— `'numpy.random'`ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ëŠ” `numpy`ì™€ `numpy.random` ê°™ì€ ëª¨ë“  ì„œë¸ŒíŒ¨í‚¤ì§€ ë° ìžì²´ ì„œë¸ŒíŒ¨í‚¤ì§€ë¥¼ í—ˆìš©í•˜ëŠ” `numpy.*`ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¹ì¸í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

> [!WARNING]
> LLMì€ ì‹¤í–‰ë  ìž„ì˜ì˜ ì½”ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤: ì•ˆì „í•˜ì§€ ì•Šì€ importëŠ” ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”!

ë¶ˆë²•ì ì¸ ìž‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ê³  ì‹œë„í•˜ëŠ” ì½”ë“œë‚˜ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ì½”ë“œì— ì¼ë°˜ì ì¸ Python ì˜¤ë¥˜ê°€ ìžˆëŠ” ê²½ìš° ì‹¤í–‰ì´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.

ë¡œì»¬ Python ì¸í„°í”„ë¦¬í„° ëŒ€ì‹  [E2B code executor](https://e2b.dev/docs#what-is-e2-b)ë‚˜ Dockerë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. E2Bì˜ ê²½ìš°, ë¨¼ì € [`E2B_API_KEY` í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •](https://e2b.dev/dashboard?tab=keys)í•œ ë‹¤ìŒ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œ `executor_type="e2b"`ë¥¼ ì „ë‹¬í•˜ì„¸ìš”. Dockerì˜ ê²½ìš°, ì´ˆê¸°í™” ì¤‘ì— `executor_type="docker"`ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.

> [!TIP]
> ì½”ë“œ ì‹¤í–‰ì— ëŒ€í•´ ë” ìžì„¸ížˆ ì•Œì•„ë³´ë ¤ë©´ [ì´ íŠœí† ë¦¬ì–¼](tutorials/secure_code_execution)ì„ í™•ì¸í•˜ì„¸ìš”.

### ToolCallingAgent[[toolcallingagent]]

[`ToolCallingAgent`]ëŠ” ë§Žì€ í”„ë ˆìž„ì›Œí¬(OpenAI API)ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ì ì¸ í˜•ì‹ì¸ JSON ë„êµ¬ í˜¸ì¶œì„ ì¶œë ¥í•˜ì—¬, ì½”ë“œ ì‹¤í–‰ ì—†ì´ êµ¬ì¡°í™”ëœ ë„êµ¬ ìƒí˜¸ìž‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ `additional_authorized_imports` ì—†ì´ë„ [`CodeAgent`]ì™€ ê±°ì˜ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìž‘ë™í•©ë‹ˆë‹¤:

```py
from smolagents import ToolCallingAgent, WebSearchTool

agent = ToolCallingAgent(tools=[WebSearchTool()], model=model)
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

## ì—ì´ì „íŠ¸ êµ¬ì¶•[[building-your-agent]]

ìµœì†Œí•œì˜ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ ìµœì†Œí•œ ë‹¤ìŒ ë‘ ì¸ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- `model`, ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ - ì—ì´ì „íŠ¸ëŠ” ë‹¨ìˆœí•œ LLMê³¼ ë‹¤ë¥´ë©°, LLMì„ ì—”ì§„ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì‹œìŠ¤í…œìž…ë‹ˆë‹¤. ë‹¤ìŒ ì˜µì…˜ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
    - [`TransformersModel`]ì€ ì‚¬ì „ ì´ˆê¸°í™”ëœ `transformers` íŒŒì´í”„ë¼ì¸ì„ ê°€ì ¸ì™€ `transformers`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    - [`InferenceClientModel`]ì€ ë‚´ë¶€ì ìœ¼ë¡œ `huggingface_hub.InferenceClient`ë¥¼ í™œìš©í•˜ë©° Hubì˜ ëª¨ë“  ì¶”ë¡  ì œê³µìžë¥¼ ì§€ì›í•©ë‹ˆë‹¤: Cerebras, Cohere, Fal, Fireworks, HF-Inference, Hyperbolic, Nebius, Novita, Replicate, SambaNova, Together ë“±.
    - [`LiteLLMModel`]ì€ ë§ˆì°¬ê°€ì§€ë¡œ [LiteLLM](https://docs.litellm.ai/)ì„ í†µí•´ 100ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì œê³µìžë¥¼ í˜¸ì¶œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!
    - [`AzureOpenAIModel`]ì€ [Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service)ì— ë°°í¬ëœ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤.
    - [`AmazonBedrockModel`]ì€ [AWS](https://aws.amazon.com/bedrock/?nc1=h_ls)ì˜ Amazon Bedrockì„ ì‚¬ìš©í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤.
    - [`MLXModel`]ì€ ë¡œì»¬ ë¨¸ì‹ ì—ì„œ ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ [mlx-lm](https://pypi.org/project/mlx-lm/) íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

- `tools`, ì—ì´ì „íŠ¸ê°€ ìž‘ì—… í•´ê²°ì— ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë„êµ¬ ëª©ë¡ìž…ë‹ˆë‹¤. ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤. add_base_tools=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ê¸°ë³¸ ì œê³µë˜ëŠ” ë„êµ¬ë“¤(ì›¹ ê²€ìƒ‰, ì½”ë“œ ì‹¤í–‰, ìŒì„± ì¸ì‹ ë“±)ì„ `tools` ëª©ë¡ì— ì¶”ê°€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

`tools`ì™€ `model` ë‘ ì¸ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. [ì¶”ë¡  ì œê³µìž](https://huggingface.co/blog/inference-providers), [transformers](https://github.com/huggingface/transformers/), [ollama](https://ollama.com/), [LiteLLM](https://www.litellm.ai/), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Amazon Bedrock](https://aws.amazon.com/bedrock/?nc1=h_ls), ë˜ëŠ” [mlx-lm](https://pypi.org/project/mlx-lm/)ì„ í†µí•´ ì›í•˜ëŠ” LLMì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ëª¨ë“  ëª¨ë¸ í´ëž˜ìŠ¤ëŠ” ì¸ìŠ¤í„´ìŠ¤í™” ì‹œì ì— ì¶”ê°€ í‚¤ì›Œë“œ ì¸ìˆ˜(ì˜ˆ: `temperature`, `max_tokens`, `top_p` ë“±)ë¥¼ ì§ì ‘ ì „ë‹¬í•˜ëŠ” ê²ƒì„ ì§€ì›í•©ë‹ˆë‹¤.
ì´ëŸ¬í•œ ë§¤ê°œë³€ìˆ˜ëŠ” ê¸°ë³¸ ëª¨ë¸ì˜ ì™„ì„± í˜¸ì¶œì— ìžë™ìœ¼ë¡œ ì „ë‹¬ë˜ì–´ ì°½ì˜ì„±, ì‘ë‹µ ê¸¸ì´, ìƒ˜í”Œë§ ì „ëžµ ë“±ì˜ ëª¨ë¸ ë™ìž‘ì„ êµ¬ì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

<hfoptions id="Pick a LLM">
<hfoption id="Inference Providers">

ì¶”ë¡  ì œê³µìžëŠ” ì¸ì¦ì„ ìœ„í•´ `HF_TOKEN`ì´ í•„ìš”í•˜ì§€ë§Œ, ë¬´ë£Œ HF ê³„ì •ì—ëŠ” ì´ë¯¸ í¬í•¨ëœ í¬ë ˆë”§ì´ ì œê³µë©ë‹ˆë‹¤. PROë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì—¬ í¬í•¨ëœ í¬ë ˆë”§ì„ ëŠ˜ë¦¬ì„¸ìš”.

ì œí•œëœ ëª¨ë¸ì— ì ‘ê·¼í•˜ê±°ë‚˜ PRO ê³„ì •ìœ¼ë¡œ ì†ë„ ì œí•œì„ ë†’ì´ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ `HF_TOKEN`ì„ ì„¤ì •í•˜ê±°ë‚˜ `InferenceClientModel` ì´ˆê¸°í™” ì‹œ `token` ë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. [ì„¤ì • íŽ˜ì´ì§€](https://huggingface.co/settings/tokens)ì—ì„œ í† í°ì„ ì–»ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```python
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

model = InferenceClientModel(model_id=model_id, token="<YOUR_HUGGINGFACEHUB_API_TOKEN>") # You can choose to not pass any model_id to InferenceClientModel to use a default model
# you can also specify a particular provider e.g. provider="together" or provider="sambanova"
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Local Transformers Model">

```python
# !pip install smolagents[transformers]
from smolagents import CodeAgent, TransformersModel

model_id = "meta-llama/Llama-3.2-3B-Instruct"

model = TransformersModel(model_id=model_id)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="OpenAI or Anthropic API">

`LiteLLMModel`ì„ ì‚¬ìš©í•˜ë ¤ë©´ í™˜ê²½ ë³€ìˆ˜ `ANTHROPIC_API_KEY` ë˜ëŠ” `OPENAI_API_KEY`ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ì´ˆê¸°í™” ì‹œ `api_key` ë³€ìˆ˜ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# !pip install smolagents[litellm]
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(model_id="anthropic/claude-3-5-sonnet-latest", api_key="YOUR_ANTHROPIC_API_KEY") # Could use 'gpt-4o'
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Ollama">

```python
# !pip install smolagents[litellm]
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(
    model_id="ollama_chat/llama3.2", # This model is a bit weak for agentic behaviours though
    api_base="http://localhost:11434", # replace with 127.0.0.1:11434 or remote open-ai compatible server if necessary
    api_key="YOUR_API_KEY", # replace with API key if necessary
    num_ctx=8192, # ollama default is 2048 which will fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Azure OpenAI">

Azure OpenAIì— ì—°ê²°í•˜ë ¤ë©´ `AzureOpenAIModel`ì„ ì§ì ‘ ì‚¬ìš©í•˜ê±°ë‚˜ `LiteLLMModel`ì„ ì‚¬ìš©í•˜ì—¬ ì ì ˆížˆ êµ¬ì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

`AzureOpenAIModel`ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´ ëª¨ë¸ ë°°í¬ ì´ë¦„ì„ ì „ë‹¬í•œ ë‹¤ìŒ `azure_endpoint`, `api_key`, `api_version` ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `OPENAI_API_VERSION`ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# !pip install smolagents[openai]
from smolagents import CodeAgent, AzureOpenAIModel

model = AzureOpenAIModel(model_id="gpt-4o-mini")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

ë§ˆì°¬ê°€ì§€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ `LiteLLMModel`ì„ êµ¬ì„±í•˜ì—¬ Azure OpenAIì— ì—°ê²°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

- ëª¨ë¸ ë°°í¬ ì´ë¦„ì„ `model_id`ë¡œ ì „ë‹¬í•˜ê³ , ì•žì— `azure/`ë¥¼ ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤.
- í™˜ê²½ ë³€ìˆ˜ `AZURE_API_VERSION`ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
- `api_base`ì™€ `api_key` ì¸ìˆ˜ë¥¼ ì „ë‹¬í•˜ê±°ë‚˜ í™˜ê²½ ë³€ìˆ˜ `AZURE_API_KEY`, `AZURE_API_BASE`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```python
import os
from smolagents import CodeAgent, LiteLLMModel

AZURE_OPENAI_CHAT_DEPLOYMENT_NAME="gpt-35-turbo-16k-deployment" # example of deployment name

os.environ["AZURE_API_KEY"] = "" # api_key
os.environ["AZURE_API_BASE"] = "" # "https://example-endpoint.openai.azure.com"
os.environ["AZURE_API_VERSION"] = "" # "2024-10-01-preview"

model = LiteLLMModel(model_id="azure/" + AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
   "Could you give me the 118th number in the Fibonacci sequence?",
)
```

</hfoption>
<hfoption id="Amazon Bedrock">

`AmazonBedrockModel` í´ëž˜ìŠ¤ëŠ” Amazon Bedrockê³¼ ì§ì ‘ ì—°ë™ë˜ì–´ API í˜¸ì¶œê³¼ ì„¸ë¶€ êµ¬ì„±ì„ ì§€ì›í•©ë‹ˆë‹¤.

ê¸°ë³¸ ì‚¬ìš©ë²•:

```python
# !pip install smolagents[aws_sdk]
from smolagents import CodeAgent, AmazonBedrockModel

model = AmazonBedrockModel(model_id="anthropic.claude-3-sonnet-20240229-v1:0")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

ê³ ê¸‰ êµ¬ì„±:

```python
import boto3
from smolagents import AmazonBedrockModel

# Create a custom Bedrock client
bedrock_client = boto3.client(
    'bedrock-runtime',
    region_name='us-east-1',
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY'
)

additional_api_config = {
    "inferenceConfig": {
        "maxTokens": 3000
    },
    "guardrailConfig": {
        "guardrailIdentifier": "identify1",
        "guardrailVersion": 'v1'
    },
}

# Initialize with comprehensive configuration
model = AmazonBedrockModel(
    model_id="us.amazon.nova-pro-v1:0",
    client=bedrock_client,  # Use custom client
    **additional_api_config
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

LiteLLMModel ì‚¬ìš©:

ë˜ëŠ” Bedrock ëª¨ë¸ê³¼ í•¨ê»˜ `LiteLLMModel`ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```python
from smolagents import LiteLLMModel, CodeAgent

model = LiteLLMModel(model_name="bedrock/anthropic.claude-3-sonnet-20240229-v1:0")
agent = CodeAgent(tools=[], model=model)

agent.run("Explain the concept of quantum computing")
```

</hfoption>
<hfoption id="mlx-lm">

```python
# !pip install smolagents[mlx-lm]
from smolagents import CodeAgent, MLXModel

mlx_model = MLXModel("mlx-community/Qwen2.5-Coder-32B-Instruct-4bit")
agent = CodeAgent(model=mlx_model, tools=[], add_base_tools=True)

agent.run("Could you give me the 118th number in the Fibonacci sequence?")
```

</hfoption>
</hfoptions>

## ê³ ê¸‰ ì—ì´ì „íŠ¸ êµ¬ì„±[[advanced-agent-configuration]]

### ì—ì´ì „íŠ¸ ì¢…ë£Œ ì¡°ê±´ ë§žì¶¤ ì„¤ì •[[customizing-agent-termination-conditions]]

ê¸°ë³¸ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ëŠ” `final_answer` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜ ìµœëŒ€ ë‹¨ê³„ ìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ê³„ì† ì‹¤í–‰ë©ë‹ˆë‹¤.
`final_answer_checks` ë§¤ê°œë³€ìˆ˜ëŠ” ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰ì„ ì¢…ë£Œí•˜ëŠ” ì‹œì ê³¼ ë°©ë²•ì„ ë” ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤:

```python
from smolagents import CodeAgent, InferenceClientModel

# Define a custom final answer check function
def is_integer(final_answer: str, agent_memory=None) -> bool:
    """Return True if final_answer is an integer."""
    try:
        int(final_answer)
        return True
    except ValueError:
        return False

# Initialize agent with custom final answer check
agent = CodeAgent(
    tools=[],
    model=InferenceClientModel(),
    final_answer_checks=[is_integer]
)

agent.run("Calculate the least common multiple of 3 and 7")
```

`final_answer_checks` ë§¤ê°œë³€ìˆ˜ëŠ” ê°ê° ë‹¤ìŒê³¼ ê°™ì€ í•¨ìˆ˜ë“¤ì˜ ëª©ë¡ì„ ë°›ìŠµë‹ˆë‹¤:
- ì—ì´ì „íŠ¸ì˜ final_answer ë¬¸ìžì—´ê³¼ ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤
- final_answerê°€ ìœ íš¨í•œì§€(True) ì•„ë‹Œì§€(False)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶ˆë¦¬ì–¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤

í•¨ìˆ˜ ì¤‘ í•˜ë‚˜ë¼ë„ `False`ë¥¼ ë°˜í™˜í•˜ë©´ ì—ì´ì „íŠ¸ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ë¡œê·¸ì— ê¸°ë¡í•˜ê³  ì‹¤í–‰ì„ ê³„ì†í•©ë‹ˆë‹¤.
ì´ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜ì€ ë‹¤ìŒì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤:
- ì¶œë ¥ í˜•ì‹ ìš”êµ¬ì‚¬í•­ ê°•ì œ(ì˜ˆ: ìˆ˜í•™ ë¬¸ì œì— ëŒ€í•œ ìˆ«ìž ë‹µë³€ ë³´ìž¥)
- ë„ë©”ì¸ë³„ ê²€ì¦ ê·œì¹™ êµ¬í˜„
- ìžì²´ ì¶œë ¥ì„ ê²€ì¦í•˜ëŠ” ë” ê²¬ê³ í•œ ì—ì´ì „íŠ¸ ìƒì„±

## ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²€ì‚¬[[inspecting-an-agent-run]]

ì‹¤í–‰ í›„ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìœ ìš©í•œ ëª‡ ê°€ì§€ ì†ì„±ì´ ìžˆìŠµë‹ˆë‹¤:

- `agent.logs`ëŠ” ì—ì´ì „íŠ¸ì˜ ìƒì„¸í•œ ì‹¤í–‰ ë¡œê·¸ë¥¼ ì €ìž¥í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ ì‹¤í–‰ì˜ ê° ë‹¨ê³„ë§ˆë‹¤ ëª¨ë“  ì •ë³´ê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ìž¥ë˜ì–´ `agent.logs`ì— ì¶”ê°€ë©ë‹ˆë‹¤.
- `agent.write_memory_to_messages()`ëŠ” ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ëª¨ë¸ì´ ë³¼ ìˆ˜ ìžˆëŠ” ì±„íŒ… ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ ë©”ì†Œë“œëŠ” ë¡œê·¸ì˜ ê° ë‹¨ê³„ë¥¼ ì‚´íŽ´ë³´ê³  ì¤‘ìš”í•œ ë‚´ìš©ë§Œ ë©”ì‹œì§€ë¡œ ì €ìž¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìž‘ì—…ì„ ê°ê° ë³„ë„ ë©”ì‹œì§€ë¡œ ì €ìž¥í•˜ê³ , ê° ë‹¨ê³„ì˜ LLM ì¶œë ¥ê³¼ ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ë¥¼ ê°œë³„ ë©”ì‹œì§€ë¡œ ì €ìž¥í•©ë‹ˆë‹¤. ì „ì²´ì ì¸ íë¦„ íŒŒì•…ì´ í•„ìš”í•  ë•Œ ê¶Œìž¥ë“œë¦½ë‹ˆë‹¤. ë‹¨, ëª¨ë“  ë¡œê·¸ê°€ ì´ ë©”ì†Œë“œë¥¼ í†µí•´ ê¸°ë¡ë˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.

## ë„êµ¬[[tools]]

ë„êµ¬ëŠ” ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë…ë¦½ì ì¸ í•¨ìˆ˜ìž…ë‹ˆë‹¤. LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € APIë¥¼ êµ¬ì„±í•´ì•¼í•˜ë©°, ë˜í•œ LLMì—ê²Œ í•´ë‹¹ ë„êµ¬ í˜¸ì¶œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì–´ì•¼í•©ë‹ˆë‹¤ :
- ì´ë¦„
- ì„¤ëª…
- ìž…ë ¥ íƒ€ìž…ê³¼ ì„¤ëª…
- ì¶œë ¥ íƒ€ìž…

ì˜ˆë¥¼ ë“¤ì–´ [`PythonInterpreterTool`]ì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤: ì´ë¦„, ì„¤ëª…, ìž…ë ¥ ì„¤ëª…, ì¶œë ¥ íƒ€ìž…, ê·¸ë¦¬ê³  ì•¡ì…˜ì„ ìˆ˜í–‰í•˜ëŠ” `forward` ë©”ì†Œë“œê°€ ìžˆìŠµë‹ˆë‹¤.

ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë  ë•Œ ë„êµ¬ ì†ì„±ì´ ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ëŠ” ë„êµ¬ ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ëŠ” ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ë„êµ¬ì™€ ê·¸ ì´ìœ ë¥¼ ì•Œ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

**ìŠ¤í‚¤ë§ˆ ì •ë³´**: `output_schema`ê°€ ì •ì˜ëœ ë„êµ¬(êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ê°€ì§„ MCP ë„êµ¬ ë“±)ì˜ ê²½ìš°, `CodeAgent` ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ìžë™ìœ¼ë¡œ JSON ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ëŠ” ì—ì´ì „íŠ¸ê°€ ë„êµ¬ ì¶œë ¥ì˜ ì˜ˆìƒ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ë°ì´í„°ì— ì ì ˆížˆ ì ‘ê·¼í•  ìˆ˜ ìžˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.

### ê¸°ë³¸ íˆ´ë°•ìŠ¤[[default-toolbox]]

"toolkit" extraì™€ í•¨ê»˜ `smolagents`ë¥¼ ì„¤ì¹˜í•˜ë©´ ì—ì´ì „íŠ¸ë¥¼ ê°•í™”í•˜ëŠ” ê¸°ë³¸ íˆ´ë°•ìŠ¤ê°€ í•¨ê»˜ ì œê³µë˜ë©°, `add_base_tools=True` ì¸ìˆ˜ë¡œ ì´ˆê¸°í™” ì‹œ ì—ì´ì „íŠ¸ì— ì¶”ê°€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

- **DuckDuckGo ì›¹ ê²€ìƒ‰***: DuckDuckGo ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- **Python ì½”ë“œ ì¸í„°í”„ë¦¬í„°**: ë³´ì•ˆ í™˜ê²½ì—ì„œ LLMì´ ìƒì„±í•œ Python ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ì½”ë“œ ê¸°ë°˜ ì—ì´ì „íŠ¸ê°€ ì´ë¯¸ ê¸°ë³¸ì ìœ¼ë¡œ Python ì½”ë“œë¥¼ ì‹¤í–‰í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ `add_base_tools=True`ë¡œ ì´ˆê¸°í™”í•  ë•Œë§Œ [`ToolCallingAgent`]ì— ì¶”ê°€ë©ë‹ˆë‹¤.
- **Transcriber**: ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” Whisper-Turbo ê¸°ë°˜ì˜ ìŒì„±-í…ìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ìž…ë‹ˆë‹¤.

ì¸ìˆ˜ì™€ í•¨ê»˜ í˜¸ì¶œí•˜ì—¬ ë„êµ¬ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```python
# !pip install smolagents[toolkit]
from smolagents import WebSearchTool

search_tool = WebSearchTool()
print(search_tool("Who's the current president of Russia?"))
```

### ìƒˆë¡œìš´ ë„êµ¬ ìƒì„±[[create-a-new-tool]]

Hugging Faceì˜ ê¸°ë³¸ ë„êµ¬ê°€ ë‹¤ë£¨ì§€ ì•ŠëŠ” ì‚¬ìš© ì‚¬ë¡€ë¥¼ ìœ„í•´ ìžì‹ ë§Œì˜ ë„êµ¬ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, Hubì—ì„œ ì£¼ì–´ì§„ ìž‘ì—…ì— ëŒ€í•´ ê°€ìž¥ ë§Žì´ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ ë°˜í™˜í•˜ëŠ” ë„êµ¬ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

ì•„ëž˜ ì½”ë“œë¶€í„° ì‹œìž‘í•˜ê² ìŠµë‹ˆë‹¤.

```python
from huggingface_hub import list_models

task = "text-classification"

most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
print(most_downloaded_model.id)
```

ì´ ì½”ë“œëŠ” í•¨ìˆ˜ë¡œ ë§Œë“¤ê³  `tool` ë°ì½”ë ˆì´í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ê°„ë‹¨ížˆ ë„êµ¬ë¡œ ë³€í™˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì´ê²ƒì´ ë„êµ¬ë¥¼ ë§Œë“œëŠ” ìœ ì¼í•œ ë°©ë²•ì€ ì•„ë‹™ë‹ˆë‹¤. [Tool]ì˜ í•˜ìœ„ í´ëž˜ìŠ¤ë¡œ ì§ì ‘ ì •ì˜í•˜ëŠ” ë°©ë²•ë„ ìžˆìœ¼ë©°, ì´ ë°©ì‹ì€ ë” ë§Žì€ ìœ ì—°ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì ì¸ í´ëž˜ìŠ¤ ì†ì„±ì„ ì´ˆê¸°í™”í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

ë‘ ì˜µì…˜ ëª¨ë‘ì—ì„œ ì–´ë–»ê²Œ ìž‘ë™í•˜ëŠ”ì§€ ì‚´íŽ´ë³´ê² ìŠµë‹ˆë‹¤:

<hfoptions id="build-a-tool">
<hfoption id="Decorate a function with @tool">

```py
from smolagents import tool

@tool
def model_download_tool(task: str) -> str:
    """
    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
    It returns the name of the checkpoint.

    Args:
        task: The task for which to get the download count.
    """
    most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
    return most_downloaded_model.id
```

í•¨ìˆ˜ì—ëŠ” ë‹¤ìŒì´ í•„ìš”í•©ë‹ˆë‹¤:
- ëª…í™•í•œ ì´ë¦„. ì´ë¦„ì€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” LLMì´ ì´í•´í•  ìˆ˜ ìžˆë„ë¡ ì´ ë„êµ¬ê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ ì¶©ë¶„ížˆ ì„¤ëª…ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ìž‘ì—…ì— ëŒ€í•´ ê°€ìž¥ ë§Žì´ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ ë°˜í™˜í•˜ë¯€ë¡œ `model_download_tool`ì´ë¼ê³  ëª…ëª…í•˜ê² ìŠµë‹ˆë‹¤.
- ìž…ë ¥ê³¼ ì¶œë ¥ ëª¨ë‘ì— ëŒ€í•œ íƒ€ìž… ížŒíŠ¸
- ê° ì¸ìˆ˜ê°€ ì„¤ëª…ë˜ëŠ” 'Args:' ë¶€ë¶„ì„ í¬í•¨í•˜ëŠ” ì„¤ëª…(ì´ë²ˆì—ëŠ” íƒ€ìž… í‘œì‹œ ì—†ì´, íƒ€ìž… ížŒíŠ¸ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤). ë„êµ¬ ì´ë¦„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ ì„¤ëª…ì€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” LLMì„ ìœ„í•œ ì„¤ëª…ì„œì´ë¯€ë¡œ ì†Œí™€ížˆ í•˜ì§€ ë§ˆì„¸ìš”.

ì´ ëª¨ë“  ìš”ì†ŒëŠ” ì´ˆê¸°í™” ì‹œ ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ìžë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤: ë”°ë¼ì„œ ìµœëŒ€í•œ ëª…í™•í•˜ê²Œ ë§Œë“¤ë„ë¡ ë…¸ë ¥í•˜ì„¸ìš”!

> [!TIP]
> ì´ ì •ì˜ í˜•ì‹ì€ `apply_chat_template`ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë„êµ¬ ìŠ¤í‚¤ë§ˆì™€ ë™ì¼í•˜ë©°, ìœ ì¼í•œ ì°¨ì´ì ì€ ì¶”ê°€ëœ `tool` ë°ì½”ë ˆì´í„°ìž…ë‹ˆë‹¤: ë„êµ¬ ì‚¬ìš© APIì— ëŒ€í•´ ë” ìžì„¸ížˆ ì•Œì•„ë³´ë ¤ë©´ [ì—¬ê¸°](https://huggingface.co/blog/unified-tool-use#passing-tools-to-a-chat-template)ë¥¼ ì½ì–´ë³´ì„¸ìš”.


ê·¸ëŸ° ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
</hfoption>
<hfoption id="Subclass Tool">

```py
from smolagents import Tool

class ModelDownloadTool(Tool):
    name = "model_download_tool"
    description = "This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It returns the name of the checkpoint."
    inputs = {"task": {"type": "string", "description": "The task for which to get the download count."}}
    output_type = "string"

    def forward(self, task: str) -> str:
        most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
        return most_downloaded_model.id
```

í•˜ìœ„ í´ëž˜ìŠ¤ì—ëŠ” ë‹¤ìŒ ì†ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤:
- ëª…í™•í•œ `name` (ì´ë¦„). ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” LLMì´ ë„êµ¬ì˜ ê¸°ëŠ¥ì„ ì´í•´í•  ìˆ˜ ìžˆë„ë¡ ì´ë¦„ì— ëŒ€í•´ ì¶©ë¶„ížˆ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ë„êµ¬ëŠ” ìž‘ì—…ì— ëŒ€í•´ ê°€ìž¥ ë§Žì´ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì„ ë°˜í™˜í•˜ë¯€ë¡œ `model_download_tool`ì´ë¼ê³  ëª…ëª…í•˜ê² ìŠµë‹ˆë‹¤.
- `description`. `name`ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ì´ ì„¤ëª…ì€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” LLMì„ ìœ„í•œ ì„¤ëª…ì„œì´ë¯€ë¡œ ì†Œí™€ížˆ í•˜ì§€ ë§ˆì„¸ìš”.
- ìž…ë ¥ íƒ€ìž…ê³¼ ì„¤ëª…
- ì¶œë ¥ íƒ€ìž…
ì´ ëª¨ë“  ì†ì„±ì€ ì´ˆê¸°í™” ì‹œ ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ìžë™ìœ¼ë¡œ í¬í•¨ë©ë‹ˆë‹¤: ë”°ë¼ì„œ ìµœëŒ€í•œ ëª…í™•í•˜ê²Œ ë§Œë“¤ë„ë¡ ë…¸ë ¥í•˜ì„¸ìš”!


ê·¸ëŸ° ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[ModelDownloadTool()], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
</hfoption>
</hfoptions>

ë‹¤ìŒ ë¡œê·¸ë¥¼ ì–»ìŠµë‹ˆë‹¤:
```text
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                          â”‚
â”‚ Can you give me the name of the model that has the most downloads in the 'text-to-video' â”‚
â”‚ task on the Hugging Face Hub?                                                            â”‚
â”‚                                                                                          â”‚
â•°â”€ InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 model_name = model_download_tool(task="text-to-video")                               â”‚
â”‚   2 print(model_name)                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Execution logs:
ByteDance/AnimateDiff-Lightning

Out: None
[Step 0: Duration 0.27 seconds| Input tokens: 2,069 | Output tokens: 60]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 final_answer("ByteDance/AnimateDiff-Lightning")                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Out - Final answer: ByteDance/AnimateDiff-Lightning
[Step 1: Duration 0.10 seconds| Input tokens: 4,288 | Output tokens: 148]
Out[20]: 'ByteDance/AnimateDiff-Lightning'
```

> [!TIP]
> ë„êµ¬ì— ëŒ€í•´ ë” ìžì„¸ížˆ ì•Œì•„ë³´ë ¤ë©´ [ì „ìš© íŠœí† ë¦¬ì–¼](./tutorials/tools#what-is-a-tool-and-how-to-build-one)ì„ ì½ì–´ë³´ì„¸ìš”.

## ë©€í‹° ì—ì´ì „íŠ¸[[multi-agents]]

ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ Microsoftì˜ í”„ë ˆìž„ì›Œí¬ [Autogen](https://huggingface.co/papers/2308.08155)ê³¼ í•¨ê»˜ ë„ìž…ë˜ì—ˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ í”„ë ˆìž„ì›Œí¬ì—ì„œëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ ëŒ€ì‹  ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ì—¬ ìž‘ì—…ì„ í•´ê²°í•©ë‹ˆë‹¤.
ì‹¤ì œë¡œ ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„±ëŠ¥ì´ í–¥ìƒë˜ëŠ” ì´ìœ ëŠ” ê°œë…ì ìœ¼ë¡œ ë‹¨ìˆœí•©ë‹ˆë‹¤. ë§Žì€ ìž‘ì—…ì—ì„œ ëª¨ë“  ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” ë²”ìš© ì‹œìŠ¤í…œë³´ë‹¤ëŠ” íŠ¹ì • í•˜ìœ„ ìž‘ì—…ì— íŠ¹í™”ëœ ì „ë¬¸ ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì´ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ ë„êµ¬ì™€ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ì—ì´ì „íŠ¸ë“¤ì„ í™œìš©í•˜ë©´ íš¨ìœ¨ì ì¸ ì—­í•  ë¶„ë‹´ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ê°€ ìˆ˜ì§‘í•œ ëª¨ë“  ì›¹íŽ˜ì´ì§€ ë‚´ìš©ì„ ì½”ë“œ ìƒì„± ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ì—ê¹Œì§€ ì €ìž¥í•  í•„ìš”ê°€ ìžˆì„ê¹Œìš”? ê°ìžì˜ ì—­í• ì— ë§žê²Œ ë¶„ë¦¬í•´ì„œ ìš´ì˜í•˜ëŠ” ê²ƒì´ í›¨ì”¬ íš¨ìœ¨ì ìž…ë‹ˆë‹¤.

`smolagents`ë¡œ ê³„ì¸µì  ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ìœ„í•´ì„œëŠ” ì—ì´ì „íŠ¸ì— `name`ê³¼ `description` ì†ì„±ë§Œ ìžˆìœ¼ë©´ ë˜ë©°, ì´ëŠ” ë„êµ¬ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê´€ë¦¬ìž ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë˜ì–´ ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
ê·¸ëŸ° ë‹¤ìŒ ê´€ë¦¬ìž ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ë•Œ `managed_agents` ë§¤ê°œë³€ìˆ˜ì— ì´ ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì€ ë„¤ì´í‹°ë¸Œ [`WebSearchTool`]ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì›¹ ê²€ìƒ‰ ì—ì´ì „íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“œëŠ” ì˜ˆì‹œìž…ë‹ˆë‹¤:

```py
from smolagents import CodeAgent, InferenceClientModel, WebSearchTool

model = InferenceClientModel()

web_agent = CodeAgent(
    tools=[WebSearchTool()],
    model=model,
    name="web_search_agent",
    description="Runs web searches for you. Give it your query as an argument."
)

manager_agent = CodeAgent(
    tools=[], model=model, managed_agents=[web_agent]
)

manager_agent.run("Who is the CEO of Hugging Face?")
```

> [!TIP]
> íš¨ìœ¨ì ì¸ ë©€í‹° ì—ì´ì „íŠ¸ êµ¬í˜„ì˜ ì‹¬í™” ì˜ˆì œë¥¼ ë³´ë ¤ë©´ [ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì„ GAIA ë¦¬ë”ë³´ë“œ ìƒìœ„ê¶Œìœ¼ë¡œ ëŒì–´ì˜¬ë¦° ë°©ë²•](https://huggingface.co/blog/beating-gaia)ì„ í™•ì¸í•˜ì„¸ìš”.

## ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ê³  ë©‹ì§„ Gradio ì¸í„°íŽ˜ì´ìŠ¤ì—ì„œ ê·¸ ì‚¬ê³  ê³¼ì •ì„ ì‹œê°í™”í•˜ê¸°[[talk-with-your-agent-and-visualize-its-thoughts-in-a-cool-gradio-interface]]

`GradioUI`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì— ëŒ€í™”í˜•ìœ¼ë¡œ ìž‘ì—…ì„ ì œì¶œí•˜ê³  ê·¸ ì‚¬ê³ ì™€ ì‹¤í–‰ ê³¼ì •ì„ ê´€ì°°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ ì˜ˆì‹œìž…ë‹ˆë‹¤:

```py
from smolagents import (
    load_tool,
    CodeAgent,
    InferenceClientModel,
    GradioUI
)

# Import tool from Hub
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

model = InferenceClientModel(model_id=model_id)

# Initialize the agent with the image generation tool
agent = CodeAgent(tools=[image_generation_tool], model=model)

GradioUI(agent).launch()
```

ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©ìžê°€ ìƒˆë¡œìš´ ìš”ì²­ì„ ìž…ë ¥í•˜ë©´ ì—ì´ì „íŠ¸ëŠ” `agent.run(user_request, reset=False)`ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
`reset=False` í”Œëž˜ê·¸ëŠ” ì´ ìƒˆë¡œìš´ ìž‘ì—…ì„ ì‹¤í–‰í•˜ê¸° ì „ì— ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ê°€ í”ŒëŸ¬ì‹œë˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•˜ë©°, ì´ë¥¼ í†µí•´ ëŒ€í™”ê°€ ê³„ì†ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œë„ ì´ `reset=False` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ê³„ì†í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

Gradio UIì—ì„œ ì‚¬ìš©ìžê°€ ì‹¤í–‰ ì¤‘ì¸ ì—ì´ì „íŠ¸ë¥¼ ì¤‘ë‹¨í•  ìˆ˜ ìžˆë„ë¡ í•˜ë ¤ë©´ `agent.interrupt()` ë©”ì†Œë“œë¥¼ íŠ¸ë¦¬ê±°í•˜ëŠ” ë²„íŠ¼ìœ¼ë¡œ ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
ì´ë ‡ê²Œ í•˜ë©´ í˜„ìž¬ ë‹¨ê³„ê°€ ëë‚  ë•Œ ì—ì´ì „íŠ¸ê°€ ì¤‘ì§€ë˜ê³  ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.

## ë‹¤ìŒ ë‹¨ê³„[[next-steps]]

ë§ˆì§€ë§‰ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ í•„ìš”ì— ë§žê²Œ êµ¬ì„±í–ˆë‹¤ë©´ Hubì— ê³µìœ í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!

```py
agent.push_to_hub("m-ric/my_agent")
```

ë§ˆì°¬ê°€ì§€ë¡œ, ë„êµ¬ì˜ ì½”ë“œë¥¼ ì‹ ë¢°í•œë‹¤ë©´ Hubì— ì—…ë¡œë“œëœ ì—ì´ì „íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ë ¤ë©´ ë‹¤ìŒì„ ì‚¬ìš©í•˜ì„¸ìš”:
```py
agent.from_hub("m-ric/my_agent", trust_remote_code=True)
```

ë” ìžì„¸í•œ í™œìš©ë²•ì„ ì›í•œë‹¤ë©´ ë‹¤ìŒ íŠœí† ë¦¬ì–¼ë“¤ì„ ì°¸ê³ í•˜ì„¸ìš”:
- [ì½”ë“œ ì—ì´ì „íŠ¸ê°€ ìž‘ë™í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì„¤ëª…](./tutorials/secure_code_execution)
- [ì¢‹ì€ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê°€ì´ë“œ](./tutorials/building_good_agents).
- [ë„êµ¬ ì‚¬ìš©ì— ëŒ€í•œ ìƒì„¸ ê°€ì´ë“œ](./tutorials/building_good_agents).



================================================
FILE: docs/source/ko/index.md
================================================
# `smolagents`[[smolagents]]

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/license_to_call.png" style="max-width:700px"/>
</div>

## smolagentsëž€ ë¬´ì—‡ì¸ê°€ìš”?[[what-is-smolagents]]

`smolagents`ëŠ” ë‹¨ ëª‡ ì¤„ì˜ ì½”ë“œë§Œìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìžˆë„ë¡ ì„¤ê³„ëœ ì˜¤í”ˆì†ŒìŠ¤ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ìž…ë‹ˆë‹¤.

`smolagents`ì˜ ì£¼ìš” íŠ¹ì§•:

âœ¨ **ë‹¨ìˆœí•¨**: ì—ì´ì „íŠ¸ ë¡œì§ì´ ì•½ ì²œ ì¤„ì˜ ì½”ë“œë¡œ êµ¬í˜„ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ì½”ë“œ ìœ„ì— ë¶ˆí•„ìš”í•œ ë³µìž¡í•œ êµ¬ì¡°ë¥¼ ì¶”ê°€í•˜ì§€ ì•Šê³  ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤!

ðŸ§‘â€ðŸ’» **ì½”ë“œ ì—ì´ì „íŠ¸ì˜ ì™„ì „í•œ ì§€ì›**: [`CodeAgent`](reference/agents#smolagents.CodeAgent)ëŠ” ë„êµ¬ í˜¸ì¶œì´ë‚˜ ê³„ì‚° ìˆ˜í–‰ì„ ìœ„í•´ ì§ì ‘ ì½”ë“œë¥¼ ìž‘ì„±í•©ë‹ˆë‹¤ ("ì½”ë“œ ìž‘ì„±ìš© ì—ì´ì „íŠ¸"ì™€ëŠ” ë°˜ëŒ€ ê°œë…). ì´ë¥¼ í†µí•´ í•¨ìˆ˜ ì¤‘ì²©, ë£¨í”„, ì¡°ê±´ë¬¸ ë“±ì„ ìžì—°ìŠ¤ëŸ½ê²Œ ì¡°í•©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë³´ì•ˆì„ ìœ„í•´ [E2B](https://e2b.dev/)ë‚˜ Dockerë¥¼ í†µí•œ [ìƒŒë“œë°•ìŠ¤ í™˜ê²½ ì‹¤í–‰](tutorials/secure_code_execution)ì„ ì§€ì›í•©ë‹ˆë‹¤.

ðŸ“¡ **ê¸°ë³¸ ë„êµ¬ í˜¸ì¶œ ì—ì´ì „íŠ¸ ì§€ì›**: CodeAgent ì™¸ì—ë„ [`ToolCallingAgent`](reference/agents#smolagents.ToolCallingAgent)ëŠ” ì¼ë°˜ì ì¸ JSON/í…ìŠ¤íŠ¸ ê¸°ë°˜ ë„êµ¬ í˜¸ì¶œ ë°©ì‹ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ìœ„í•´ ì§€ì›ë©ë‹ˆë‹¤.

ðŸ¤— **Hub í†µí•©**: Gradio Spacesë¡œ ì—ì´ì „íŠ¸ì™€ ë„êµ¬ë¥¼ Hubì—ì„œ ì›í™œí•˜ê²Œ ê³µìœ í•˜ê³  ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ðŸŒ **ëª¨ë¸ ë…ë¦½ì **: Hubì˜ [Inference providers](https://huggingface.co/docs/inference-providers/index)ë‚˜ OpenAI, Anthropic ë“±ì˜ APIë¥¼ í†µí•´ ì ‘ê·¼í•˜ê±°ë‚˜, LiteLLM í†µí•©ìœ¼ë¡œ ë‹¤ì–‘í•œ LLMì„ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. Transformersë‚˜ Ollamaë¥¼ ì‚¬ìš©í•œ ë¡œì»¬ ì‹¤í–‰ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì›í•˜ëŠ” LLMìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ êµ¬ë™í•˜ëŠ” ê²ƒì´ ê°„ë‹¨í•˜ê³  ìœ ì—°í•©ë‹ˆë‹¤.

ðŸ‘ï¸ **ëª¨ë‹¬ë¦¬í‹° ë…ë¦½ì **: í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ë¹„ì „, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ìž…ë ¥ë„ ì²˜ë¦¬í•  ìˆ˜ ìžˆì–´ í™œìš© ê°€ëŠ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë²”ìœ„ê°€ í™•ìž¥ë©ë‹ˆë‹¤. ë¹„ì „ ê´€ë ¨ [íŠœí† ë¦¬ì–¼](examples/web_browser)ì„ í™•ì¸í•´ë³´ì„¸ìš”.

ðŸ› ï¸ **ë„êµ¬ ë…ë¦½ì **: [MCP ì„œë²„](reference/tools#smolagents.ToolCollection.from_mcp)ì˜ ë„êµ¬ë‚˜ [LangChain](reference/tools#smolagents.Tool.from_langchain)ì˜ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆê³ , [Hub Space](reference/tools#smolagents.Tool.from_space)ë„ ë„êµ¬ë¡œ í™œìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ðŸ’» **CLI ë„êµ¬**: ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì½”ë“œ ìž‘ì„± ì—†ì´ ì—ì´ì „íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ì‹¤í–‰í•  ìˆ˜ ìžˆëŠ” ëª…ë ¹ì¤„ ìœ í‹¸ë¦¬í‹°(smolagent, webagent)ê°€ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.

## ë¹ ë¥¸ ì‹œìž‘[[quickstart]]

[[open-in-colab]]

smolagentsë¥¼ ë‹¨ ëª‡ ë¶„ ë§Œì— ì‹œìž‘í•´ë³´ì„¸ìš”! ì´ ê°€ì´ë“œëŠ” ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ì„¤ì¹˜[[installation]]

pipìœ¼ë¡œ smolagentsë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install 'smolagents[toolkit]'  # ì›¹ ê²€ìƒ‰ê³¼ ê°™ì€ ê¸°ë³¸ ë„êµ¬ í¬í•¨
```

### ì²« ì—ì´ì „íŠ¸ ë§Œë“¤ê¸°[[create-your-first-agent]]

ë‹¤ìŒì€ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ìµœì†Œí•œì˜ ì˜ˆì œìž…ë‹ˆë‹¤:

```python
from smolagents import CodeAgent, InferenceClientModel

# ëª¨ë¸ ì´ˆê¸°í™” (Hugging Face Inference API ì‚¬ìš©)
model = InferenceClientModel()  # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©

# ë„êµ¬ ì—†ì´ ì—ì´ì „íŠ¸ ìƒì„±
agent = CodeAgent(tools=[], model=model)

# ìž‘ì—…ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰
result = agent.run("Calculate the sum of numbers from 1 to 10")
print(result)
```

ëìž…ë‹ˆë‹¤! ì—ì´ì „íŠ¸ê°€ Python ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìž‘ì—…ì„ í•´ê²°í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

### ë„êµ¬ ì¶”ê°€[[adding-tools]]

ëª‡ ê°€ì§€ ë„êµ¬ë¥¼ ì¶”ê°€í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ë” ê°•ë ¥í•˜ê²Œ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤:

```python
from smolagents import CodeAgent, InferenceClientModel, DuckDuckGoSearchTool

model = InferenceClientModel()
agent = CodeAgent(
    tools=[DuckDuckGoSearchTool()],
    model=model,
)

# ì´ì œ ì—ì´ì „íŠ¸ê°€ ì›¹ì„ ê²€ìƒ‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!
result = agent.run("What is the current weather in Paris?")
print(result)
```

### ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°[[using-different-models]]

ì—ì´ì „íŠ¸ì™€ í•¨ê»˜ ë‹¤ì–‘í•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```python
# Hugging Faceì˜ íŠ¹ì • ëª¨ë¸ ì‚¬ìš©
model = InferenceClientModel(model_id="meta-llama/Llama-2-70b-chat-hf")

# OpenAI/Anthropic ì‚¬ìš© ('smolagents[litellm]' í•„ìš”)
from smolagents import LiteLLMModel
model = LiteLLMModel(model_id="gpt-4")

# ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ('smolagents[transformers]' í•„ìš”)
from smolagents import TransformersModel
model = TransformersModel(model_id="meta-llama/Llama-2-7b-chat-hf")
```

## ë‹¤ìŒ ë‹¨ê³„[[next-steps]]

- [ì„¤ì¹˜ ê°€ì´ë“œ](installation)ì—ì„œ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ë„êµ¬ë¡œ smolagentsë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì„¸ìš”
- ë” ê³ ê¸‰ ê¸°ëŠ¥ì€ [ì•ˆë‚´ì„œ](guided_tour)ë¥¼ í™•ì¸í•˜ì„¸ìš”
- [ì»¤ìŠ¤í…€ ë„êµ¬ êµ¬ì¶•](tutorials/tools)ì— ëŒ€í•´ ì•Œì•„ë³´ì„¸ìš”
- [ì•ˆì „í•œ ì½”ë“œ ì‹¤í–‰](tutorials/secure_code_execution)ì„ ì‚´íŽ´ë³´ì„¸ìš”
- [ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ](tutorials/building_good_agents) ìƒì„± ë°©ë²•ì„ í™•ì¸í•˜ì„¸ìš”

<div class="mt-10">
  <div class="w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5">
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./guided_tour"
      ><div class="w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">ì•ˆë‚´ì„œ</div>
      <p class="text-gray-700">ê¸°ë³¸ ì‚¬í•­ì„ ë°°ìš°ê³  ì—ì´ì „íŠ¸ ì‚¬ìš©ì— ìµìˆ™í•´ì§€ì„¸ìš”. ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì—¬ê¸°ì„œ ì‹œìž‘í•˜ì„¸ìš”!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./examples/text_to_sql"
      ><div class="w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">ì‹¤ìŠµ ê°€ì´ë“œ</div>
      <p class="text-gray-700">íŠ¹ì • ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì‹¤ìš©ì ì¸ ê°€ì´ë“œ: SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!</p>
    </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./conceptual_guides/intro_agents"
      ><div class="w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">ê°œë… ê°€ì´ë“œ</div>
      <p class="text-gray-700">ì¤‘ìš”í•œ ì£¼ì œì— ëŒ€í•œ ì „ì²´ì ì¸ ì´í•´ë¥¼ ë•ëŠ” ì„¤ëª…ìž…ë‹ˆë‹¤.</p>
   </a>
    <a class="!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg" href="./tutorials/building_good_agents"
      ><div class="w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed">íŠœí† ë¦¬ì–¼</div>
      <p class="text-gray-700">ì—ì´ì „íŠ¸ êµ¬ì¶•ì˜ ì¤‘ìš”í•œ ì¸¡ë©´ì„ ë‹¤ë£¨ëŠ” í¬ê´„ì ì¸ íŠœí† ë¦¬ì–¼ìž…ë‹ˆë‹¤.</p>
    </a>
  </div>
</div>



================================================
FILE: docs/source/ko/installation.md
================================================
# ì„¤ì¹˜ ì˜µì…˜[[installation-options]]

`smolagents` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” pipë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ì–‘í•œ ì„¤ì¹˜ ë°©ë²•ê³¼ ì˜µì…˜ì„ ì†Œê°œí•©ë‹ˆë‹¤.

## ì‚¬ì „ ìš”êµ¬ì‚¬í•­[[prerequisites]]
- Python 3.10 ì´ìƒ
- Python íŒ¨í‚¤ì§€ ê´€ë¦¬ìž: [`pip`](https://pip.pypa.io/en/stable/) ë˜ëŠ” [`uv`](https://docs.astral.sh/uv/)

## ê°€ìƒ í™˜ê²½[[virtual-environment]]

`smolagents`ë¥¼ Python ê°€ìƒ í™˜ê²½ ë‚´ì—ì„œ ì„¤ì¹˜í•˜ëŠ” ê²ƒì„ ê°•ë ¥ížˆ ê¶Œìž¥í•©ë‹ˆë‹¤.
ê°€ìƒ í™˜ê²½ì€ í”„ë¡œì íŠ¸ì˜ ì˜ì¡´ì„±ì„ ë‹¤ë¥¸ Python í”„ë¡œì íŠ¸ì™€ ì‹œìŠ¤í…œ Python ì„¤ì¹˜ë¡œë¶€í„° ê²©ë¦¬í•˜ì—¬
ë²„ì „ ì¶©ëŒì„ ë°©ì§€í•˜ê³  íŒ¨í‚¤ì§€ ê´€ë¦¬ë¥¼ ë”ìš± ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.

<hfoptions id="virtual-environment">
<hfoption id="venv">

[`venv`](https://docs.python.org/3/library/venv.html) ì‚¬ìš©:

```bash
python -m venv .venv
source .venv/bin/activate
```

</hfoption>
<hfoption id="uv">

[`uv`](https://docs.astral.sh/uv/) ì‚¬ìš©:

```bash
uv venv .venv
source .venv/bin/activate
```

</hfoption>
</hfoptions>

## ê¸°ë³¸ ì„¤ì¹˜[[basic-installation]]

`smolagents` í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:

<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install smolagents
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install smolagents
```
</hfoption>
</hfoptions>

## ì¶”ê°€ ê¸°ëŠ¥ê³¼ í•¨ê»˜ ì„¤ì¹˜[[installation-with-extras]]

`smolagents`ëŠ” í•„ìš”ì— ë”°ë¼ ì„¤ì¹˜í•  ìˆ˜ ìžˆëŠ” ì—¬ëŸ¬ ì„ íƒì  ì˜ì¡´ì„±(extras)ì„ ì œê³µí•©ë‹ˆë‹¤.
ë‹¤ìŒ êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ ì¶”ê°€ ê¸°ëŠ¥ì„ ì„¤ì¹˜í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[extra1,extra2]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[extra1,extra2]"
```
</hfoption>
</hfoptions>

### ë„êµ¬[[tools]]
ë‹¤ìŒ ì¶”ê°€ ê¸°ëŠ¥ì€ ë‹¤ì–‘í•œ ë„êµ¬ì™€ í†µí•©ì„ í¬í•¨í•©ë‹ˆë‹¤:
<hfoptions id="installation">
<hfoption id="pip">
- **toolkit**: ì¼ë°˜ì ì¸ ìž‘ì—…ì„ ìœ„í•œ ê¸°ë³¸ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[toolkit]"
  ```
- **mcp**: ì™¸ë¶€ ë„êµ¬ ë° ì„œë¹„ìŠ¤ì™€ í†µí•©í•˜ê¸° ìœ„í•œ Model Context Protocol (MCP) ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[mcp]"
  ```
</hfoption>
<hfoption id="uv">
- **toolkit**: ì¼ë°˜ì ì¸ ìž‘ì—…ì„ ìœ„í•œ ê¸°ë³¸ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[toolkit]"
  ```
- **mcp**: ì™¸ë¶€ ë„êµ¬ ë° ì„œë¹„ìŠ¤ì™€ í†µí•©í•˜ê¸° ìœ„í•œ Model Context Protocol (MCP) ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[mcp]"
  ```
</hfoption>
</hfoptions>

### ëª¨ë¸ í†µí•©[[model-integration]]
ë‹¤ìŒ ì¶”ê°€ ê¸°ëŠ¥ì€ ë‹¤ì–‘í•œ AI ëª¨ë¸ ë° í”„ë ˆìž„ì›Œí¬ì™€ì˜ í†µí•©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤:
<hfoptions id="installation">
<hfoption id="pip">
- **openai**: OpenAI API ëª¨ë¸ ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[openai]"
  ```
- **transformers**: Hugging Face íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[transformers]"
  ```
- **vllm**: íš¨ìœ¨ì ì¸ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ VLLM ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[vllm]"
  ```
- **mlx-lm**: MLX-LM ëª¨ë¸ ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[mlx-lm]"
  ```
- **litellm**: ê²½ëŸ‰ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ LiteLLM ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[litellm]"
  ```
- **bedrock**: AWS Bedrock ëª¨ë¸ ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[bedrock]"
  ```
</hfoption>
<hfoption id="uv">
- **openai**: OpenAI API ëª¨ë¸ ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[openai]"
  ```
- **transformers**: Hugging Face íŠ¸ëžœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[transformers]"
  ```
- **vllm**: íš¨ìœ¨ì ì¸ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ VLLM ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[vllm]"
  ```
- **mlx-lm**: MLX-LM ëª¨ë¸ ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[mlx-lm]"
  ```
- **litellm**: ê²½ëŸ‰ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•œ LiteLLM ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[litellm]"
  ```
- **bedrock**: AWS Bedrock ëª¨ë¸ ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[bedrock]"
  ```
</hfoption>
</hfoptions>

### ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥[[multimodal-capabilities]]
ë‹¤ì–‘í•œ ë¯¸ë””ì–´ ìœ í˜• ë° ìž…ë ¥ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ ê¸°ëŠ¥:
<hfoptions id="installation">
<hfoption id="pip">
- **vision**: ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì»´í“¨í„° ë¹„ì „ ìž‘ì—… ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[vision]"
  ```
- **audio**: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[audio]"
  ```
</hfoption>
<hfoption id="uv">
- **vision**: ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ì»´í“¨í„° ë¹„ì „ ìž‘ì—… ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[vision]"
  ```
- **audio**: ì˜¤ë””ì˜¤ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[audio]"
  ```
</hfoption>
</hfoptions>

### ì›ê²© ì‹¤í–‰[[remote-execution]]
ì½”ë“œë¥¼ ì›ê²©ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì¶”ê°€ ê¸°ëŠ¥:
<hfoptions id="installation">
<hfoption id="pip">
- **docker**: Docker ì»¨í…Œì´ë„ˆì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[docker]"
  ```
- **e2b**: ì›ê²© ì‹¤í–‰ì„ ìœ„í•œ E2B ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[e2b]"
  ```
</hfoption>
<hfoption id="uv">
- **docker**: Docker ì»¨í…Œì´ë„ˆì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[docker]"
  ```
- **e2b**: ì›ê²© ì‹¤í–‰ì„ ìœ„í•œ E2B ì§€ì›ì„ í™œì„±í™”í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[e2b]"
  ```
</hfoption>
</hfoptions>

### í…”ë ˆë©”íŠ¸ë¦¬ ë° ì‚¬ìš©ìž ì¸í„°íŽ˜ì´ìŠ¤[[telemetry-and-user-interface]]
í…”ë ˆë©”íŠ¸ë¦¬, ëª¨ë‹ˆí„°ë§ ë° ì‚¬ìš©ìž ì¸í„°íŽ˜ì´ìŠ¤ êµ¬ì„± ìš”ì†Œë¥¼ ìœ„í•œ ì¶”ê°€ ê¸°ëŠ¥:
<hfoptions id="installation">
<hfoption id="pip">
- **telemetry**: ëª¨ë‹ˆí„°ë§ ë° ì¶”ì  ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[telemetry]"
  ```
- **gradio**: ëŒ€í™”í˜• Gradio UI êµ¬ì„± ìš”ì†Œ ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  pip install "smolagents[gradio]"
  ```
</hfoption>
<hfoption id="uv">
- **telemetry**: ëª¨ë‹ˆí„°ë§ ë° ì¶”ì  ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[telemetry]"
  ```
- **gradio**: ëŒ€í™”í˜• Gradio UI êµ¬ì„± ìš”ì†Œ ì§€ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
  ```bash
  uv pip install "smolagents[gradio]"
  ```
</hfoption>
</hfoptions>

### ì „ì²´ ì„¤ì¹˜[[complete-installation]]
ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì¶”ê°€ ê¸°ëŠ¥ì„ ì„¤ì¹˜í•˜ë ¤ë©´ ë‹¤ìŒì„ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
<hfoptions id="installation">
<hfoption id="pip">
```bash
pip install "smolagents[all]"
```
</hfoption>
<hfoption id="uv">
```bash
uv pip install "smolagents[all]"
```
</hfoption>
</hfoptions>

## ì„¤ì¹˜ í™•ì¸[[verifying-installation]]
ì„¤ì¹˜ í›„, ë‹¤ìŒ ì½”ë“œë¥¼ ì‹¤í–‰í•´ `smolagents`ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
```python
import smolagents
print(smolagents.__version__)
```

## ë‹¤ìŒ ë‹¨ê³„[[next-steps]]
`smolagents`ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜í–ˆë‹¤ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
- [ì•ˆë‚´ì„œ](./guided_tour)ë¥¼ ë”°ë¼ ê¸°ë³¸ ê°œë…ì„ ë°°ì›Œë³´ì„¸ìš”.
- ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ [ì‚¬ìš©ë²• ê°€ì´ë“œ](./examples/text_to_sql)ë¥¼ ì‚´íŽ´ë³´ì„¸ìš”.
- ê³ ìˆ˜ì¤€ ì„¤ëª…ì„ ë³´ë ¤ë©´ [ê°œë… ê°€ì´ë“œ](./conceptual_guides/intro_agents)ë¥¼ ì½ì–´ë³´ì„¸ìš”.
- ì—ì´ì „íŠ¸ êµ¬ì¶•ì— ëŒ€í•œ ì‹¬í™” íŠœí† ë¦¬ì–¼ì€ [íŠœí† ë¦¬ì–¼](./tutorials/building_good_agents)ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.
- í´ëž˜ìŠ¤ì™€ í•¨ìˆ˜ì— ëŒ€í•œ ìžì„¸í•œ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ì‹¶ìœ¼ì‹œë©´ [API ë ˆí¼ëŸ°ìŠ¤](./reference/index)ë¥¼ ì‚´íŽ´ë³´ì„¸ìš”.



================================================
FILE: docs/source/ko/conceptual_guides/react.md
================================================
# ë©€í‹°ìŠ¤í… ì—ì´ì „íŠ¸ëŠ” ì–´ë–»ê²Œ ë™ìž‘í•˜ë‚˜ìš”?[[how-do-multi-step-agents-work]]

ReAct í”„ë ˆìž„ì›Œí¬([Yao et al., 2022](https://huggingface.co/papers/2210.03629))ëŠ” í˜„ìž¬ ì—ì´ì „íŠ¸ êµ¬ì¶•í•˜ëŠ” ê°€ìž¥ ì¼ë°˜ì ì¸ ì ‘ê·¼ ë°©ì‹ìž…ë‹ˆë‹¤.

ReActë¼ëŠ” ì´ë¦„ì€ "ì¶”ë¡ (Reason)"ê³¼ "í–‰ë™(Act)"ì„ ê²°í•©í•œ ê²ƒìž…ë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ êµ¬ì¡°ë¥¼ ë”°ë¥´ëŠ” ì—ì´ì „íŠ¸ëŠ” ì£¼ì–´ì§„ ìž‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë§Œí¼ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤. ê° ë‹¨ê³„ëŠ” ì¶”ë¡  ë‹¨ê³„ì™€ í–‰ë™ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì ¸ ìžˆìœ¼ë©°, í–‰ë™ ë‹¨ê³„ì—ì„œëŠ” ìž‘ì—… í•´ê²°ì— ê°€ê¹Œì›Œì§€ë„ë¡ ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

`smolagents`ê°€ ì œê³µí•˜ëŠ” ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ReAct í”„ë ˆìž„ì›Œí¬ë¥¼ ì¶”ìƒí™”í•œ ë‹¨ì¼ `MultiStepAgent` í´ëž˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

ì´ í´ëž˜ìŠ¤ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì•„ëž˜ì™€ ê°™ì€ ë£¨í”„ë¡œ ë™ìž‘í•˜ë©°, ê¸°ì¡´ ë³€ìˆ˜ì™€ ì§€ì‹ë„ ì—ì´ì „íŠ¸ ë¡œê·¸ì— í•¨ê»˜ ë°˜ì˜ë©ë‹ˆë‹¤.

ì´ˆê¸°í™”: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” `SystemPromptStep`ì— ì €ìž¥ë˜ê³ , ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ ì¿¼ë¦¬ëŠ” `TaskStep`ì— ê¸°ë¡ë©ë‹ˆë‹¤.

While ë£¨í”„ (ReAct ë£¨í”„):

- `agent.write_memory_to_messages()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ë¡œê·¸ë¥¼ LLMì´ ì½ì„ ìˆ˜ ìžˆëŠ” [ì±„íŒ… ë©”ì‹œì§€](https://huggingface.co/docs/transformers/en/chat_templating) ëª©ë¡ì— ìž‘ì„±í•©ë‹ˆë‹¤.
- ì´ ë©”ì‹œì§€ë¥¼ `Model` ê°ì²´ì— ì „ì†¡í•˜ì—¬ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì•¡ì…˜(`ToolCallingAgent`ì˜ ê²½ìš° JSON blob, `CodeAgent`ì˜ ê²½ìš° ì½”ë“œ ìŠ¤ë‹ˆíŽ«)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
- ì•¡ì…˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ê¸°ë¡í•©ë‹ˆë‹¤(`ActionStep`).
- ê° ë‹¨ê³„ê°€ ëë‚  ë•Œë§ˆë‹¤ `agent.step_callbacks`ì— ì •ì˜ëœ ëª¨ë“  ì½œë°± í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**ê³„íš(planning)**ì´ í™œì„±í™”ëœ ê²½ìš°ì—ëŠ” ì£¼ê¸°ì ìœ¼ë¡œ ê³„íšì„ ìˆ˜ì •í•˜ê³  ì´ë¥¼ `PlanningStep`ì— ì €ìž¥í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ ê³¼ì •ì—ëŠ” í˜„ìž¬ ìž‘ì—…ê³¼ ê´€ë ¨ëœ ì‚¬ì‹¤ì„ ë©”ëª¨ë¦¬ì— ê¸°ë¡í•˜ëŠ” ê²ƒë„ í¬í•¨ë©ë‹ˆë‹¤.

`CodeAgent`ì˜ ê²½ìš°, ì´ ê³¼ì •ì€ ì•„ëž˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.

<div class="flex justify-center">
    <img
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/codeagent_docs.png"
    />
</div>

ë¹„ë””ì˜¤ë¥¼ í†µí•´ ë™ìž‘ ê³¼ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”.

<div class="flex justify-center">
    <img
        class="block dark:hidden"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
    <img
        class="hidden dark:block"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
</div>

`smolagents` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë‘ ê°€ì§€ ë²„ì „ì˜ ì—ì´ì „íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- ë„êµ¬ í˜¸ì¶œì„ Python ì½”ë“œ ìŠ¤ë‹ˆíŽ« í˜•íƒœë¡œ ìƒì„±í•˜ëŠ” [`CodeAgent`]
- ë§Žì€ í”„ë ˆìž„ì›Œí¬ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì²˜ëŸ¼ ë„êµ¬ í˜¸ì¶œì„ JSON í˜•íƒœë¡œ ìž‘ì„±í•˜ëŠ” [`ToolCallingAgent`]

í•„ìš”ì— ë”°ë¼ ë‘ ë°©ì‹ ì¤‘ ì–´ëŠ ê²ƒì´ë“  ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›¹ ë¸Œë¼ìš°ì§•ì²˜ëŸ¼ ê° íŽ˜ì´ì§€ ìƒí˜¸ìž‘ìš© í›„ ëŒ€ê¸° ì‹œê°„ì´ í•„ìš”í•œ ê²½ìš° JSON ê¸°ë°˜ ë„êµ¬ í˜¸ì¶œì´ ìž˜ ë§žì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

> [!TIP]
> ë©€í‹°ìŠ¤í… ì—ì´ì „íŠ¸ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ë‹¤ë©´ í—ˆê¹…íŽ˜ì´ìŠ¤ ë¸”ë¡œê·¸ì˜ [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) í¬ìŠ¤íŠ¸ë¥¼ ì½ì–´ë³´ì„¸ìš”.


================================================
FILE: docs/source/ko/examples/async_agent.md
================================================
# ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ë¹„ë™ê¸° ì• í”Œë¦¬ì¼€ì´ì…˜[[async-applications-with-agents]]

ì´ ê°€ì´ë“œëŠ” smolagents ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë™ê¸° ì—ì´ì „íŠ¸ë¥¼ Starlette ê¸°ë°˜ì˜ ë¹„ë™ê¸° Python ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì— í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
ë¹„ë™ê¸° Pythonê³¼ ì—ì´ì „íŠ¸ í†µí•©ì„ ì²˜ìŒ ì ‘í•˜ëŠ” ì‚¬ìš©ìžë“¤ì´ ë™ê¸° ì—ì´ì „íŠ¸ ë¡œì§ê³¼ ë¹„ë™ê¸° ì›¹ ì„œë²„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ëª¨ë²” ì‚¬ë¡€ë¥¼ ìµíž ìˆ˜ ìžˆë„ë¡ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

## ê°œìš”[[overview]]

- **Starlette**: Pythonì—ì„œ ë¹„ë™ê¸° ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê²½ëŸ‰ ASGI í”„ë ˆìž„ì›Œí¬ìž…ë‹ˆë‹¤.
- **anyio.to_thread.run_sync**: ë¸”ë¡œí‚¹(ë™ê¸°) ì½”ë“œë¥¼ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ë¹„ë™ê¸° ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì°¨ë‹¨í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ìœ í‹¸ë¦¬í‹°ìž…ë‹ˆë‹¤.
- **CodeAgent**: í”„ë¡œê·¸ëž˜ë° ë°©ì‹ìœ¼ë¡œ ìž‘ì—…ì„ í•´ê²°í•  ìˆ˜ ìžˆëŠ” `smolagents` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

## ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?[[why-use-a-background-thread?]]

`CodeAgent.run()`ì€ Python ì½”ë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ë¹„ë™ê¸° ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì§ì ‘ í˜¸ì¶œí•˜ë©´ Starletteì˜ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ì°¨ë‹¨í•˜ì—¬ ì„±ëŠ¥ê³¼ í™•ìž¥ì„±ì´ ì €í•˜ë©ë‹ˆë‹¤. `anyio.to_thread.run_sync`ë¡œ ì´ ìž‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ìœ„ìž„í•˜ë©´ ë†’ì€ ë™ì‹œì„±ì—ì„œë„ ì•±ì˜ ì‘ë‹µì„±ê³¼ íš¨ìœ¨ì„±ì„ ìœ ì§€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## ì˜ˆì‹œ ì›Œí¬í”Œë¡œìš°[[example-workflow]]

- Starlette ì•±ì€ `task` ë¬¸ìžì—´ì´ í¬í•¨ëœ JSON íŽ˜ì´ë¡œë“œë¥¼ ë°›ëŠ” `/run-agent` ì—”ë“œí¬ì¸íŠ¸ë¥¼ ë…¸ì¶œí•©ë‹ˆë‹¤.
- ìš”ì²­ì´ ìˆ˜ì‹ ë˜ë©´ `anyio.to_thread.run_sync`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” JSON ì‘ë‹µìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.

## CodeAgentë¥¼ í™œìš©í•œ Starlette ì•± êµ¬ì¶•[[building-a-starlette-app-with-a-codeagent]]

### 1. ì˜ì¡´ì„± ì„¤ì¹˜[[1.-install-dependencies]]

```bash
pip install smolagents starlette anyio uvicorn
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ (`main.py`)[[2.-application-code-(`main.py`)]]

```python
import anyio.to_thread
from starlette.applications import Starlette
from starlette.requests import Request
from starlette.responses import JSONResponse
from starlette.routing import Route

from smolagents import CodeAgent, InferenceClientModel

agent = CodeAgent(
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
    tools=[],
)

async def run_agent(request: Request):
    data = await request.json()
    task = data.get("task", "")
    # Run the agent synchronously in a background thread
    result = await anyio.to_thread.run_sync(agent.run, task)
    return JSONResponse({"result": result})

app = Starlette(routes=[
    Route("/run-agent", run_agent, methods=["POST"]),
])
```

### 3. ì•± ì‹¤í–‰[[3.-run-the-app]]

```bash
uvicorn async_agent.main:app --reload
```

### 4. ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸[[4.-test-the-endpoint]]

```bash
curl -X POST http://localhost:8000/run-agent -H 'Content-Type: application/json' -d '{"task": "What is 2+2?"}'
```

**ì˜ˆìƒ ì‘ë‹µ:**

```json
{"result": "4"}
```

## ì¶”ê°€ ìžë£Œ[[further-reading]]

- [Starlette ë¬¸ì„œ](https://www.starlette.io/)
- [anyio ë¬¸ì„œ](https://anyio.readthedocs.io/)

---

ì „ì²´ ì½”ë“œëŠ” [`examples/async_agent`](https://github.com/huggingface/smolagents/tree/main/examples/async_agent)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.



================================================
FILE: docs/source/ko/examples/multiagents.md
================================================
# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ðŸ¤–ðŸ¤ðŸ¤–

[[Colabì—ì„œ ì—´ê¸°]]

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ë©€í‹° ì—ì´ì „íŠ¸ ì›¹ ë¸Œë¼ìš°ì €**ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. ì´ëŠ” ì›¹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ëŠ” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œìž…ë‹ˆë‹¤!

ë©€í‹° ì—ì´ì „íŠ¸ëŠ” ê°„ë‹¨í•œ ê³„ì¸µ êµ¬ì¡°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```
              +----------------+
              | Manager agent  |
              +----------------+
                       |
        _______________|______________
       |                              |
Code Interpreter            +------------------+
    tool                    | Web Search agent |
                            +------------------+
                               |            |
                        Web Search tool     |
                                   Visit webpage tool
```
ì´ ì‹œìŠ¤í…œì„ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. 

ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```py
!pip install smolagents[toolkit] --upgrade -q
```

Inference Providersë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ Hugging Faceì— ë¡œê·¸ì¸í•©ë‹ˆë‹¤:

```py
from huggingface_hub import login

login()
```

âš¡ï¸ ì—ì´ì „íŠ¸ëŠ” Hugging Faceì˜ Inference APIë¥¼ ì‚¬ìš©í•˜ëŠ” `InferenceClientModel` í´ëž˜ìŠ¤ë¥¼ í†µí•´ [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)ë¡œ êµ¬ë™ë©ë‹ˆë‹¤. Inference APIë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë“  ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ ë¹ ë¥´ê³  ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

> [!TIP]
> Inference ProvidersëŠ” ì„œë²„ë¦¬ìŠ¤ ì¶”ë¡  íŒŒíŠ¸ë„ˆê°€ ì§€ì›í•˜ëŠ” ìˆ˜ë°± ê°œì˜ ëª¨ë¸ì— ëŒ€í•œ ì•¡ì„¸ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” í”„ë¡œë°”ì´ë” ëª©ë¡ì€ [ì—¬ê¸°](https://huggingface.co/docs/inference-providers/index)ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```py
model_id = "Qwen/Qwen3-Next-80B-A3B-Thinking"
```

## ðŸ” ì›¹ ê²€ìƒ‰ ë„êµ¬ ìƒì„±

ì›¹ ë¸Œë¼ìš°ì§•ì„ ìœ„í•´ Google ê²€ìƒ‰ê³¼ ë™ë“±í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ê¸°ë³¸ [`WebSearchTool`] ë„êµ¬ë¥¼ ì´ë¯¸ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ `WebSearchTool`ì—ì„œ ì°¾ì€ íŽ˜ì´ì§€ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆëŠ” ê¸°ëŠ¥ë„ í•„ìš”í•©ë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë‚´ìž¥ëœ `VisitWebpageTool`ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆì§€ë§Œ, ìž‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì§ì ‘ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

`markdownify`ë¥¼ ì‚¬ìš©í•˜ì—¬ `VisitWebpageTool` ë„êµ¬ë¥¼ ì²˜ìŒë¶€í„° ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```py
import re
import requests
from markdownify import markdownify
from requests.exceptions import RequestException
from smolagents import tool


@tool
def visit_webpage(url: str) -> str:
    """ì£¼ì–´ì§„ URLì˜ ì›¹íŽ˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê·¸ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë°˜í™˜í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
        url: ë°©ë¬¸í•  ì›¹íŽ˜ì´ì§€ì˜ URL.

    ë°˜í™˜ê°’:
        ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ëœ ì›¹íŽ˜ì´ì§€ ë‚´ìš©, ë˜ëŠ” ìš”ì²­ì´ ì‹¤íŒ¨í•  ê²½ìš° ì˜¤ë¥˜ ë©”ì‹œì§€.
    """
    try:
        # URLì— GET ìš”ì²­ ì „ì†¡
        response = requests.get(url)
        response.raise_for_status()  # ìž˜ëª»ëœ ìƒíƒœ ì½”ë“œì— ëŒ€í•´ ì˜ˆì™¸ ë°œìƒ

        # HTML ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        markdown_content = markdownify(response.text).strip()

        # ì—¬ëŸ¬ ì¤„ ë°”ê¿ˆ ì œê±°
        markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

        return markdown_content

    except RequestException as e:
        return f"Error fetching the webpage: {str(e)}"
    except Exception as e:
        return f"An unexpected error occurred: {str(e)}"
```

ì´ì œ ë„êµ¬ë¥¼ ì´ˆê¸°í™”í•˜ê³  í…ŒìŠ¤íŠ¸í•´ë³´ê² ìŠµë‹ˆë‹¤!

```py
print(visit_webpage("https://en.wikipedia.org/wiki/Hugging_Face")[:500])
```

## ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ êµ¬ì¶• ðŸ¤–ðŸ¤ðŸ¤–

ì´ì œ `search`ì™€ `visit_webpage` ë„êµ¬ê°€ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì´ ì—ì´ì „íŠ¸ì— ì–´ë–¤ êµ¬ì„±ì„ ì„ íƒí• ê¹Œìš”?
- ì›¹ ë¸Œë¼ìš°ì§•ì€ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”ì—†ëŠ” ë‹¨ì¼ íƒ€ìž„ë¼ì¸ ìž‘ì—…ì´ë¯€ë¡œ, JSON ë„êµ¬ í˜¸ì¶œ ë°©ì‹ì´ ì í•©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ `ToolCallingAgent`ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
- ë˜í•œ ì›¹ ê²€ìƒ‰ì€ ì˜¬ë°”ë¥¸ ë‹µì„ ì°¾ê¸° ì „ì— ë§Žì€ íŽ˜ì´ì§€ë¥¼ íƒìƒ‰í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ìžˆìœ¼ë¯€ë¡œ, `max_steps`ë¥¼ 10ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    InferenceClientModel,
    WebSearchTool,
)

model = InferenceClientModel(model_id=model_id)

web_agent = ToolCallingAgent(
    tools=[WebSearchTool(), visit_webpage],
    model=model,
    max_steps=10,
    name="web_search_agent",
    description="Runs web searches for you.",
)
```

ì´ ì—ì´ì „íŠ¸ì— `name`ê³¼ `description` ì†ì„±ì„ ë¶€ì—¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì´ ì—ì´ì „íŠ¸ê°€ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ì— ì˜í•´ í˜¸ì¶œë  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” í•„ìˆ˜ ì†ì„±ìž…ë‹ˆë‹¤.

ê·¸ ë‹¤ìŒ ë§¤ë‹ˆì € ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ì´ˆê¸°í™” ì‹œ `managed_agents` ì¸ìˆ˜ì— ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.

ì´ ì—ì´ì „íŠ¸ëŠ” ê³„íšê³¼ ì‚¬ê³ ë¥¼ ë‹´ë‹¹í•˜ë¯€ë¡œ, ê³ ê¸‰ ì¶”ë¡ ì´ ìœ ìš©í•  ê²ƒìž…ë‹ˆë‹¤. ë”°ë¼ì„œ `CodeAgent`ê°€ ìž˜ ìž‘ë™í•  ê²ƒìž…ë‹ˆë‹¤.

ë˜í•œ í˜„ìž¬ ì—°ë„ë¥¼ í¬í•¨í•˜ê³  ì¶”ê°€ ë°ì´í„° ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ê³  ì‹¶ìœ¼ë¯€ë¡œ, ì—ì´ì „íŠ¸ê°€ ì´ëŸ¬í•œ íŒ¨í‚¤ì§€ë¥¼ í•„ìš”ë¡œ í•  ê²½ìš°ì— ëŒ€ë¹„í•´ `additional_authorized_imports=["time", "numpy", "pandas"]`ë¥¼ ì¶”ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤.

```py
manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[web_agent],
    additional_authorized_imports=["time", "numpy", "pandas"],
)
```

ì´ê²Œ ì „ë¶€ìž…ë‹ˆë‹¤! ì´ì œ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤! ê³„ì‚°ê³¼ ì—°êµ¬ê°€ ëª¨ë‘ í•„ìš”í•œ ì§ˆë¬¸ì„ ì„ íƒí•©ë‹ˆë‹¤.

```py
answer = manager_agent.run("LLM í›ˆë ¨ì´ í˜„ìž¬ ì†ë„ë¡œ 2030ë…„ê¹Œì§€ ê³„ì† í™•ìž¥ëœë‹¤ë©´, 2030ë…„ê¹Œì§€ ê°€ìž¥ í° í›ˆë ¨ ì‹¤í–‰ì— ì „ë ¥ì„ ê³µê¸‰í•˜ëŠ” ë° í•„ìš”í•œ ì „ë ¥ëŸ‰ì€ GW ë‹¨ìœ„ë¡œ ì–¼ë§ˆê°€ ë ê¹Œìš”? ì´ëŠ” ì¼ë¶€ êµ­ê°€ë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ ë¬´ì—‡ì— í•´ë‹¹í• ê¹Œìš”? ì‚¬ìš©ëœ ëª¨ë“  ìˆ˜ì¹˜ì— ëŒ€í•œ ì¶œì²˜ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.")
```

ë‹µë³€ìœ¼ë¡œ ì´ëŸ° ë³´ê³ ì„œë¥¼ ë°›ìŠµë‹ˆë‹¤.
```
í˜„ìž¬ ì„±ìž¥ ì „ë§ê³¼ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ ì¶”ì •ì— ë”°ë¥´ë©´,
2030ë…„ê¹Œì§€ LLM êµìœ¡ì´ í˜„ìž¬ ì†ë„ë¡œ ê³„ì† í™•ìž¥ëœë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì˜ˆìƒë©ë‹ˆë‹¤.

1. 2030ë…„ê¹Œì§€ ê°€ìž¥ í° í›ˆë ¨ ì‹¤í–‰ì— ì „ë ¥ì„ ê³µê¸‰í•˜ëŠ” ë° í•„ìš”í•œ ì „ë ¥ëŸ‰ì€ ì•½ 303.74 GWê°€ ë  ê²ƒì´ë©°, 
ì´ëŠ” ì—°ê°„ ì•½ 2,660,762 GWhë¡œ í™˜ì‚°ë©ë‹ˆë‹¤.

2. êµ­ê°€ë³„ ì „ë ¥ ì†Œë¹„ëŸ‰ ë¹„êµ
   - ì¤‘êµ­ ì´ ì „ë ¥ ì†Œë¹„ëŸ‰ì˜ ì•½ 34%ì— í•´ë‹¹í•©ë‹ˆë‹¤.
   - ì¸ë„(184%), ëŸ¬ì‹œì•„(267%), ì¼ë³¸(291%)ì˜ ì´ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ì´ˆê³¼í•  ê²ƒìž…ë‹ˆë‹¤.
   - ì´íƒˆë¦¬ì•„ë‚˜ ë©•ì‹œì½” ê°™ì€ êµ­ê°€ë“¤ì˜ ì „ë ¥ ì†Œë¹„ëŸ‰ì˜ ê±°ì˜ 9ë°°ê°€ ë©ë‹ˆë‹¤.

3. ìˆ˜ì¹˜ ì¶œì²˜
   - ë¯¸ëž˜ LLM í›ˆë ¨ì„ ìœ„í•œ 5 GWì˜ ì´ˆê¸° ì¶”ì •ì¹˜ëŠ” AWS CEO Matt Garmanì—ì„œ ë‚˜ì˜¨ ê²ƒìž…ë‹ˆë‹¤.
   - ì„±ìž¥ ì˜ˆì¸¡ì€ Springsì˜ ì‹œìž¥ ì¡°ì‚¬ì—ì„œ 79.80%ì˜ CAGRì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
   - êµ­ê°€ ì „ë ¥ ì†Œë¹„ ë°ì´í„°ëŠ” ì£¼ë¡œ 2021ë…„ ê¸°ì¤€ìœ¼ë¡œ ë¯¸êµ­ ì—ë„ˆì§€ ì •ë³´ ê´€ë¦¬ì²­ì—ì„œ ë‚˜ì˜¨ ê²ƒìž…ë‹ˆë‹¤.
```

[ìŠ¤ì¼€ì¼ë§ ê°€ì„¤](https://gwern.net/scaling-hypothesis)ì´ ê³„ì† ì°¸ì´ë¼ë©´ ìƒë‹¹ížˆ í° ë°œì „ì†Œê°€ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì—ì´ì „íŠ¸ë“¤ì´ ìž‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ íš¨ìœ¨ì ìœ¼ë¡œ í˜‘ë ¥í–ˆìŠµë‹ˆë‹¤! âœ…

ðŸ’¡ ì´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ë” ë§Žì€ ì—ì´ì „íŠ¸ë¡œ ì‰½ê²Œ í™•ìž¥í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤: í•˜ë‚˜ëŠ” ì½”ë“œ ì‹¤í–‰ì„, ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì›¹ ê²€ìƒ‰ì„,  ë˜ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” íŒŒì¼ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì‹ìœ¼ë¡œ...



================================================
FILE: docs/source/ko/examples/plan_customization.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/examples/rag.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/examples/text_to_sql.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/examples/using_different_models.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/examples/web_browser.md
================================================
# ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ì›¹ ë¸Œë¼ìš°ì € ìžë™í™” ðŸ¤–ðŸŒ[[web-browser-automation-with-agents-ðŸ¤–ðŸŒ]]

[[open-in-colab]]

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ì—ì´ì „íŠ¸ ê¸°ë°˜ ì›¹ ë¸Œë¼ìš°ì € ìžë™í™” ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤! ì´ ì‹œìŠ¤í…œì€ ì›¹ì‚¬ì´íŠ¸ íƒìƒ‰, ìš”ì†Œ ìƒí˜¸ìž‘ìš©, ì •ë³´ ìžë™ ì¶”ì¶œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

- [x] ì›¹ íŽ˜ì´ì§€ íƒìƒ‰
- [x] ìš”ì†Œ í´ë¦­
- [x] íŽ˜ì´ì§€ ë‚´ ê²€ìƒ‰
- [x] íŒì—… ë° ëª¨ë‹¬ ì²˜ë¦¬
- [x] ì •ë³´ ì¶”ì¶œ

ë‹¨ê³„ë³„ë¡œ ì´ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤!

ë¨¼ì € í•„ìš”í•œ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”.

```bash
pip install smolagents selenium helium pillow -q
```

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³  í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
from io import BytesIO
from time import sleep

import helium
from dotenv import load_dotenv
from PIL import Image
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from smolagents import CodeAgent, tool
from smolagents.agents import ActionStep

# í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
load_dotenv()
```

ì´ì œ ì—ì´ì „íŠ¸ê°€ ì›¹ íŽ˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ê³  ìƒí˜¸ìž‘ìš©í•  ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” í•µì‹¬ ë¸Œë¼ìš°ì € ìƒí˜¸ìž‘ìš© ë„êµ¬ë“¤ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```python
@tool
def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
    """
    í˜„ìž¬ íŽ˜ì´ì§€ì—ì„œ Ctrl + Fë¥¼ ì‚¬ìš©í•´ ì§€ì •ëœ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³ , në²ˆì§¸ë¡œ ë“±ìž¥í•˜ëŠ” ìœ„ì¹˜ë¡œ ì´ë™í•©ë‹ˆë‹¤.
    ì¸ìž:
        text: ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
        nth_result: ì´ë™í•  në²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ (ê¸°ë³¸ê°’: 1)
    """
    elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{text}')]")
    if nth_result > len(elements):
        raise Exception(f"Match nÂ°{nth_result} not found (only {len(elements)} matches found)")
    result = f"Found {len(elements)} matches for '{text}'."
    elem = elements[nth_result - 1]
    driver.execute_script("arguments[0].scrollIntoView(true);", elem)
    result += f"Focused on element {nth_result} of {len(elements)}"
    return result

@tool
def go_back() -> None:
    """ì´ì „ íŽ˜ì´ì§€ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."""
    driver.back()

@tool
def close_popups() -> str:
    """
    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows!
    This does not work on cookie consent banners.
    """
    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()
```

Chromeìœ¼ë¡œ ë¸Œë¼ìš°ì €ë¥¼ ì„¤ì •í•˜ê³  ìŠ¤í¬ë¦°ìƒ· ê¸°ëŠ¥ì„ êµ¬ì„±í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# Configure Chrome options
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--force-device-scale-factor=1")
chrome_options.add_argument("--window-size=1000,1350")
chrome_options.add_argument("--disable-pdf-viewer")
chrome_options.add_argument("--window-position=0,0")

# Initialize the browser
driver = helium.start_chrome(headless=False, options=chrome_options)

# Set up screenshot callback
def save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    driver = helium.get_driver()
    current_step = memory_step.step_number
    if driver is not None:
        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing
            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:
                previous_memory_step.observations_images = None
        png_bytes = driver.get_screenshot_as_png()
        image = Image.open(BytesIO(png_bytes))
        print(f"Captured a browser screenshot: {image.size} pixels")
        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists

    # Update observations with current URL
    url_info = f"Current url: {driver.current_url}"
    memory_step.observations = (
        url_info if memory_step.observations is None else memory_step.observations + "\n" + url_info
    )
```

ì´ì œ ì›¹ ìžë™í™” ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```python
from smolagents import InferenceClientModel

# Initialize the model
model_id = "Qwen/Qwen2-VL-72B-Instruct"  # You can change this to your preferred VLM model
model = InferenceClientModel(model_id=model_id)

# Create the agent
agent = CodeAgent(
    tools=[go_back, close_popups, search_item_ctrl_f],
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[save_screenshot],
    max_steps=20,
    verbosity_level=2,
)

# Import helium for the agent
agent.python_executor("from helium import *", agent.state)
```

ì—ì´ì „íŠ¸ê°€ ì›¹ ìžë™í™”ë¥¼ ìœ„í•´ Heliumì„ ì‚¬ìš©í•˜ë ¤ë©´ ì§€ì¹¨ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì œê³µí•  ì§€ì¹¨ìž…ë‹ˆë‹¤.

```python
helium_instructions = """
You can use helium to access websites. Don't bother about the helium driver, it's already managed.
We've already ran "from helium import *"
Then you can go to pages!
Code:
```py
go_to('github.com/trending')
```<end_code>

You can directly click clickable elements by inputting the text that appears on them.
Code:
```py
click("Top products")
```<end_code>

If it's a link:
Code:
```py
click(Link("Top products"))
```<end_code>

If you try to interact with an element and it's not found, you'll get a LookupError.
In general stop your action after each button click to see what happens on your screenshot.
Never try to login in a page.

To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
Code:
```py
scroll_down(num_pixels=1200) # This will scroll one viewport down
```<end_code>

When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
Just use your built-in tool `close_popups` to close them:
Code:
```py
close_popups()
```<end_code>

You can use .exists() to check for the existence of an element. For example:
Code:
```py
if Text('Accept cookies?').exists():
    click('I accept')
```<end_code>
"""
```

ì´ì œ ìž‘ì—…ê³¼ í•¨ê»˜ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤! Wikipediaì—ì„œ ì •ë³´ë¥¼ ì°¾ëŠ” ê²ƒì„ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
search_request = """
Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
"""

agent_output = agent.run(search_request + helium_instructions)
print("Final output:")
print(agent_output)
```

ìš”ì²­ì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ë¥¸ ìž‘ì—…ì„ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì œê°€ ì–¼ë§ˆë‚˜ ì—´ì‹¬ížˆ ì¼í•´ì•¼ í•˜ëŠ”ì§€ ì•Œì•„ë³´ëŠ” ìž‘ì—…ìž…ë‹ˆë‹¤.

```python
github_request = """
I'm trying to find how hard I have to work to get a repo in github.com/trending.
Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
"""

agent_output = agent.run(github_request + helium_instructions)
print("Final output:")
print(agent_output)
```

ì´ ì‹œìŠ¤í…œì€ íŠ¹ížˆ ë‹¤ìŒê³¼ ê°™ì€ ìž‘ì—…ì— íš¨ê³¼ì ìž…ë‹ˆë‹¤.
- ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
- ì›¹ ë¦¬ì„œì¹˜ ìžë™í™”
- UI í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- ì½˜í…ì¸  ëª¨ë‹ˆí„°ë§



================================================
FILE: docs/source/ko/reference/agents.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/reference/models.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/reference/tools.md
================================================
[Binary file]


================================================
FILE: docs/source/ko/tutorials/building_good_agents.md
================================================
# ì¢‹ì€ ì—ì´ì „íŠ¸ êµ¬ì¶•í•˜ê¸°[[building-good-agents]]

[[open-in-colab]]

ì„±ê³µí•˜ëŠ” ì—ì´ì „íŠ¸ì™€ ì‹¤íŒ¨í•˜ëŠ” ì—ì´ì „íŠ¸ ì‚¬ì´ì—ëŠ” í° ì°¨ì´ê°€ ìžˆìŠµë‹ˆë‹¤.
ì„±ê³µí•˜ëŠ” ì—ì´ì „íŠ¸ëŠ” ì–´ë–»ê²Œ ë§Œë“¤ ìˆ˜ ìžˆì„ê¹Œìš”?
ì´ ê°€ì´ë“œì—ì„œ ì—ì´ì „íŠ¸ êµ¬ì¶•ì˜ í•µì‹¬ ì›ì¹™ë“¤ì„ ì†Œê°œí•˜ê² ìŠµë‹ˆë‹¤.

> [!TIP]
> ì—ì´ì „íŠ¸ êµ¬ì¶•ì´ ì²˜ìŒì´ë¼ë©´ ë¨¼ì € [ì—ì´ì „íŠ¸ ì†Œê°œ](../conceptual_guides/intro_agents)ì™€ [ì•ˆë‚´ì„œ](../guided_tour)ë¥¼ ì½ì–´ë³´ì„¸ìš”.

### ìµœê³ ì˜ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ê°€ìž¥ ë‹¨ìˆœí•©ë‹ˆë‹¤: ì›Œí¬í”Œë¡œìš°ë¥¼ ìµœëŒ€í•œ ë‹¨ìˆœí•˜ê²Œ ë§Œë“œì„¸ìš”[[the-best-agentic-systems-are-the-simplest:-simplify-the-workflow-as-much-as-you-can]]

ì›Œí¬í”Œë¡œìš°ì— LLMì—ê²Œ ì–´ëŠ ì •ë„ì˜ ìžìœ¨ì„±ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì€ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìœ„í—˜ì´ ìžˆìŠµë‹ˆë‹¤.

ìž˜ ì„¤ê³„ëœ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì€ ì˜¤ë¥˜ë¥¼ ê¸°ë¡í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ëŠ” ê¸°ëŠ¥ì„ í†µí•´ LLMì´ ìžì‹ ì˜ ì‹¤ìˆ˜ë¥¼ êµì •í•  ìˆ˜ ìžˆê²Œ í•´ì¤ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ê³  í•´ë„ ì²˜ìŒë¶€í„° LLMì´ ì‹¤ìˆ˜í•˜ì§€ ì•Šë„ë¡ ì›Œí¬í”Œë¡œìš°ë¥¼ ê°„ë‹¨í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ í›¨ì”¬ íš¨ê³¼ì ìž…ë‹ˆë‹¤.

[ì—ì´ì „íŠ¸ ì†Œê°œ](../conceptual_guides/intro_agents)ì˜ ì˜ˆì‹œë¥¼ ë‹¤ì‹œ ì‚´íŽ´ë³´ê² ìŠµë‹ˆë‹¤: ì„œí•‘ ì—¬í–‰ì‚¬ ì´ìš©ìžë“¤ì˜ ë¬¸ì˜ì— ëŒ€ì‘í•˜ëŠ” ë´‡ìž…ë‹ˆë‹¤.
ìƒˆë¡œìš´ ì„œí•‘ ìŠ¤íŒŸì— ëŒ€í•´ ì§ˆë¬¸ì„ ë°›ì„ ë•Œë§ˆë‹¤ ì—ì´ì „íŠ¸ê°€ "ì—¬í–‰ ê±°ë¦¬ API"ì™€ "ë‚ ì”¨ API"ì— ê°ê° 2ë²ˆì˜ ì„œë¡œ ë‹¤ë¥¸ í˜¸ì¶œì„ í•˜ë„ë¡ í•˜ëŠ” ëŒ€ì‹ , ë‘ APIë¥¼ í•œ ë²ˆì— í˜¸ì¶œí•˜ê³  ì—°ê²°ëœ ì¶œë ¥ì„ ì‚¬ìš©ìžì—ê²Œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì¸ "return_spot_information"ì´ë¼ëŠ” í•˜ë‚˜ì˜ í†µí•©ëœ ë„êµ¬ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì´ë ‡ê²Œ í•˜ë©´ ë¹„ìš©, ì§€ì—° ì‹œê°„, ì˜¤ë¥˜ ìœ„í—˜ì„ ì¤„ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤!

ì£¼ìš” ì§€ì¹¨ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: LLM í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœëŒ€í•œ ì¤„ì´ì„¸ìš”.

ì´ê²ƒì€ ëª‡ ê°€ì§€ ê²°ë¡ ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤:
- ê°€ëŠ¥í•˜ë©´ ì–¸ì œë“ ì§€ ë‘ ê°œì˜ API ì˜ˆì‹œì²˜ëŸ¼ 2ê°œì˜ ë„êµ¬ë¥¼ í•˜ë‚˜ë¡œ ê·¸ë£¹í™”í•˜ì„¸ìš”.
- ê°€ëŠ¥í•˜ë©´ ì–¸ì œë“ ì§€ ë¡œì§ì€ ì—ì´ì „íŠ¸ì˜ ê²°ì •ë³´ë‹¤ëŠ” ê²°ì •ë¡ ì  í•¨ìˆ˜ë¡œ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

### LLM ì—”ì§„ìœ¼ë¡œì˜ ì •ë³´ íë¦„ì„ ê°œì„ í•˜ì„¸ìš”[[improve-the-information-flow-to-the-llm-engine]]

LLMì€ ìª½ì§€ë¥¼ í†µí•´ì„œë§Œ ì†Œí†µí•  ìˆ˜ ìžˆëŠ” ë°€íëœ ë°© ì•ˆì˜ *ë˜‘ë˜‘í•œ* ë¡œë´‡ì´ë¼ê³  ìƒê°í•˜ë©´ ë©ë‹ˆë‹¤.

í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ëŠ”ì§€ ì „í˜€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‹ˆê¹Œ ì¼ë‹¨ ìž‘ì—…ì„ ì•„ì£¼ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ëŠ” ê²ƒë¶€í„° ì‹œìž‘í•˜ì„¸ìš”!
ì—ì´ì „íŠ¸ëŠ” LLMìœ¼ë¡œ ìž‘ë™í•˜ê¸° ë•Œë¬¸ì—, ìž‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ë°©ì‹ì´ ì¡°ê¸ˆë§Œ ë‹¬ë¼ì ¸ë„ ê²°ê³¼ê°€ ì™„ì „ížˆ ë°”ë€” ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ê·¸ ë‹¤ìŒì—” ë„êµ¬ì—ì„œ ì—ì´ì „íŠ¸ë¡œ ì •ë³´ê°€ ìž˜ ì „ë‹¬ë˜ë„ë¡ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.

êµ¬ì²´ì ìœ¼ë¡œëŠ” ì´ë ‡ê²Œ í•˜ì„¸ìš”:
- ê° ë„êµ¬ëŠ” LLMì—ê²Œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ë¥¼ ëª¨ë‘ ê¸°ë¡í•´ì•¼ í•©ë‹ˆë‹¤.(ë„êµ¬ì˜ `forward` ë©”ì„œë“œ ì•ˆì—ì„œ `print`ë¬¸ì„ ì“°ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.)
  - íŠ¹ížˆ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ì— ëŒ€í•œ ìžì„¸í•œ ì •ë³´ë¥¼ ê¸°ë¡í•˜ë©´ í° ë„ì›€ì´ ë©ë‹ˆë‹¤!

ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì¹˜ì™€ ë‚ ì§œ-ì‹œê°„ì„ ë°›ì•„ì„œ ë‚ ì”¨ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬ë¥¼ ë³´ê² ìŠµë‹ˆë‹¤:

ë¨¼ì € ì¢‹ì§€ ì•Šì€ ë²„ì „ìž…ë‹ˆë‹¤:
```python
import datetime
from smolagents import tool

def get_weather_report_at_coordinates(coordinates, date_time):
    # ë”ë¯¸ í•¨ìˆ˜, [ì„­ì”¨ ì˜¨ë„, 0-1 ì²™ë„ì˜ ë¹„ ì˜¬ í™•ë¥ , ë¯¸í„° ë‹¨ìœ„ íŒŒë„ ë†’ì´] ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    return [28.0, 0.35, 0.85]

def convert_location_to_coordinates(location):
    # ë”ë¯¸ ì¢Œí‘œë¥¼ ë°˜í™˜
    return [3.3, -42.0]

@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for.
        date_time: the date and time for which you want the report.
    """
    lon, lat = convert_location_to_coordinates(location)
    date_time = datetime.strptime(date_time)
    return str(get_weather_report_at_coordinates((lon, lat), date_time))
```

ë¬¸ì œì ì€ ë¬´ì—‡ì¼ê¹Œìš”?
- `date_time`ì— ì‚¬ìš©í•´ì•¼ í•˜ëŠ” í˜•ì‹ì— ëŒ€í•œ ì •í™•í•œ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.
- ìœ„ì¹˜ë¥¼ ì–´ë–»ê²Œ ì§€ì •í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.
- ìœ„ì¹˜ê°€ ì ì ˆí•œ í˜•ì‹ì´ ì•„ë‹ˆê±°ë‚˜ `date_time`ì´ ì œëŒ€ë¡œ í˜•ì‹í™”ë˜ì§€ ì•Šì€ ê²½ìš°ì™€ ê°™ì€ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë¡í•  ìˆ˜ ìžˆëŠ” ë¡œê¹… ë©”ì»¤ë‹ˆì¦˜ì´ ì—†ìŠµë‹ˆë‹¤.
- ì¶œë ¥ í˜•ì‹ì„ ì´í•´í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ë„êµ¬ í˜¸ì¶œì´ ì‹¤íŒ¨í•˜ë©´ ë©”ëª¨ë¦¬ì— ë¡œê¹…ëœ ì˜¤ë¥˜ ì¶”ì ì´ LLMì´ ë„êµ¬ë¥¼ ì—­ì„¤ê³„í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì™œ ê·¸ë ‡ê²Œ ë§Žì€ ë¬´ê±°ìš´ ìž‘ì—…ì„ ë§¡ê²¨ì•¼ í• ê¹Œìš”?

ì´ ë„êµ¬ë¥¼ êµ¬ì¶•í•˜ëŠ” ë” ë‚˜ì€ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
```python
@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for. Should be a place name, followed by possibly a city name, then a country, like "Anchor Point, Taghazout, Morocco".
        date_time: the date and time for which you want the report, formatted as '%m/%d/%y %H:%M:%S'.
    """
    lon, lat = convert_location_to_coordinates(location)
    try:
        date_time = datetime.strptime(date_time)
    except Exception as e:
        raise ValueError("Conversion of `date_time` to datetime format failed, make sure to provide a string in format '%m/%d/%y %H:%M:%S'. Full trace:" + str(e))
    temperature_celsius, risk_of_rain, wave_height = get_weather_report_at_coordinates((lon, lat), date_time)
    return f"Weather report for {location}, {date_time}: Temperature will be {temperature_celsius}Â°C, risk of rain is {risk_of_rain*100:.0f}%, wave height is {wave_height}m."
```

LLMì˜ ë¶€ë‹´ì„ ëœì–´ì£¼ë ¤ë©´ ì´ëŸ° ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”: "ë§Œì•½ ë‚´ê°€ ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ìƒíƒœì—ì„œ ì´ ë„êµ¬ë¥¼ ì²˜ìŒ ì‚¬ìš©í•œë‹¤ë©´, ì‹¤ìˆ˜í–ˆì„ ë•Œ ìŠ¤ìŠ¤ë¡œ ê³ ì¹˜ê¸°ê°€ ì–¼ë§ˆë‚˜ ì‰¬ìš¸ê¹Œ?"

### ì—ì´ì „íŠ¸ì— ë” ë§Žì€ ë§¤ê°œë³€ìˆ˜ ì œê³µ[[give-more-arguments-to-the-agent]]

ìž‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ë‹¨ìˆœí•œ ë¬¸ìžì—´ ì™¸ì— ì—ì´ì „íŠ¸ì— ì¶”ê°€ ê°ì²´ë¥¼ ì „ë‹¬í•˜ë ¤ë©´ `additional_args` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìœ í˜•ì˜ ê°ì²´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```py
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

agent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), add_base_tools=True)

agent.run(
    "Why does Mike not know many people in New York?",
    additional_args={"mp3_sound_file_url":'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3'}
)
```
ì˜ˆë¥¼ ë“¤ì–´, `additional_args` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ê°€ í™œìš©í•  ìˆ˜ ìžˆë„ë¡ ì›í•˜ëŠ” ì´ë¯¸ì§€ë‚˜ ë¬¸ìžì—´ì„ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ ë””ë²„ê¹… ë°©ë²•[[how-to-debug-your-agent]]

### 1. ë” ê°•ë ¥í•œ LLM ì‚¬ìš©[[use-a-stronger-llm]]

ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ì¤‘ ì¼ë¶€ëŠ” ì‹¤ì œ ì˜¤ë¥˜ì´ê³ , ë‹¤ë¥¸ ì¼ë¶€ëŠ” LLM ì—”ì§„ì´ ì œëŒ€ë¡œ ì¶”ë¡ í•˜ì§€ ëª»í•œ íƒ“ìž…ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ìžë™ì°¨ ê·¸ë¦¼ì„ ë§Œë“¤ì–´ ë‹¬ë¼ê³  ìš”ì²­í•œ `CodeAgent`ì— ëŒ€í•œ ë‹¤ìŒ ì¶”ì ì„ ê³ ë ¤í•´ë³´ì„¸ìš”:
```
==================================================================================================== New task ====================================================================================================
Make me a cool car picture
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
image_generator(prompt="A cool, futuristic sports car with LED headlights, aerodynamic design, and vibrant color, high-res, photorealistic")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Step 1:

- Time taken: 16.35 seconds
- Input tokens: 1,383
- Output tokens: 77
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final_answer("/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Print outputs:

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Final answer:
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
```
ì‚¬ìš©ìžëŠ” ì´ë¯¸ì§€ê°€ ë°˜í™˜ë˜ëŠ” ëŒ€ì‹  ê²½ë¡œê°€ ë°˜í™˜ë˜ëŠ” ê²ƒì„ ë³´ê²Œ ë©ë‹ˆë‹¤.
ì‹œìŠ¤í…œì˜ ë²„ê·¸ì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìžˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì´ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¨ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤: ë‹¨ì§€ LLMì´ ì´ë¯¸ì§€ ì¶œë ¥ì„ ë³€ìˆ˜ì— ì €ìž¥í•˜ì§€ ì•ŠëŠ” ì‹¤ìˆ˜ë¥¼ í–ˆì„ ë¿ìž…ë‹ˆë‹¤.
ë”°ë¼ì„œ ì´ë¯¸ì§€ë¥¼ ì €ìž¥í•˜ë©´ì„œ ë¡œê¹…ëœ ê²½ë¡œë¥¼ í™œìš©í•˜ëŠ” ê²ƒ ì™¸ì—ëŠ” ì´ë¯¸ì§€ì— ë‹¤ì‹œ ì ‘ê·¼í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë¯¸ì§€ ëŒ€ì‹  ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ì—ì´ì „íŠ¸ë¥¼ ë””ë²„ê¹…í•˜ëŠ” ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” "ë” ê°•ë ¥í•œ LLMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ"ìž…ë‹ˆë‹¤. `Qwen2/5-72B-Instruct`ì™€ ê°™ì€ ëŒ€ì•ˆì€ ê·¸ëŸ° ì‹¤ìˆ˜ë¥¼ í•˜ì§€ ì•Šì•˜ì„ ê²ƒìž…ë‹ˆë‹¤.

### 2. ë” ë§Žì€ ì •ë³´ë‚˜ êµ¬ì²´ì ì¸ ì§€ì¹¨ ì œê³µ[[provide-more-information-or-specific-instructions]]

ë” ìžì„¸í•˜ê²Œ ì•ˆë‚´í•´ì¤€ë‹¤ë©´ ì„±ëŠ¥ì´ ë‚®ì€ ëª¨ë¸ë„ ì¶©ë¶„ížˆ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ëª¨ë¸ì˜ ê´€ì ì—ì„œ ìƒê°í•´ë³´ì„¸ìš”: ë‚´ê°€ ëª¨ë¸ì´ ë˜ì–´ì„œ ì´ ìž‘ì—…ì„ í•´ê²°í•´ì•¼ í•œë‹¤ë©´, ì§€ê¸ˆ ì£¼ì–´ì§„ ì •ë³´(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ìž‘ì—… ì„¤ëª… + ë„êµ¬ ì„¤ëª…)ë§Œìœ¼ë¡œë„ ì¶©ë¶„í• ê¹Œìš”?

ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ê°€ í•„ìš”í• ê¹Œìš”?

- ì§€ì¹¨ì´ í•­ìƒ ì—ì´ì „íŠ¸ì—ê²Œ ì£¼ì–´ì ¸ì•¼ í•˜ëŠ” ê²½ìš°(ì¼ë°˜ì ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìž‘ë™í•œë‹¤ê³  ì´í•´í•˜ëŠ” ê²ƒì²˜ëŸ¼): ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œ `instructions` ë§¤ê°œë³€ìˆ˜ ì•„ëž˜ì— ë¬¸ìžì—´ë¡œ ì „ë‹¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- í•´ê²°í•  íŠ¹ì • ìž‘ì—…ì— ê´€í•œ ê²ƒì´ë¼ë©´: ì´ ëª¨ë“  ì„¸ë¶€ ì‚¬í•­ì„ ìž‘ì—…ì— ì¶”ê°€í•˜ì„¸ìš”. ìž‘ì—…ì€ ìˆ˜ì‹­ íŽ˜ì´ì§€ì²˜ëŸ¼ ë§¤ìš° ê¸¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- íŠ¹ì • ë„êµ¬ ì‚¬ìš© ë°©ë²•ì— ê´€í•œ ê²ƒì´ë¼ë©´: í•´ë‹¹ ë„êµ¬ì˜ `description` ì†ì„±ì— í¬í•¨ì‹œí‚¤ì„¸ìš”.

### 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ê²½ (ì¼ë°˜ì ìœ¼ë¡œ ê¶Œìž¥ë˜ì§€ ì•ŠìŒ)[[change-the-prompt-templates-(generally-not-advised)]]

ìœ„ì˜ ë°©ë²•ë“¤ë¡œë„ ë¶€ì¡±í•˜ë‹¤ë©´ ì—ì´ì „íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ìž‘ë™ ì›ë¦¬ë¥¼ ì‚´íŽ´ë³´ê² ìŠµë‹ˆë‹¤. [CodeAgent]ì˜ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤(ì œë¡œìƒ· ì˜ˆì œëŠ” ìƒëžµí•˜ê³  ê°„ë‹¨ížˆ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤).

```python
print(agent.prompt_templates["system_prompt"])
```
Here is what you get:
```text
You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
To solve the task, you must plan forward to proceed in a series of steps, in a cycle of Thought, Code, and Observation sequences.

At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
Then in the Code sequence you should write the code in simple Python. The code sequence must be opened with '{{code_block_opening_tag}}', and closed with '{{code_block_closing_tag}}'.
During each intermediate step, you can use 'print()' to save whatever important information you will then need.
These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
In the end you have to return a final answer using the `final_answer` tool.

Here are a few examples using notional tools:
---
Task: "Generate an image of the oldest person in this document."

Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.
{{code_block_opening_tag}}
answer = document_qa(document=document, question="Who is the oldest person mentioned?")
print(answer)
{{code_block_closing_tag}}
Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

Thought: I will now generate an image showcasing the oldest person.
{{code_block_opening_tag}}
image = image_generator("A portrait of John Doe, a 55-year-old man living in Canada.")
final_answer(image)
{{code_block_closing_tag}}

---
Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

Thought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool
{{code_block_opening_tag}}
result = 5 + 3 + 1294.678
final_answer(result)
{{code_block_closing_tag}}

---
Task:
"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.
You have been provided with these additional arguments, that you can access using the keys as variables in your python code:
{'question': 'Quel est l'animal sur l'image?', 'image': 'path/to/image.jpg'}"

Thought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.
{{code_block_opening_tag}}
translated_question = translator(question=question, src_lang="French", tgt_lang="English")
print(f"The translated question is {translated_question}.")
answer = image_qa(image=image, question=translated_question)
final_answer(f"The answer is {answer}")
{{code_block_closing_tag}}

---
Task:
In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.
What does he say was the consequence of Einstein learning too much math on his creativity, in one word?

Thought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.
{{code_block_opening_tag}}
pages = web_search(query="1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein")
print(pages)
{{code_block_closing_tag}}
Observation:
No result found for query "1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein".

Thought: The query was maybe too restrictive and did not find any results. Let's try again with a broader query.
{{code_block_opening_tag}}
pages = web_search(query="1979 interview Stanislaus Ulam")
print(pages)
{{code_block_closing_tag}}
Observation:
Found 6 pages:
[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)

[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)

(truncated)

Thought: I will read the first 2 pages to know more.
{{code_block_opening_tag}}
for url in ["https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/", "https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/"]:
    whole_page = visit_webpage(url)
    print(whole_page)
    print("\n" + "="*80 + "\n")  # Print separator between pages
{{code_block_closing_tag}}
Observation:
Manhattan Project Locations:
Los Alamos, NM
Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at
(truncated)

Thought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: "He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity." Let's answer in one word.
{{code_block_opening_tag}}
final_answer("diminished")
{{code_block_closing_tag}}

---
Task: "Which city has the highest population: Guangzhou or Shanghai?"

Thought: I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.
{{code_block_opening_tag}}
for city in ["Guangzhou", "Shanghai"]:
    print(f"Population {city}:", web_search(f"{city} population")
{{code_block_closing_tag}}
Observation:
Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
Population Shanghai: '26 million (2019)'

Thought: Now I know that Shanghai has the highest population.
{{code_block_opening_tag}}
final_answer("Shanghai")
{{code_block_closing_tag}}

---
Task: "What is the current age of the pope, raised to the power 0.36?"

Thought: I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.
{{code_block_opening_tag}}
pope_age_wiki = wikipedia_search(query="current pope age")
print("Pope age as per wikipedia:", pope_age_wiki)
pope_age_search = web_search(query="current pope age")
print("Pope age as per google search:", pope_age_search)
{{code_block_closing_tag}}
Observation:
Pope age: "The pope Francis is currently 88 years old."

Thought: I know that the pope is 88 years old. Let's compute the result using python code.
{{code_block_opening_tag}}
pope_current_age = 88 ** 0.36
final_answer(pope_current_age)
{{code_block_closing_tag}}

Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:
{{code_block_opening_tag}}
{%- for tool in tools.values() %}
{{ tool.to_code_prompt() }}
{% endfor %}
{{code_block_closing_tag}}

{%- if managed_agents and managed_agents.values() | list %}
You can also give tasks to team members.
Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
You can also include any relevant variables or context using the 'additional_args' argument.
Here is a list of the team members that you can call:
{{code_block_opening_tag}}
{%- for agent in managed_agents.values() %}
def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
    """{{ agent.description }}

    Args:
        task: Long detailed description of the task.
        additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
    """
{% endfor %}
{{code_block_closing_tag}}
{%- endif %}

Here are the rules you should always follow to solve your task:
1. Always provide a 'Thought:' sequence, and a '{{code_block_opening_tag}}' sequence ending with '{{code_block_closing_tag}}', else you will fail.
2. Use only variables that you have defined!
3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = wikipedia_search(query="What is the place where James Bond lives?")'.
4. For tools WITHOUT JSON output schema: Take care to not chain too many sequential tool calls in the same code block, as their output format is unpredictable. For instance, a call to wikipedia_search without a JSON output schema has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.
5. For tools WITH JSON output schema: You can confidently chain multiple tool calls and directly access structured output fields in the same code block! When a tool has a JSON output schema, you know exactly what fields and data types to expect, allowing you to write robust code that directly accesses the structured response (e.g., result['field_name']) without needing intermediate print() statements.
6. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.
7. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.
8. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.
9. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}
10. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.
11. Don't give up! You're in charge of solving the task, not providing directions to solve it.

{%- if custom_instructions %}
{{custom_instructions}}
{%- endif %}

Now Begin!
```

ë³´ì‹œë‹¤ì‹œí”¼ `"{{ tool.description }}"`ì™€ ê°™ì€ í”Œë ˆì´ìŠ¤í™€ë”ë“¤ì´ ìžˆìŠµë‹ˆë‹¤. ì´ê²ƒë“¤ì€ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ë„êµ¬ë‚˜ ê´€ë¦¬ ì—ì´ì „íŠ¸ì— ëŒ€í•œ ì„¤ëª…ì„ ìžë™ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ `system_prompt` ë§¤ê°œë³€ìˆ˜ì— ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ì–´ì„œ ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë®ì–´ì“¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ëŠ” ì´ëŸ° í”Œë ˆì´ìŠ¤í™€ë”ë“¤ì„ í¬í•¨í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:
- ë„êµ¬ ì„¤ëª…ì„ ì‚½ìž…í•˜ë ¤ë©´:
  ```
  {%- for tool in tools.values() %}
  - {{ tool.to_tool_calling_prompt() }}
  {%- endfor %}
  ```
- ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ê°€ ìžˆëŠ” ê²½ìš° í•´ë‹¹ ì„¤ëª…ì„ ì‚½ìž…í•˜ë ¤ë©´:
  ```
  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  {%- for agent in managed_agents.values() %}
  - {{ agent.name }}: {{ agent.description }}
  {%- endfor %}
  {%- endif %}
  ```
- `CodeAgent`ì—ë§Œ í•´ë‹¹í•˜ë©°, ìŠ¹ì¸ëœ import ëª©ë¡ì„ ì‚½ìž…í•˜ë ¤ë©´: `"{{authorized_imports}}"`

ê·¸ëŸ° ë‹¤ìŒ ë‹¤ìŒê³¼ ê°™ì´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³€ê²½í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

```py
agent.prompt_templates["system_prompt"] = agent.prompt_templates["system_prompt"] + "\nHere you go!"
```

ì´ëŠ” [`ToolCallingAgent`]ì—ì„œë„ ìž‘ë™í•©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì‹œ `instructions` ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ë” ê°„ë‹¨í•©ë‹ˆë‹¤:
```py
agent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), instructions="Always talk like a 5 year old.")
```

### 4. ì¶”ê°€ ê³„íš[[extra-planning]]

ì¼ë°˜ì ì¸ ìž‘ì—… ë‹¨ê³„ë“¤ ì¤‘ê°„ì¤‘ê°„ì— ì—ì´ì „íŠ¸ê°€ ì¶”ê°€ë¡œ ê³„íšì„ ì„¸ìš°ëŠ” ë‹¨ê³„ë¥¼ ë„£ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´ë•ŒëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , LLMì´ í˜„ìž¬ê¹Œì§€ íŒŒì•…í•œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ê³  ê·¸ ì •ë³´ë¥¼ í† ëŒ€ë¡œ ì•žìœ¼ë¡œì˜ ê³„íšì„ ë‹¤ì‹œ ì ê²€í•˜ê²Œ ë©ë‹ˆë‹¤.

```py
from smolagents import load_tool, CodeAgent, InferenceClientModel, WebSearchTool
from dotenv import load_dotenv

load_dotenv()

# Import tool from Hub
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

search_tool = WebSearchTool()

agent = CodeAgent(
    tools=[search_tool, image_generation_tool],
    model=InferenceClientModel(model_id="Qwen/Qwen2.5-72B-Instruct"),
    planning_interval=3 # This is where you activate planning!
)

# Run it!
result = agent.run(
    "How long would a cheetah at full speed take to run the length of Pont Alexandre III?",
)
```


================================================
FILE: docs/source/ko/tutorials/inspect_runs.md
================================================
# OpenTelemetryë¡œ ì‹¤í–‰ ê²€ì‚¬í•˜ê¸°[[inspecting-runs-with-opentelemetry]]

[[open-in-colab]]

> [!TIP]
> ì—ì´ì „íŠ¸ êµ¬ì¶•ì´ ì²˜ìŒì´ë¼ë©´ ë¨¼ì € [ì—ì´ì „íŠ¸ ì†Œê°œ](../conceptual_guides/intro_agents)ì™€ [ì•ˆë‚´ì„œ](../guided_tour)ë¥¼ ì½ì–´ë³´ì„¸ìš”.

## ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ë¡œê¹…í•˜ëŠ” ì´ìœ ëŠ”?[[why-log-your-agent-runs?]]

ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ë””ë²„ê¹…í•˜ëŠ” ê²ƒì€ ë³µìž¡í•œ ìž‘ì—…ìž…ë‹ˆë‹¤.

ì‹¤í–‰ì´ ì œëŒ€ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°ëŠ” ì„¤ê³„ìƒ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ìž…ë‹ˆë‹¤(ë§Œì•½ ì˜ˆì¸¡ ê°€ëŠ¥í–ˆë‹¤ë©´ ì¼ë°˜ì ì¸ ì½”ë“œë¥¼ ì‚¬ìš©í–ˆì„ ê²ƒìž…ë‹ˆë‹¤).

ì‹¤í–‰ ê³¼ì •ì„ ì‚´íŽ´ë³´ëŠ” ê²ƒë„ ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë‹¨ê³„ ì—ì´ì „íŠ¸ëŠ” ì½˜ì†”ì„ ë¡œê·¸ë¡œ ë¹ ë¥´ê²Œ ì±„ìš°ëŠ” ê²½í–¥ì´ ìžˆìœ¼ë©°, ëŒ€ë¶€ë¶„ì˜ ì˜¤ë¥˜ëŠ” ë‹¨ìˆœí•œ "LLMì˜ ì‹¤ìˆ˜" ìœ í˜•ìœ¼ë¡œ, LLMì´ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ë” ë‚˜ì€ ì½”ë“œë‚˜ ë„êµ¬ í˜¸ì¶œì„ ìž‘ì„±í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ êµì •í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ ë‚˜ì¤‘ì— ê²€ì‚¬í•˜ê³  ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìžˆë„ë¡ ê³„ì¸¡ì„ í†µí•´ ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ê¸°ë¡í•˜ëŠ” ê²ƒì´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” í•„ìˆ˜ìž…ë‹ˆë‹¤!

ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ê³„ì¸¡í•˜ê¸° ìœ„í•´ [OpenTelemetry](https://opentelemetry.io/) í‘œì¤€ì„ ë„ìž…í–ˆìŠµë‹ˆë‹¤.

ì¦‰, ê³„ì¸¡ ì½”ë“œë¥¼ ì‹¤í–‰í•œ í›„ ì—ì´ì „íŠ¸ë¥¼ í‰ì†Œì²˜ëŸ¼ ì‹¤í–‰í•˜ë©´ ëª¨ë“  ë‚´ìš©ì´ í”Œëž«í¼ì— ìžë™ìœ¼ë¡œ ë¡œê¹…ë©ë‹ˆë‹¤. ë‹¤ì–‘í•œ OpenTelemetry ë°±ì—”ë“œì—ì„œ ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì˜ ì˜ˆì‹œë¥¼ ì•„ëž˜ì— ì œì‹œí•©ë‹ˆë‹¤.

í”Œëž«í¼ì—ì„œì˜ ì‹¤ì œ ëª¨ìŠµì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/inspect_run_phoenix.gif"/>
</div>

## Arize AI Phoenixë¡œ í…”ë ˆë©”íŠ¸ë¦¬ ì„¤ì •[[setting-up-telemetry-with-arize-ai-phoenix]]

ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë¡œê·¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ê²€ì‚¬í•˜ê¸°ì— ì¢‹ì€ ì†”ë£¨ì…˜ì¸ [Arize AIì˜ Phoenix](https://github.com/Arize-ai/phoenix)ë¥¼ ì„¤ì¹˜í•˜ì§€ë§Œ, ì´ ê³¼ì •ì—ëŠ” ë‹¤ë¥¸ OpenTelemetry í˜¸í™˜ í”Œëž«í¼ì„ í™œìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

```shell
pip install 'smolagents[telemetry,toolkit]'
```

ë‹¤ìŒ ë‹¨ê³„ë¡œ ìˆ˜ì§‘ê¸°ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•©ë‹ˆë‹¤.

```shell
python -m phoenix.server.main serve
```

ë§ˆì§€ë§‰ìœ¼ë¡œ `SmolagentsInstrumentor`ë¥¼ ì„¤ì •í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì¶”ì í•˜ê³  Phoenix ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ë¡œ í•´ë‹¹ ì¶”ì  ë°ì´í„°ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.

```python
from phoenix.otel import register
from openinference.instrumentation.smolagents import SmolagentsInstrumentor

register()
SmolagentsInstrumentor().instrument()
```
ì´ì œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!

```py
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    VisitWebpageTool,
    InferenceClientModel,
)

model = InferenceClientModel()

search_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="search_agent",
    description="This is an agent that can do web search.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[search_agent],
)
manager_agent.run(
    "If the US keeps its 2024 growth rate, how many years will it take for the GDP to double?"
)
```
ëìž…ë‹ˆë‹¤!
ì´ì œ `http://0.0.0.0:6006/projects/`ë¡œ ì´ë™í•˜ì—¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤!

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/inspect_run_phoenix.png">

CodeAgentê°€ ê´€ë¦¬í•˜ëŠ” ToolCallingAgentë¥¼ í˜¸ì¶œí•˜ì—¬(ì°¸ê³ ë¡œ ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ëŠ” CodeAgentê°€ ë  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤) ë¯¸êµ­ 2024ë…„ ì„±ìž¥ë¥ ì„ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ë„ë¡ ìš”ì²­í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì´í›„ ê´€ë¦¬ë˜ëŠ” ì—ì´ì „íŠ¸ê°€ ê²°ê³¼ë¥¼ ë³´ê³ í•˜ë©´, ê´€ë¦¬ìž ì—ì´ì „íŠ¸ê°€ ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê²½ì œ ë°°ì¦ ì‹œê°„ì„ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤! í¥ë¯¸ë¡­ì£ ?

## ðŸª¢ Langfuseë¡œ í…”ë ˆë©”íŠ¸ë¦¬ ì„¤ì •[[setting-up-telemetry-with-ðŸª¢-langfuse]]

ì´ ë¶€ë¶„ì€ `SmolagentsInstrumentor`ë¥¼ ì‚¬ìš©í•˜ì—¬ **Langfuse**ë¡œ Hugging Face **smolagents**ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

> **Langfuseëž€?** [Langfuse](https://langfuse.com)ëŠ” LLM ì—”ì§€ë‹ˆì–´ë§ì„ ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œëž«í¼ìž…ë‹ˆë‹¤. AI ì—ì´ì „íŠ¸ë¥¼ ìœ„í•œ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬ ê°œë°œìžê°€ ì œí’ˆì„ ë””ë²„ê¹…í•˜ê³ , ë¶„ì„í•˜ê³ , ìµœì í™”í•  ìˆ˜ ìžˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. LangfuseëŠ” ë„¤ì´í‹°ë¸Œ í†µí•©, OpenTelemetry, SDKë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë„êµ¬ì™€ í”„ë ˆìž„ì›Œí¬ì™€ í†µí•©ë©ë‹ˆë‹¤.

### 1ë‹¨ê³„: ì˜ì¡´ì„± ì„¤ì¹˜[[step-1:-install-dependencies]]

```python
%pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents
```

### 2ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •[[step-2:-set-up-environment-variables]]

Langfuse API í‚¤ë¥¼ ì„¤ì •í•˜ê³  Langfuseë¡œ ì¶”ì ì„ ë³´ë‚´ë„ë¡ OpenTelemetry ì—”ë“œí¬ì¸íŠ¸ë¥¼ êµ¬ì„±í•˜ì„¸ìš”. [Langfuse Cloud](https://cloud.langfuse.com)ì— ê°€ìž…í•˜ê±°ë‚˜ [Langfuseë¥¼ ìžì²´ í˜¸ìŠ¤íŒ…](https://langfuse.com/self-hosting)í•˜ì—¬ Langfuse API í‚¤ë¥¼ ì–»ìœ¼ì„¸ìš”.

ë˜í•œ [Hugging Face í† í°](https://huggingface.co/settings/tokens) (`HF_TOKEN`)ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ì¶”ê°€í•˜ì„¸ìš”.

```python
import os
# í”„ë¡œì íŠ¸ ì„¤ì • íŽ˜ì´ì§€(https://cloud.langfuse.com)ì—ì„œ í”„ë¡œì íŠ¸ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. 
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..." 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # ðŸ‡ªðŸ‡º ìœ ëŸ½ ì§€ì—­
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ðŸ‡ºðŸ‡¸ ë¯¸êµ­ ì§€ì—­
 
# Hugging Face í† í°ì„ ìž…ë ¥í•©ë‹ˆë‹¤.
os.environ["HF_TOKEN"] = "hf_..."
```

í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ë©´ ì´ì œ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. `get_client()`ëŠ” í™˜ê²½ ë³€ìˆ˜ì— ì œê³µëœ ìžê²© ì¦ëª…ì„ ì‚¬ìš©í•˜ì—¬ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

```python
from langfuse import get_client
 
langfuse = get_client()
 
# ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤.
if langfuse.auth_check():
    print("Langfuse client is authenticated and ready!")
else:
    print("Authentication failed. Please check your credentials and host.")
```

### 3ë‹¨ê³„: `SmolagentsInstrumentor` ì´ˆê¸°í™”[[step-3:-initialize-the-`smolagentsinstrumentor`]]

ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— `SmolagentsInstrumentor`ë¥¼ ì´ˆê¸°í™”í•˜ì„¸ìš”.

```python
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 
SmolagentsInstrumentor().instrument()
```

### 4ë‹¨ê³„: smolagent ì‹¤í–‰[[step-4:-run-your-smolagent]]

```python
from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    WebSearchTool,
    VisitWebpageTool,
    InferenceClientModel,
)

model = InferenceClientModel(
    model_id="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
)

search_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="search_agent",
    description="This is an agent that can do web search.",
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[search_agent],
)
manager_agent.run(
    "How can Langfuse be used to monitor and improve the reasoning and decision-making of smolagents when they execute multi-step tasks, like dynamically adjusting a recipe based on user feedback or available ingredients?"
)
```

### 5ë‹¨ê³„: Langfuseì—ì„œ ì¶”ì  ë³´ê¸°[[step-5:-view-traces-in-langfuse]]

ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•œ í›„, Langfuseì˜ smolagents ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ìƒì„±ëœ ì¶”ì  ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. AI ì—ì´ì „íŠ¸ì˜ ë””ë²„ê¹…ê³¼ ìµœì í™”ì— ë„ì›€ì´ ë˜ëŠ” LLM ìƒí˜¸ìž‘ìš©ì˜ ìƒì„¸í•œ ì„¸ë¶€ ê³¼ì •ì„ ì‚´íŽ´ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

![smolagents example trace](https://langfuse.com/images/cookbook/integration-smolagents/smolagent_example_trace.png)

_[Langfuseì˜ ì¶”ì  ì˜ˆì‹œ](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/ce5160f9bfd5a6cd63b07d2bfcec6f54?timestamp=2025-02-11T09%3A25%3A45.163Z&display=details)_



================================================
FILE: docs/source/ko/tutorials/memory.md
================================================
# ðŸ“š ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ê´€ë¦¬[[-manage-your-agents-memory]]

[[open-in-colab]]

ê²°êµ­ ì—ì´ì „íŠ¸ëŠ” ë„êµ¬ì™€ í”„ë¡¬í”„íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ë‹¨ìˆœí•œ êµ¬ì„±ìš”ì†Œë¡œ ì •ì˜ë©ë‹ˆë‹¤.
ê·¸ë¦¬ê³  ë¬´ì—‡ë³´ë‹¤ ì¤‘ìš”í•œ ê²ƒì€ ì—ì´ì „íŠ¸ê°€ ê³¼ê±° ë‹¨ê³„ì˜ ë©”ëª¨ë¦¬ë¥¼ ê°€ì§€ê³  ìžˆì–´ ê³„íš, ì‹¤í–‰, ì˜¤ë¥˜ì˜ ì´ë ¥ì„ ì¶”ì í•œë‹¤ëŠ” ì ìž…ë‹ˆë‹¤.

### ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ìž¬ìƒ[[replay-your-agents-memory]]

ê³¼ê±° ì‹¤í–‰ëœ ì—ì´ì „íŠ¸ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

[ê³„ì¸¡ ê°€ì´ë“œ](./inspect_runs)ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´, ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ê³„ì¸¡í•˜ì—¬ íŠ¹ì • ë‹¨ê³„ë¥¼ í™•ëŒ€í•˜ê±°ë‚˜ ì¶•ì†Œí•  ìˆ˜ ìžˆëŠ” ìš°ìˆ˜í•œ UIë¡œ ì‹œê°í™”í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë˜í•œ ë‹¤ìŒê³¼ ê°™ì´ `agent.replay()`ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•œ í›„,
```py
from smolagents import InferenceClientModel, CodeAgent

agent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=0)

result = agent.run("What's the 20th Fibonacci number?")
```

ì´ ë§ˆì§€ë§‰ ì‹¤í–‰ì„ ë‹¤ì‹œ ìž¬ìƒí•˜ê³  ì‹¶ë‹¤ë©´, ë‹¤ìŒ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.
```py
agent.replay()
```

### ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ë™ì  ë³€ê²½[[dynamically-change-the-agents-memory]]

ë§Žì€ ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ì—ì„œëŠ” ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì ‘ê·¼í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.


```py
from smolagents import ActionStep

system_prompt_step = agent.memory.system_prompt
print("The system prompt given to the agent was:")
print(system_prompt_step.system_prompt)

task_step = agent.memory.steps[0]
print("\n\nThe first task step was:")
print(task_step.task)

for step in agent.memory.steps:
    if isinstance(step, ActionStep):
        if step.error is not None:
            print(f"\nStep {step.step_number} got this error:\n{step.error}\n")
        else:
            print(f"\nStep {step.step_number} got these observations:\n{step.observations}\n")
```

`agent.memory.get_full_steps()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë‹¨ê³„ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ë˜í•œ ë‹¨ê³„ ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.

ë‹¨ê³„ ì½œë°±ì€ ì¸ìžë¡œ `agent` ê°ì²´ ìžì²´ì— ì ‘ê·¼í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, ìœ„ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ëª¨ë“  ë©”ëª¨ë¦¬ ë‹¨ê³„ì— ì ‘ê·¼í•˜ì—¬ í•„ìš”í•œ ê²½ìš° ìˆ˜ì •í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì›¹ ë¸Œë¼ìš°ì € ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•˜ëŠ” ê° ë‹¨ê³„ì˜ ìŠ¤í¬ë¦°ìƒ·ì„ ê´€ì°°í•˜ê³  ìžˆë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ ê²½ìš° ìµœì‹  ìŠ¤í¬ë¦°ìƒ·ì€ ìœ ì§€í•˜ë©´ì„œ í† í° ë¹„ìš©ì„ ì ˆì•½í•˜ê¸° ìœ„í•´ ì´ì „ ë‹¨ê³„ì˜ ì´ë¯¸ì§€ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

ì´ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ì½”ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
_ì£¼ì˜: ì´ ì½”ë“œëŠ” ê°„ê²°í•¨ì„ ìœ„í•´ ì¼ë¶€ ìž„í¬íŠ¸ ë° ê°ì²´ ì •ì˜ê°€ ìƒëžµëœ ë¶ˆì™„ì „í•œ ì˜ˆì‹œìž…ë‹ˆë‹¤. ì „ì²´ ìž‘ë™ ë²„ì „ì˜ ì½”ë“œëŠ” [ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py)ì—ì„œ í™•ì¸í•˜ì„¸ìš”._

```py
import helium
from PIL import Image
from io import BytesIO
from time import sleep

def update_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # JavaScript ì• ë‹ˆë©”ì´ì…˜ì´ ì™„ë£Œëœ í›„ì— ìŠ¤í¬ë¦°ìƒ·ì„ ì°ë„ë¡ í•©ë‹ˆë‹¤.
    driver = helium.get_driver()
    latest_step = memory_step.step_number
    for previous_memory_step in agent.memory.steps:  # ì´ì „ ìŠ¤í¬ë¦°ìƒ·ì„ ë¡œê·¸ì—ì„œ ì œê±°í•˜ì—¬ ì²˜ë¦¬ ê³¼ì •ì„ ê°„ì†Œí™”í•©ë‹ˆë‹¤.
        if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= latest_step - 2:
            previous_memory_step.observations_images = None
    png_bytes = driver.get_screenshot_as_png()
    image = Image.open(BytesIO(png_bytes))
    memory_step.observations_images = [image.copy()]
```

ê·¸ ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ `step_callbacks` ì¸ìˆ˜ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

```py
CodeAgent(
    tools=[WebSearchTool(), go_back, close_popups, search_item_ctrl_f],
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[update_screenshot],
    max_steps=20,
    verbosity_level=2,
)
```

ì „ì²´ ìž‘ë™ ì˜ˆì‹œëŠ” [ë¹„ì „ ì›¹ ë¸Œë¼ìš°ì € ì½”ë“œ](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py)ì—ì„œ í™•ì¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

### ì—ì´ì „íŠ¸ë¥¼ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰[[run-agents-one-step-at-a-time]]

ì´ ê¸°ëŠ¥ì€ ë„êµ¬ í˜¸ì¶œì— ì˜¤ëžœ ì‹œê°„ì´ ê±¸ë¦¬ëŠ” ê²½ìš°ì— ìœ ìš©í•©ë‹ˆë‹¤.
ì—ì´ì „íŠ¸ë¥¼ í•œ ë‹¨ê³„ì”© ì‹¤í–‰í•˜ë©´ì„œ ê° ë‹¨ê³„ì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

```py
from smolagents import InferenceClientModel, CodeAgent, ActionStep, TaskStep

agent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=1)
agent.python_executor.send_tools({**agent.tools})
print(agent.memory.system_prompt)

task = "What is the 20th Fibonacci number?"

# í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ë¶ˆëŸ¬ì™€ ë©”ëª¨ë¦¬ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
# agent.memory.steps = previous_agent.memory.steps

# ìƒˆë¡œìš´ ìž‘ì—…ì„ ì‹œìž‘í•©ë‹ˆë‹¤!
agent.memory.steps.append(TaskStep(task=task, task_images=[]))

final_answer = None
step_number = 1
while final_answer is None and step_number <= 10:
    memory_step = ActionStep(
        step_number=step_number,
        observations_images=[],
    )
    # í•œ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    final_answer = agent.step(memory_step)
    agent.memory.steps.append(memory_step)
    step_number += 1

    # í•„ìš”í•œ ê²½ìš° ë©”ëª¨ë¦¬ë¥¼ ìˆ˜ì •í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤
    # ì˜ˆë¥¼ ë“¤ì–´ ìµœì‹  ë‹¨ê³„ë¥¼ ì—…ë°ì´íŠ¸ í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì²˜ë¦¬í•©ë‹ˆë‹¤:
    # agent.memory.steps[-1] = ...

print("The final answer is:", final_answer)
```



================================================
FILE: docs/source/zh/_config.py
================================================
# docstyle-ignore
INSTALL_CONTENT = """
# Installation
! pip install smolagents
# To install from source instead of the last release, comment the command above and uncomment the following one.
# ! pip install git+https://github.com/huggingface/smolagents.git
"""

notebook_first_cells = [{"type": "code", "content": INSTALL_CONTENT}]
black_avoid_patterns = {
    "{processor_class}": "FakeProcessorClass",
    "{model_class}": "FakeModelClass",
    "{object_class}": "FakeObjectClass",
}



================================================
FILE: docs/source/zh/_toctree.yml
================================================
[Binary file]


================================================
FILE: docs/source/zh/guided_tour.md
================================================
# Agents - å¯¼è§ˆ

[[open-in-colab]]

åœ¨æœ¬å¯¼è§ˆä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•æž„å»ºä¸€ä¸ª agentï¼ˆæ™ºèƒ½ä½“ï¼‰ï¼Œå¦‚ä½•è¿è¡Œå®ƒï¼Œä»¥åŠå¦‚ä½•è‡ªå®šä¹‰å®ƒä»¥ä½¿å…¶æ›´å¥½åœ°é€‚åº”æ‚¨çš„ä½¿ç”¨åœºæ™¯ã€‚

> [!TIP]
> è¯‘è€…æ³¨ï¼šAgent çš„ä¸šå†…æœ¯è¯­æ˜¯â€œæ™ºèƒ½ä½“â€ã€‚æœ¬è¯‘æ–‡å°†ä¿ç•™ agentï¼Œä¸ä½œç¿»è¯‘ï¼Œä»¥å¸¦æ¥æ›´é«˜æ•ˆçš„é˜…è¯»ä½“éªŒã€‚(åœ¨ä¸­æ–‡ä¸ºä¸»çš„æ–‡ç« ä¸­ï¼ŒIt's easier to æ³¨æ„åˆ°è‹±æ–‡ã€‚Attention Is All You Need!)

> [!TIP]
> ä¸­æ–‡ç¤¾åŒºå‘å¸ƒäº†å…³äºŽ smolagents çš„ä»‹ç»å’Œå®žè·µè®²è§£è§†é¢‘(æ¥æºï¼š[Issue#80](https://github.com/huggingface/smolagents/issues/80))ï¼Œä½ å¯ä»¥è®¿é—®[è¿™é‡Œ](https://www.youtube.com/watch?v=wwN3oAugc4c)è¿›è¡Œè§‚çœ‹ï¼

### æž„å»ºæ‚¨çš„ agent

è¦åˆå§‹åŒ–ä¸€ä¸ªæœ€å°åŒ–çš„ agentï¼Œæ‚¨è‡³å°‘éœ€è¦ä»¥ä¸‹ä¸¤ä¸ªå‚æ•°ï¼š

- `model`ï¼Œä¸€ä¸ªä¸ºæ‚¨çš„ agent æä¾›åŠ¨åŠ›çš„æ–‡æœ¬ç”Ÿæˆæ¨¡åž‹ - å› ä¸º agent ä¸Žç®€å•çš„ LLM ä¸åŒï¼Œå®ƒæ˜¯ä¸€ä¸ªä½¿ç”¨ LLM ä½œä¸ºå¼•æ“Žçš„ç³»ç»Ÿã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€é€‰é¡¹ï¼š
    - [`TransformersModel`] ä½¿ç”¨é¢„åˆå§‹åŒ–çš„ `transformers` ç®¡é“åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡ŒæŽ¨ç†
    - [`InferenceClientModel`] åœ¨åº•å±‚ä½¿ç”¨ `huggingface_hub.InferenceClient`
    - [`LiteLLMModel`] è®©æ‚¨é€šè¿‡ [LiteLLM](https://docs.litellm.ai/) è°ƒç”¨ 100+ ä¸åŒçš„æ¨¡åž‹ï¼
    - [`AzureOpenAIModel`] å…è®¸æ‚¨ä½¿ç”¨éƒ¨ç½²åœ¨ [Azure](https://azure.microsoft.com/en-us/products/ai-services/openai-service) ä¸­çš„ OpenAI æ¨¡åž‹ã€‚
    - [`MLXModel`] å¯åˆ›å»º [mlx-lm](https://pypi.org/project/mlx-lm/) æµæ°´çº¿ï¼Œä»¥ä¾¿åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡ŒæŽ¨ç†ã€‚

- `tools`ï¼Œagent å¯ä»¥ç”¨æ¥è§£å†³ä»»åŠ¡çš„ `Tools` åˆ—è¡¨ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªç©ºåˆ—è¡¨ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡å®šä¹‰å¯é€‰å‚æ•° `add_base_tools=True` åœ¨æ‚¨çš„ `tools` åˆ—è¡¨ä¹‹ä¸Šæ·»åŠ é»˜è®¤å·¥å…·ç®±ã€‚

ä¸€æ—¦æœ‰äº†è¿™ä¸¤ä¸ªå‚æ•° `tools` å’Œ `model`ï¼Œæ‚¨å°±å¯ä»¥åˆ›å»ºä¸€ä¸ª agent å¹¶è¿è¡Œå®ƒã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•æ‚¨å–œæ¬¢çš„ LLMï¼Œæ— è®ºæ˜¯é€šè¿‡ [Hugging Face API](https://huggingface.co/docs/api-inference/en/index)ã€[transformers](https://github.com/huggingface/transformers/)ã€[ollama](https://ollama.com/)ã€[LiteLLM](https://www.litellm.ai/)ã€[Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)ï¼Œè¿˜æ˜¯[mlx-lm](https://pypi.org/project/mlx-lm/).ã€‚

<hfoptions id="é€‰æ‹©ä¸€ä¸ªLLM">
<hfoption id="Hugging Face API">

Hugging Face API å¯ä»¥å…è´¹ä½¿ç”¨è€Œæ— éœ€ tokenï¼Œä½†ä¼šæœ‰é€ŸçŽ‡é™åˆ¶ã€‚

è¦è®¿é—®å—é™æ¨¡åž‹æˆ–ä½¿ç”¨ PRO è´¦æˆ·æé«˜é€ŸçŽ‡é™åˆ¶ï¼Œæ‚¨éœ€è¦è®¾ç½®çŽ¯å¢ƒå˜é‡ `HF_TOKEN` æˆ–åœ¨åˆå§‹åŒ– `InferenceClientModel` æ—¶ä¼ é€’ `token` å˜é‡ã€‚

```python
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

model = InferenceClientModel(model_id=model_id, token="<YOUR_HUGGINGFACEHUB_API_TOKEN>")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="æœ¬åœ°Transformersæ¨¡åž‹">

```python
# !pip install 'smolagents[transformers]'
from smolagents import CodeAgent, TransformersModel

model_id = "meta-llama/Llama-3.2-3B-Instruct"

model = TransformersModel(model_id=model_id)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="OpenAIæˆ–Anthropic API">

è¦ä½¿ç”¨ `LiteLLMModel`ï¼Œæ‚¨éœ€è¦è®¾ç½®çŽ¯å¢ƒå˜é‡ `ANTHROPIC_API_KEY` æˆ– `OPENAI_API_KEY`ï¼Œæˆ–è€…åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’ `api_key` å˜é‡ã€‚

```python
# !pip install 'smolagents[litellm]'
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(model_id="anthropic/claude-3-5-sonnet-latest", api_key="YOUR_ANTHROPIC_API_KEY") # ä¹Ÿå¯ä»¥ä½¿ç”¨ 'gpt-4o'
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Ollama">

```python
# !pip install 'smolagents[litellm]'
from smolagents import CodeAgent, LiteLLMModel

model = LiteLLMModel(
    model_id="ollama_chat/llama3.2", # è¿™ä¸ªæ¨¡åž‹å¯¹äºŽ agent è¡Œä¸ºæ¥è¯´æœ‰ç‚¹å¼±
    api_base="http://localhost:11434", # å¦‚æžœéœ€è¦å¯ä»¥æ›¿æ¢ä¸ºè¿œç¨‹ open-ai å…¼å®¹æœåŠ¡å™¨
    api_key="YOUR_API_KEY" # å¦‚æžœéœ€è¦å¯ä»¥æ›¿æ¢ä¸º API key
    num_ctx=8192 # https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator
)

agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```
</hfoption>
<hfoption id="Azure OpenAI">

è¦è¿žæŽ¥åˆ° Azure OpenAIï¼Œæ‚¨å¯ä»¥ç›´æŽ¥ä½¿ç”¨ `AzureOpenAIModel`ï¼Œæˆ–ä½¿ç”¨ `LiteLLMModel` å¹¶è¿›è¡Œç›¸åº”é…ç½®ã€‚

åˆå§‹åŒ– `AzureOpenAIModel` å®žä¾‹æ—¶ï¼Œéœ€è¦ä¼ é€’æ¨¡åž‹éƒ¨ç½²åç§°ï¼Œå¯é€‰æ‹©ä»¥ä¸‹ä»»ä¸€ç§æ–¹å¼ï¼š1.ä¼ é€’ `azure_endpoint`ã€`api_key` å’Œ `api_version` å‚æ•°ï¼›2.è®¾ç½®çŽ¯å¢ƒå˜é‡ `AZURE_OPENAI_ENDPOINT`ã€`AZURE_OPENAI_API_KEY` å’Œ `OPENAI_API_VERSION`

```python
# !pip install 'smolagents[openai]'
from smolagents import CodeAgent, AzureOpenAIModel

model = AzureOpenAIModel(model_id="gpt-4o-mini")
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
    "Could you give me the 118th number in the Fibonacci sequence?",
)
```

ä¹Ÿå¯æŒ‰å¦‚ä¸‹æ–¹å¼é…ç½® `LiteLLMModel` è¿žæŽ¥ Azure OpenAIï¼š

- å°†æ¨¡åž‹éƒ¨ç½²åç§°ä½œä¸º `model_id` å‚æ•°ä¼ é€’ï¼Œå¹¶ç¡®ä¿å…¶å‰ç¼€ä¸º `azure/`
- ç¡®ä¿è®¾ç½®çŽ¯å¢ƒå˜é‡ `AZURE_API_VERSION`
- ä»»é€‰å…¶ä¸€ï¼š1.ä¼ é€’ `api_base` å’Œ `api_key` å‚æ•°ï¼›2.è®¾ç½®çŽ¯å¢ƒå˜é‡ `AZURE_API_KEY` å’Œ `AZURE_API_BASE`

```python
import os
from smolagents import CodeAgent, LiteLLMModel

AZURE_OPENAI_CHAT_DEPLOYMENT_NAME="gpt-35-turbo-16k-deployment" # example of deployment name

os.environ["AZURE_API_KEY"] = "" # api_key
os.environ["AZURE_API_BASE"] = "" # "https://example-endpoint.openai.azure.com"
os.environ["AZURE_API_VERSION"] = "" # "2024-10-01-preview"

model = LiteLLMModel(model_id="azure/" + AZURE_OPENAI_CHAT_DEPLOYMENT_NAME)
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

agent.run(
   "Could you give me the 118th number in the Fibonacci sequence?",
)
```

</hfoption>
<hfoption id="mlx-lm">

```python
# !pip install 'smolagents[mlx-lm]'
from smolagents import CodeAgent, MLXModel

mlx_model = MLXModel("mlx-community/Qwen2.5-Coder-32B-Instruct-4bit")
agent = CodeAgent(model=mlx_model, tools=[], add_base_tools=True)

agent.run("Could you give me the 118th number in the Fibonacci sequence?")
```

</hfoption>
</hfoptions>

#### CodeAgent å’Œ ToolCallingAgent

[`CodeAgent`] æ˜¯æˆ‘ä»¬çš„é»˜è®¤ agentã€‚å®ƒå°†åœ¨æ¯ä¸€æ­¥ç¼–å†™å¹¶æ‰§è¡Œ Python ä»£ç ç‰‡æ®µã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰§è¡Œæ˜¯åœ¨æ‚¨çš„æœ¬åœ°çŽ¯å¢ƒä¸­å®Œæˆçš„ã€‚
è¿™åº”è¯¥æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºå”¯ä¸€å¯ä»¥è°ƒç”¨çš„å‡½æ•°æ˜¯æ‚¨æä¾›çš„å·¥å…·ï¼ˆç‰¹åˆ«æ˜¯å¦‚æžœåªæœ‰ Hugging Face çš„å·¥å…·ï¼‰å’Œä¸€ç»„é¢„å®šä¹‰çš„å®‰å…¨å‡½æ•°ï¼Œå¦‚ `print` æˆ– `math` æ¨¡å—ä¸­çš„å‡½æ•°ï¼Œæ‰€ä»¥æ‚¨å·²ç»é™åˆ¶äº†å¯ä»¥æ‰§è¡Œçš„å†…å®¹ã€‚

Python è§£é‡Šå™¨é»˜è®¤ä¹Ÿä¸å…è®¸åœ¨å®‰å…¨åˆ—è¡¨ä¹‹å¤–å¯¼å…¥ï¼Œæ‰€ä»¥æ‰€æœ‰æœ€æ˜Žæ˜¾çš„æ”»å‡»éƒ½ä¸åº”è¯¥æˆä¸ºé—®é¢˜ã€‚
æ‚¨å¯ä»¥é€šè¿‡åœ¨åˆå§‹åŒ– [`CodeAgent`] æ—¶å°†æŽˆæƒæ¨¡å—ä½œä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™å‚æ•° `additional_authorized_imports` æ¥æŽˆæƒé¢å¤–çš„å¯¼å…¥ï¼š

```py
from smolagents import CodeAgent

model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, additional_authorized_imports=['requests', 'bs4'])
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

> [!WARNING]
> LLM å¯ä»¥ç”Ÿæˆä»»æ„ä»£ç ç„¶åŽæ‰§è¡Œï¼šä¸è¦æ·»åŠ ä»»ä½•ä¸å®‰å…¨çš„å¯¼å…¥ï¼

å¦‚æžœç”Ÿæˆçš„ä»£ç å°è¯•æ‰§è¡Œéžæ³•æ“ä½œæˆ–å‡ºçŽ°å¸¸è§„ Python é”™è¯¯ï¼Œæ‰§è¡Œå°†åœæ­¢ã€‚

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ [E2B ä»£ç æ‰§è¡Œå™¨](https://e2b.dev/docs#what-is-e2-b) æˆ– Docker è€Œä¸æ˜¯æœ¬åœ° Python è§£é‡Šå™¨ã€‚å¯¹äºŽ E2Bï¼Œé¦–å…ˆ [è®¾ç½® `E2B_API_KEY` çŽ¯å¢ƒå˜é‡](https://e2b.dev/dashboard?tab=keys)ï¼Œç„¶åŽåœ¨åˆå§‹åŒ– agent æ—¶ä¼ é€’ `executor_type="e2b"`ã€‚å¯¹äºŽ Dockerï¼Œåœ¨åˆå§‹åŒ–æ—¶ä¼ é€’ `executor_type="docker"`ã€‚

> [!TIP]
> åœ¨ [è¯¥æ•™ç¨‹ä¸­](tutorials/secure_code_execution) äº†è§£æ›´å¤šå…³äºŽä»£ç æ‰§è¡Œçš„å†…å®¹ã€‚

æˆ‘ä»¬è¿˜æ”¯æŒå¹¿æ³›ä½¿ç”¨çš„å°†åŠ¨ä½œç¼–å†™ä¸º JSON-like å—çš„æ–¹å¼ï¼š[`ToolCallingAgent`]ï¼Œå®ƒçš„å·¥ä½œæ–¹å¼ä¸Ž [`CodeAgent`] éžå¸¸ç›¸ä¼¼ï¼Œå½“ç„¶æ²¡æœ‰ `additional_authorized_imports`ï¼Œå› ä¸ºå®ƒä¸æ‰§è¡Œä»£ç ï¼š

```py
from smolagents import ToolCallingAgent, WebSearchTool

agent = ToolCallingAgent(tools=[WebSearchTool()], model=model)
agent.run("Could you get me the title of the page at url 'https://huggingface.co/blog'?")
```

### æ£€æŸ¥ agent è¿è¡Œ

ä»¥ä¸‹æ˜¯ä¸€äº›æœ‰ç”¨çš„å±žæ€§ï¼Œç”¨äºŽæ£€æŸ¥è¿è¡ŒåŽå‘ç”Ÿäº†ä»€ä¹ˆï¼š
- `agent.logs` å­˜å‚¨ agent çš„ç»†ç²’åº¦æ—¥å¿—ã€‚åœ¨ agent è¿è¡Œçš„æ¯ä¸€æ­¥ï¼Œæ‰€æœ‰å†…å®¹éƒ½ä¼šå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œç„¶åŽé™„åŠ åˆ° `agent.logs` ä¸­ã€‚
- è¿è¡Œ `agent.write_memory_to_messages()` ä¼šä¸º LLM åˆ›å»ºä¸€ä¸ª agent æ—¥å¿—çš„å†…éƒ¨å†…å­˜ï¼Œä½œä¸ºèŠå¤©æ¶ˆæ¯åˆ—è¡¨ã€‚æ­¤æ–¹æ³•ä¼šéåŽ†æ—¥å¿—çš„æ¯ä¸€æ­¥ï¼Œå¹¶ä»…å­˜å‚¨å®ƒæ„Ÿå…´è¶£çš„å†…å®¹ä½œä¸ºæ¶ˆæ¯ï¼šä¾‹å¦‚ï¼Œå®ƒä¼šå°†ç³»ç»Ÿæç¤ºå’Œä»»åŠ¡å­˜å‚¨ä¸ºå•ç‹¬çš„æ¶ˆæ¯ï¼Œç„¶åŽå¯¹äºŽæ¯ä¸€æ­¥ï¼Œå®ƒä¼šå°† LLM è¾“å‡ºå­˜å‚¨ä¸ºä¸€æ¡æ¶ˆæ¯ï¼Œå·¥å…·è°ƒç”¨è¾“å‡ºå­˜å‚¨ä¸ºå¦ä¸€æ¡æ¶ˆæ¯ã€‚å¦‚æžœæ‚¨æƒ³è¦æ›´é«˜çº§åˆ«çš„è§†å›¾ - ä½†ä¸æ˜¯æ¯ä¸ªæ—¥å¿—éƒ½ä¼šè¢«æ­¤æ–¹æ³•è½¬å½•ã€‚

## å·¥å…·

å·¥å…·æ˜¯ agent ä½¿ç”¨çš„åŽŸå­å‡½æ•°ã€‚ä¸ºäº†è¢« LLM ä½¿ç”¨ï¼Œå®ƒè¿˜éœ€è¦ä¸€äº›æž„æˆå…¶ API çš„å±žæ€§ï¼Œè¿™äº›å±žæ€§å°†ç”¨äºŽå‘ LLM æè¿°å¦‚ä½•è°ƒç”¨æ­¤å·¥å…·ï¼š
- åç§°
- æè¿°
- è¾“å…¥ç±»åž‹å’Œæè¿°
- è¾“å‡ºç±»åž‹

ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æŸ¥çœ‹ [`PythonInterpreterTool`]ï¼šå®ƒæœ‰ä¸€ä¸ªåç§°ã€æè¿°ã€è¾“å…¥æè¿°ã€è¾“å‡ºç±»åž‹å’Œä¸€ä¸ªæ‰§è¡Œæ“ä½œçš„ `forward` æ–¹æ³•ã€‚

å½“ agent åˆå§‹åŒ–æ—¶ï¼Œå·¥å…·å±žæ€§ç”¨äºŽç”Ÿæˆå·¥å…·æè¿°ï¼Œè¯¥æè¿°è¢«åµŒå…¥åˆ° agent çš„ç³»ç»Ÿæç¤ºä¸­ã€‚è¿™è®© agent çŸ¥é“å®ƒå¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·ä»¥åŠä¸ºä»€ä¹ˆã€‚

### é»˜è®¤å·¥å…·ç®±

`smolagents` é™„å¸¦äº†ä¸€ä¸ªç”¨äºŽå¢žå¼º agent çš„é»˜è®¤å·¥å…·ç®±ï¼Œæ‚¨å¯ä»¥åœ¨åˆå§‹åŒ–æ—¶é€šè¿‡å‚æ•° `add_base_tools=True` å°†å…¶æ·»åŠ åˆ°æ‚¨çš„ agent ä¸­ï¼š

- **DuckDuckGo ç½‘é¡µæœç´¢**ï¼šä½¿ç”¨ DuckDuckGo æµè§ˆå™¨æ‰§è¡Œç½‘é¡µæœç´¢ã€‚
- **Python ä»£ç è§£é‡Šå™¨**ï¼šåœ¨å®‰å…¨çŽ¯å¢ƒä¸­è¿è¡Œ LLM ç”Ÿæˆçš„ Python ä»£ç ã€‚åªæœ‰åœ¨ä½¿ç”¨ `add_base_tools=True` åˆå§‹åŒ– [`ToolCallingAgent`] æ—¶æ‰ä¼šæ·»åŠ æ­¤å·¥å…·ï¼Œå› ä¸ºåŸºäºŽä»£ç çš„ agent å·²ç»å¯ä»¥åŽŸç”Ÿæ‰§è¡Œ Python ä»£ç 
- **è½¬å½•å™¨**ï¼šåŸºäºŽ Whisper-Turbo æž„å»ºçš„è¯­éŸ³è½¬æ–‡æœ¬ç®¡é“ï¼Œå°†éŸ³é¢‘è½¬å½•ä¸ºæ–‡æœ¬ã€‚

æ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨ [`load_tool`] å‡½æ•°å’Œè¦æ‰§è¡Œçš„ä»»åŠ¡æ‰‹åŠ¨ä½¿ç”¨å·¥å…·ã€‚

```python
from smolagents import WebSearchTool

search_tool = WebSearchTool()
print(search_tool("Who's the current president of Russia?"))
```

### åˆ›å»ºä¸€ä¸ªæ–°å·¥å…·

æ‚¨å¯ä»¥åˆ›å»ºè‡ªå·±çš„å·¥å…·ï¼Œç”¨äºŽ Hugging Face é»˜è®¤å·¥å…·æœªæ¶µç›–çš„ç”¨ä¾‹ã€‚
ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œè¿”å›ž Hub ä¸Šç»™å®šä»»åŠ¡ä¸‹è½½é‡æœ€å¤šçš„æ¨¡åž‹ã€‚

æ‚¨å°†ä»Žä»¥ä¸‹ä»£ç å¼€å§‹ã€‚

```python
from huggingface_hub import list_models

task = "text-classification"

most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
print(most_downloaded_model.id)
```

è¿™æ®µä»£ç å¯ä»¥é€šè¿‡å°†å…¶åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­å¹¶æ·»åŠ  `tool` è£…é¥°å™¨å¿«é€Ÿè½¬æ¢ä¸ºå·¥å…·ï¼š
è¿™ä¸æ˜¯æž„å»ºå·¥å…·çš„å”¯ä¸€æ–¹æ³•ï¼šæ‚¨å¯ä»¥ç›´æŽ¥å°†å…¶å®šä¹‰ä¸º [`Tool`] çš„å­ç±»ï¼Œè¿™ä¸ºæ‚¨æä¾›äº†æ›´å¤šçš„çµæ´»æ€§ï¼Œä¾‹å¦‚åˆå§‹åŒ–é‡åž‹ç±»å±žæ€§çš„å¯èƒ½æ€§ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸¤ç§é€‰é¡¹çš„å·¥ä½œåŽŸç†ï¼š

<hfoptions id="æž„å»ºå·¥å…·">
<hfoption id="ä½¿ç”¨@toolè£…é¥°ä¸€ä¸ªå‡½æ•°">

```py
from smolagents import tool

@tool
def model_download_tool(task: str) -> str:
    """
    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
    It returns the name of the checkpoint.

    Args:
        task: The task for which to get the download count.
    """
    most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
    return most_downloaded_model.id
```

è¯¥å‡½æ•°éœ€è¦ï¼š
- ä¸€ä¸ªæ¸…æ™°çš„åç§°ã€‚åç§°åº”è¯¥è¶³å¤Ÿæè¿°æ­¤å·¥å…·çš„åŠŸèƒ½ï¼Œä»¥å¸®åŠ©ä¸º agent æä¾›åŠ¨åŠ›çš„ LLMã€‚ç”±äºŽæ­¤å·¥å…·è¿”å›žä»»åŠ¡ä¸‹è½½é‡æœ€å¤šçš„æ¨¡åž‹ï¼Œæˆ‘ä»¬å°†å…¶å‘½åä¸º `model_download_tool`ã€‚
- è¾“å…¥å’Œè¾“å‡ºçš„ç±»åž‹æç¤º
- ä¸€ä¸ªæè¿°ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ª 'Args:' éƒ¨åˆ†ï¼Œå…¶ä¸­æ¯ä¸ªå‚æ•°éƒ½è¢«æè¿°ï¼ˆè¿™æ¬¡æ²¡æœ‰ç±»åž‹æŒ‡ç¤ºï¼Œå®ƒå°†ä»Žç±»åž‹æç¤ºä¸­æå–ï¼‰ã€‚ä¸Žå·¥å…·åç§°ä¸€æ ·ï¼Œæ­¤æè¿°æ˜¯ä¸ºæ‚¨çš„ agent æä¾›åŠ¨åŠ›çš„ LLM çš„è¯´æ˜Žä¹¦ï¼Œæ‰€ä»¥ä¸è¦å¿½è§†å®ƒã€‚
æ‰€æœ‰è¿™äº›å…ƒç´ å°†åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨åµŒå…¥åˆ° agent çš„ç³»ç»Ÿæç¤ºä¸­ï¼šå› æ­¤è¦åŠªåŠ›ä½¿å®ƒä»¬å°½å¯èƒ½æ¸…æ™°ï¼

> [!TIP]
> æ­¤å®šä¹‰æ ¼å¼ä¸Ž `apply_chat_template` ä¸­ä½¿ç”¨çš„å·¥å…·æ¨¡å¼ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æ·»åŠ äº† `tool` è£…é¥°å™¨ï¼š[è¿™é‡Œ](https://huggingface.co/blog/unified-tool-use#passing-tools-to-a-chat-template) äº†è§£æ›´å¤šå…³äºŽæˆ‘ä»¬çš„å·¥å…·ä½¿ç”¨ APIã€‚


ç„¶åŽæ‚¨å¯ä»¥ç›´æŽ¥åˆå§‹åŒ–æ‚¨çš„ agentï¼š
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[model_download_tool], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
</hfoption>
<hfoption id="å­ç±»åŒ–Tool">

```py
from smolagents import Tool

class ModelDownloadTool(Tool):
    name = "model_download_tool"
    description = "This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. It returns the name of the checkpoint."
    inputs = {"task": {"type": "string", "description": "The task for which to get the download count."}}
    output_type = "string"

    def forward(self, task: str) -> str:
        most_downloaded_model = next(iter(list_models(filter=task, sort="downloads", direction=-1)))
        return most_downloaded_model.id
```

å­ç±»éœ€è¦ä»¥ä¸‹å±žæ€§ï¼š
- ä¸€ä¸ªæ¸…æ™°çš„ `name`ã€‚åç§°åº”è¯¥è¶³å¤Ÿæè¿°æ­¤å·¥å…·çš„åŠŸèƒ½ï¼Œä»¥å¸®åŠ©ä¸º agent æä¾›åŠ¨åŠ›çš„ LLMã€‚ç”±äºŽæ­¤å·¥å…·è¿”å›žä»»åŠ¡ä¸‹è½½é‡æœ€å¤šçš„æ¨¡åž‹ï¼Œæˆ‘ä»¬å°†å…¶å‘½åä¸º `model_download_tool`ã€‚
- ä¸€ä¸ª `description`ã€‚ä¸Ž `name` ä¸€æ ·ï¼Œæ­¤æè¿°æ˜¯ä¸ºæ‚¨çš„ agent æä¾›åŠ¨åŠ›çš„ LLM çš„è¯´æ˜Žä¹¦ï¼Œæ‰€ä»¥ä¸è¦å¿½è§†å®ƒã€‚
- è¾“å…¥ç±»åž‹å’Œæè¿°
- è¾“å‡ºç±»åž‹


ç„¶åŽæ‚¨å¯ä»¥ç›´æŽ¥åˆå§‹åŒ–æ‚¨çš„ agentï¼š
```py
from smolagents import CodeAgent, InferenceClientModel
agent = CodeAgent(tools=[ModelDownloadTool()], model=InferenceClientModel())
agent.run(
    "Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?"
)
```
æ‰€æœ‰è¿™äº›å±žæ€§å°†åœ¨åˆå§‹åŒ–æ—¶è‡ªåŠ¨åµŒå…¥åˆ° agent çš„ç³»ç»Ÿæç¤ºä¸­ï¼šå› æ­¤è¦åŠªåŠ›ä½¿å®ƒä»¬å°½å¯èƒ½æ¸…æ™°ï¼
</hfoption>
</hfoptions>

æ‚¨å°†èŽ·å¾—ä»¥ä¸‹æ—¥å¿—ï¼š
```text
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                          â”‚
â”‚ Can you give me the name of the model that has the most downloads in the 'text-to-video' â”‚
â”‚ task on the Hugging Face Hub?                                                            â”‚
â”‚                                                                                          â”‚
â•°â”€ InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 0 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 model_name = model_download_tool(task="text-to-video")                               â”‚
â”‚   2 print(model_name)                                                                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Execution logs:
ByteDance/AnimateDiff-Lightning

Out: None
[Step 0: Duration 0.27 seconds| Input tokens: 2,069 | Output tokens: 60]
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â•­â”€ Executing this code: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   1 final_answer("ByteDance/AnimateDiff-Lightning")                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
Out - Final answer: ByteDance/AnimateDiff-Lightning
[Step 1: Duration 0.10 seconds| Input tokens: 4,288 | Output tokens: 148]
Out[20]: 'ByteDance/AnimateDiff-Lightning'
```

> [!TIP]
> åœ¨ [ä¸“ç”¨æ•™ç¨‹](./tutorials/tools#what-is-a-tool-and-how-to-build-one) ä¸­äº†è§£æ›´å¤šå…³äºŽå·¥å…·çš„å†…å®¹ã€‚

## å¤š agent

å¤š agent ç³»ç»Ÿæ˜¯éšç€å¾®è½¯çš„æ¡†æž¶ [Autogen](https://huggingface.co/papers/2308.08155) å¼•å…¥çš„ã€‚

åœ¨è¿™ç§ç±»åž‹çš„æ¡†æž¶ä¸­ï¼Œæ‚¨æœ‰å¤šä¸ª agent ä¸€èµ·å·¥ä½œæ¥è§£å†³æ‚¨çš„ä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªæœ‰ä¸€ä¸ªã€‚
ç»éªŒè¡¨æ˜Žï¼Œè¿™åœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­è¡¨çŽ°æ›´å¥½ã€‚è¿™ç§æ›´å¥½è¡¨çŽ°çš„åŽŸå› åœ¨æ¦‚å¿µä¸Šå¾ˆç®€å•ï¼šå¯¹äºŽè®¸å¤šä»»åŠ¡ï¼Œä¸Žå…¶ä½¿ç”¨ä¸€ä¸ªå…¨èƒ½ç³»ç»Ÿï¼Œæ‚¨æ›´æ„¿æ„å°†å•å…ƒä¸“é—¨ç”¨äºŽå­ä»»åŠ¡ã€‚åœ¨è¿™é‡Œï¼Œæ‹¥æœ‰å…·æœ‰å•ç‹¬å·¥å…·é›†å’Œå†…å­˜çš„ agent å¯ä»¥å®žçŽ°é«˜æ•ˆçš„ä¸“ä¸šåŒ–ã€‚ä¾‹å¦‚ï¼Œä¸ºä»€ä¹ˆè¦ç”¨ç½‘é¡µæœç´¢ agent è®¿é—®çš„æ‰€æœ‰ç½‘é¡µå†…å®¹å¡«å……ä»£ç ç”Ÿæˆ agent çš„å†…å­˜ï¼Ÿæœ€å¥½å°†å®ƒä»¬åˆ†å¼€ã€‚

æ‚¨å¯ä»¥ä½¿ç”¨ `smolagents` è½»æ¾æž„å»ºåˆ†å±‚å¤š agent ç³»ç»Ÿã€‚

ä¸ºæ­¤ï¼Œå°† agent å°è£…åœ¨ [`ManagedAgent`] å¯¹è±¡ä¸­ã€‚æ­¤å¯¹è±¡éœ€è¦å‚æ•° `agent`ã€`name` å’Œ `description`ï¼Œè¿™äº›å‚æ•°å°†åµŒå…¥åˆ°ç®¡ç† agent çš„ç³»ç»Ÿæç¤ºä¸­ï¼Œä»¥è®©å®ƒçŸ¥é“å¦‚ä½•è°ƒç”¨æ­¤æ‰˜ç®¡ agentï¼Œå°±åƒæˆ‘ä»¬å¯¹å·¥å…·æ‰€åšçš„é‚£æ ·ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨æˆ‘ä»¬çš„ [`WebSearchTool`] åˆ¶ä½œä¸€ä¸ªç®¡ç†ç‰¹å®šç½‘é¡µæœç´¢ agent çš„ agent çš„ç¤ºä¾‹ï¼š

```py
from smolagents import CodeAgent, InferenceClientModel, WebSearchTool, ManagedAgent

model = InferenceClientModel()

web_agent = CodeAgent(tools=[WebSearchTool()], model=model)

managed_web_agent = ManagedAgent(
    agent=web_agent,
    name="web_search",
    description="Runs web searches for you. Give it your query as an argument."
)

manager_agent = CodeAgent(
    tools=[], model=model, managed_agents=[managed_web_agent]
)

manager_agent.run("Who is the CEO of Hugging Face?")
```

> [!TIP]
> æœ‰å…³é«˜æ•ˆå¤š agent å®žçŽ°çš„æ·±å…¥ç¤ºä¾‹ï¼Œè¯·å‚é˜… [æˆ‘ä»¬å¦‚ä½•å°†å¤š agent ç³»ç»ŸæŽ¨å‘ GAIA æŽ’è¡Œæ¦œçš„é¡¶éƒ¨](https://huggingface.co/blog/beating-gaia)ã€‚


## ä¸Žæ‚¨çš„ agent äº¤è°ˆå¹¶åœ¨é…·ç‚«çš„ Gradio ç•Œé¢ä¸­å¯è§†åŒ–å…¶æ€è€ƒè¿‡ç¨‹

æ‚¨å¯ä»¥ä½¿ç”¨ `GradioUI` äº¤äº’å¼åœ°å‘æ‚¨çš„ agent æäº¤ä»»åŠ¡å¹¶è§‚å¯Ÿå…¶æ€è€ƒå’Œæ‰§è¡Œè¿‡ç¨‹ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š

```py
from smolagents import (
    load_tool,
    CodeAgent,
    InferenceClientModel,
    GradioUI
)

# ä»Ž Hub å¯¼å…¥å·¥å…·
image_generation_tool = load_tool("m-ric/text-to-image")

model = InferenceClientModel(model_id=model_id)

# ä½¿ç”¨å›¾åƒç”Ÿæˆå·¥å…·åˆå§‹åŒ– agent
agent = CodeAgent(tools=[image_generation_tool], model=model)

GradioUI(agent).launch()
```

åœ¨åº•å±‚ï¼Œå½“ç”¨æˆ·è¾“å…¥æ–°ç­”æ¡ˆæ—¶ï¼Œagent ä¼šä»¥ `agent.run(user_request, reset=False)` å¯åŠ¨ã€‚
`reset=False` æ ‡å¿—æ„å‘³ç€åœ¨å¯åŠ¨æ­¤æ–°ä»»åŠ¡ä¹‹å‰ä¸ä¼šåˆ·æ–° agent çš„å†…å­˜ï¼Œè¿™ä½¿å¾—å¯¹è¯å¯ä»¥ç»§ç»­ã€‚

æ‚¨ä¹Ÿå¯ä»¥åœ¨å…¶ä»– agent åŒ–åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨æ­¤ `reset=False` å‚æ•°æ¥ä¿æŒå¯¹è¯ç»§ç»­ã€‚

## ä¸‹ä¸€æ­¥

æœ€åŽï¼Œå½“æ‚¨æŒ‰éœ€é…ç½®å¥½agentåŽï¼Œå³å¯å°†å…¶åˆ†äº«è‡³ Hubï¼

```py
agent.push_to_hub("m-ric/my_agent")
```

ç±»ä¼¼åœ°ï¼Œè‹¥è¦åŠ è½½å·²æŽ¨é€è‡³ Hub çš„agentï¼Œåœ¨ä¿¡ä»»å…¶å·¥å…·ä»£ç çš„å‰æä¸‹ï¼Œå¯ä½¿ç”¨ï¼š

```py
agent.from_hub("m-ric/my_agent", trust_remote_code=True)
```

è¦æ›´æ·±å…¥åœ°ä½¿ç”¨ï¼Œæ‚¨å°†éœ€è¦æŸ¥çœ‹æˆ‘ä»¬çš„æ•™ç¨‹ï¼š
- [æˆ‘ä»¬çš„ä»£ç  agent å¦‚ä½•å·¥ä½œçš„è§£é‡Š](./tutorials/secure_code_execution)
- [æœ¬æŒ‡å—å…³äºŽå¦‚ä½•æž„å»ºå¥½çš„ agent](./tutorials/building_good_agents)ã€‚
- [å·¥å…·ä½¿ç”¨çš„æ·±å…¥æŒ‡å—](./tutorials/tools)ã€‚



================================================
FILE: docs/source/zh/index.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/conceptual_guides/intro_agents.md
================================================
# Agent ç®€ä»‹

> [!TIP]
> è¯‘è€…æ³¨ï¼šAgent çš„ä¸šå†…æœ¯è¯­æ˜¯â€œæ™ºèƒ½ä½“â€ã€‚æœ¬è¯‘æ–‡å°†ä¿ç•™ agentï¼Œä¸ä½œç¿»è¯‘ï¼Œä»¥å¸¦æ¥æ›´é«˜æ•ˆçš„é˜…è¯»ä½“éªŒã€‚(åœ¨ä¸­æ–‡ä¸ºä¸»çš„æ–‡ç« ä¸­ï¼ŒIt's easier to æ³¨æ„åˆ°è‹±æ–‡ã€‚Attention Is All You Need!)

## ðŸ¤” ä»€ä¹ˆæ˜¯ agentï¼Ÿ

ä»»ä½•ä½¿ç”¨ AI çš„é«˜æ•ˆç³»ç»Ÿéƒ½éœ€è¦ä¸º LLM æä¾›æŸç§è®¿é—®çŽ°å®žä¸–ç•Œçš„æ–¹å¼ï¼šä¾‹å¦‚è°ƒç”¨æœç´¢å·¥å…·èŽ·å–å¤–éƒ¨ä¿¡æ¯ï¼Œæˆ–è€…æ“ä½œæŸäº›ç¨‹åºä»¥å®Œæˆä»»åŠ¡ã€‚æ¢å¥è¯è¯´ï¼ŒLLM åº”è¯¥å…·æœ‰ **_Agent èƒ½åŠ›_**ã€‚Agent ç¨‹åºæ˜¯ LLM é€šå¾€å¤–éƒ¨ä¸–ç•Œçš„é—¨æˆ·ã€‚

> [!TIP]
> AI agent æ˜¯ **LLM è¾“å‡ºæŽ§åˆ¶å·¥ä½œæµçš„ç¨‹åº**ã€‚

ä»»ä½•åˆ©ç”¨ LLM çš„ç³»ç»Ÿéƒ½ä¼šå°† LLM è¾“å‡ºé›†æˆåˆ°ä»£ç ä¸­ã€‚LLM è¾“å…¥å¯¹ä»£ç å·¥ä½œæµçš„å½±å“ç¨‹åº¦å°±æ˜¯ LLM åœ¨ç³»ç»Ÿä¸­çš„ agent èƒ½åŠ›çº§åˆ«ã€‚

è¯·æ³¨æ„ï¼Œæ ¹æ®è¿™ä¸ªå®šä¹‰ï¼Œ"Agent" ä¸æ˜¯ä¸€ä¸ªç¦»æ•£çš„ã€éž 0 å³ 1 çš„å®šä¹‰ï¼šç›¸åï¼Œ"Agent èƒ½åŠ›" æ˜¯ä¸€ä¸ªè¿žç»­è°±ç³»ï¼Œéšç€ä½ åœ¨å·¥ä½œæµä¸­ç»™äºˆ LLM æ›´å¤šæˆ–æ›´å°‘çš„æƒåŠ›è€Œå˜åŒ–ã€‚

è¯·å‚è§ä¸‹è¡¨ä¸­ agent èƒ½åŠ›åœ¨ä¸åŒç³»ç»Ÿä¸­çš„å˜åŒ–ï¼š

| Agent èƒ½åŠ›çº§åˆ« | æè¿°                                           | åç§°       | ç¤ºä¾‹æ¨¡å¼                                           |
| ------------ | ---------------------------------------------- | ---------- | -------------------------------------------------- |
| â˜†â˜†â˜†          | LLM è¾“å‡ºå¯¹ç¨‹åºæµç¨‹æ²¡æœ‰å½±å“                     | ç®€å•å¤„ç†å™¨ | `process_llm_output(llm_response)`                 |
| â˜…â˜†â˜†          | LLM è¾“å‡ºå†³å®š if/else åˆ†æ”¯                      | è·¯ç”±       | `if llm_decision(): path_a() else: path_b()`       |
| â˜…â˜…â˜†          | LLM è¾“å‡ºå†³å®šå‡½æ•°æ‰§è¡Œ                           | å·¥å…·è°ƒç”¨è€… | `run_function(llm_chosen_tool, llm_chosen_args)`   |
| â˜…â˜…â˜…          | LLM è¾“å‡ºæŽ§åˆ¶è¿­ä»£å’Œç¨‹åºç»§ç»­                     | å¤šæ­¥ Agent | `while llm_should_continue(): execute_next_step()` |
| â˜…â˜…â˜…          | ä¸€ä¸ª agent å·¥ä½œæµå¯ä»¥å¯åŠ¨å¦ä¸€ä¸ª agent å·¥ä½œæµ | å¤š Agent   | `if llm_trigger(): execute_agent()`                |

å¤šæ­¥ agent å…·æœ‰ä»¥ä¸‹ä»£ç ç»“æž„ï¼š

```python
memory = [user_defined_task]
while llm_should_continue(memory): # è¿™ä¸ªå¾ªçŽ¯æ˜¯å¤šæ­¥éƒ¨åˆ†
    action = llm_get_next_action(memory) # è¿™æ˜¯å·¥å…·è°ƒç”¨éƒ¨åˆ†
    observations = execute_action(action)
    memory += [action, observations]
```

è¿™ä¸ª agent ç³»ç»Ÿåœ¨ä¸€ä¸ªå¾ªçŽ¯ä¸­è¿è¡Œï¼Œæ¯ä¸€æ­¥æ‰§è¡Œä¸€ä¸ªæ–°åŠ¨ä½œï¼ˆè¯¥åŠ¨ä½œå¯èƒ½æ¶‰åŠè°ƒç”¨ä¸€äº›é¢„å®šä¹‰çš„ *å·¥å…·*ï¼Œè¿™äº›å·¥å…·åªæ˜¯å‡½æ•°ï¼‰ï¼Œç›´åˆ°å…¶è§‚å¯Ÿç»“æžœè¡¨æ˜Žå·²è¾¾åˆ°è§£å†³ç»™å®šä»»åŠ¡çš„æ»¡æ„çŠ¶æ€ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤šæ­¥ agent å¦‚ä½•è§£å†³ç®€å•æ•°å­¦é—®é¢˜çš„ç¤ºä¾‹ï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"/>
</div>

## âœ… ä½•æ—¶ä½¿ç”¨ agent / â›” ä½•æ—¶é¿å…ä½¿ç”¨

å½“ä½ éœ€è¦ LLM ç¡®å®šåº”ç”¨ç¨‹åºçš„å·¥ä½œæµæ—¶ï¼Œagent å¾ˆæœ‰ç”¨ã€‚ä½†å®ƒä»¬é€šå¸¸æœ‰äº›è¿‡åº¦ã€‚é—®é¢˜æ˜¯ï¼šæˆ‘çœŸçš„éœ€è¦å·¥ä½œæµçš„çµæ´»æ€§æ¥æœ‰æ•ˆè§£å†³æ‰‹å¤´çš„ä»»åŠ¡å—ï¼Ÿ
å¦‚æžœé¢„å®šä¹‰çš„å·¥ä½œæµç»å¸¸ä¸è¶³ï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦æ›´å¤šçš„çµæ´»æ€§ã€‚
è®©æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­ï¼šå‡è®¾ä½ æ­£åœ¨å¼€å‘ä¸€ä¸ªå¤„ç†å†²æµªæ—…è¡Œç½‘ç«™å®¢æˆ·è¯·æ±‚çš„åº”ç”¨ç¨‹åºã€‚

ä½ å¯ä»¥æå‰çŸ¥é“è¯·æ±‚å°†å±žäºŽ 2 ä¸ªç±»åˆ«ä¹‹ä¸€ï¼ˆåŸºäºŽç”¨æˆ·é€‰æ‹©ï¼‰ï¼Œå¹¶ä¸”ä½ ä¸ºè¿™ 2 ç§æƒ…å†µéƒ½æœ‰é¢„å®šä¹‰çš„å·¥ä½œæµã€‚

1. æƒ³è¦äº†è§£æ—…è¡Œä¿¡æ¯ï¼Ÿâ‡’ ç»™ä»–ä»¬è®¿é—®æœç´¢æ ä»¥æœç´¢ä½ çš„çŸ¥è¯†åº“
2. æƒ³ä¸Žé”€å”®äº¤è°ˆï¼Ÿâ‡’ è®©ä»–ä»¬å¡«å†™è”ç³»è¡¨å•ã€‚

å¦‚æžœè¿™ä¸ªç¡®å®šæ€§å·¥ä½œæµé€‚åˆæ‰€æœ‰æŸ¥è¯¢ï¼Œé‚£å°±ç›´æŽ¥ç¼–ç å§ï¼è¿™å°†ä¸ºä½ æä¾›ä¸€ä¸ª 100% å¯é çš„ç³»ç»Ÿï¼Œæ²¡æœ‰è®©ä¸å¯é¢„æµ‹çš„ LLM å¹²æ‰°ä½ çš„å·¥ä½œæµè€Œå¼•å…¥é”™è¯¯çš„é£Žé™©ã€‚ä¸ºäº†ç®€å•å’Œç¨³å¥èµ·è§ï¼Œå»ºè®®è§„èŒƒåŒ–ä¸ä½¿ç”¨ä»»ä½• agent è¡Œä¸ºã€‚

ä½†å¦‚æžœå·¥ä½œæµä¸èƒ½æå‰ç¡®å®šå¾—é‚£ä¹ˆå¥½å‘¢ï¼Ÿ

ä¾‹å¦‚ï¼Œç”¨æˆ·æƒ³é—®ï¼š`"I can come on Monday, but I forgot my passport so risk being delayed to Wednesday, is it possible to take me and my stuff to surf on Tuesday morning, with a cancellation insurance?"` è¿™ä¸ªé—®é¢˜æ¶‰åŠè®¸å¤šå› ç´ ï¼Œå¯èƒ½ä¸Šè¿°é¢„å®šçš„æ ‡å‡†éƒ½ä¸è¶³ä»¥æ»¡è¶³è¿™ä¸ªè¯·æ±‚ã€‚

å¦‚æžœé¢„å®šä¹‰çš„å·¥ä½œæµç»å¸¸ä¸è¶³ï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦æ›´å¤šçš„çµæ´»æ€§ã€‚

è¿™å°±æ˜¯ agent è®¾ç½®å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œä½ å¯ä»¥åˆ›å»ºä¸€ä¸ªå¤šæ­¥ agentï¼Œå®ƒå¯ä»¥è®¿é—®å¤©æ°” API èŽ·å–å¤©æ°”é¢„æŠ¥ï¼ŒGoogle Maps API è®¡ç®—æ—…è¡Œè·ç¦»ï¼Œå‘˜å·¥åœ¨çº¿ä»ªè¡¨æ¿å’Œä½ çš„çŸ¥è¯†åº“ä¸Šçš„ RAG ç³»ç»Ÿã€‚

ç›´åˆ°æœ€è¿‘ï¼Œè®¡ç®—æœºç¨‹åºè¿˜å±€é™äºŽé¢„å®šä¹‰çš„å·¥ä½œæµï¼Œè¯•å›¾é€šè¿‡å †ç§¯ if/else åˆ†æ”¯æ¥å¤„ç†å¤æ‚æ€§ã€‚å®ƒä»¬ä¸“æ³¨äºŽæžå…¶ç‹­çª„çš„ä»»åŠ¡ï¼Œå¦‚"è®¡ç®—è¿™äº›æ•°å­—çš„æ€»å’Œ"æˆ–"æ‰¾åˆ°è¿™ä¸ªå›¾ä¸­çš„æœ€çŸ­è·¯å¾„"ã€‚ä½†å®žé™…ä¸Šï¼Œå¤§å¤šæ•°çŽ°å®žç”Ÿæ´»ä¸­çš„ä»»åŠ¡ï¼Œå¦‚æˆ‘ä»¬ä¸Šé¢çš„æ—…è¡Œç¤ºä¾‹ï¼Œéƒ½ä¸é€‚åˆé¢„å®šä¹‰çš„å·¥ä½œæµã€‚agent ç³»ç»Ÿä¸ºç¨‹åºæ‰“å¼€äº†çŽ°å®žä¸–ç•Œä»»åŠ¡çš„å¤§é—¨ï¼

## ä¸ºä»€ä¹ˆé€‰æ‹© `smolagents`ï¼Ÿ

å¯¹äºŽä¸€äº›ä½Žçº§çš„ agent ç”¨ä¾‹ï¼Œå¦‚é“¾æˆ–è·¯ç”±å™¨ï¼Œä½ å¯ä»¥è‡ªå·±ç¼–å†™æ‰€æœ‰ä»£ç ã€‚è¿™æ ·ä¼šæ›´å¥½ï¼Œå› ä¸ºå®ƒå¯ä»¥è®©ä½ æ›´å¥½åœ°æŽ§åˆ¶å’Œç†è§£ä½ çš„ç³»ç»Ÿã€‚

ä½†ä¸€æ—¦ä½ å¼€å§‹è¿½æ±‚æ›´å¤æ‚çš„è¡Œä¸ºï¼Œæ¯”å¦‚è®© LLM è°ƒç”¨å‡½æ•°ï¼ˆå³"å·¥å…·è°ƒç”¨"ï¼‰æˆ–è®© LLM è¿è¡Œ while å¾ªçŽ¯ï¼ˆ"å¤šæ­¥ agent"ï¼‰ï¼Œä¸€äº›æŠ½è±¡å°±å˜å¾—å¿…è¦ï¼š

- å¯¹äºŽå·¥å…·è°ƒç”¨ï¼Œä½ éœ€è¦è§£æž agent çš„è¾“å‡ºï¼Œå› æ­¤è¿™ä¸ªè¾“å‡ºéœ€è¦ä¸€ä¸ªé¢„å®šä¹‰çš„æ ¼å¼ï¼Œå¦‚"Thought: I should call tool 'get_weather'. Action: get_weather(Paris)."ï¼Œä½ ç”¨é¢„å®šä¹‰çš„å‡½æ•°è§£æžå®ƒï¼Œå¹¶ä¸”ç»™ LLM çš„ç³»ç»Ÿæç¤ºåº”è¯¥é€šçŸ¥å®ƒè¿™ä¸ªæ ¼å¼ã€‚
- å¯¹äºŽ LLM è¾“å‡ºå†³å®šå¾ªçŽ¯çš„å¤šæ­¥ agentï¼Œä½ éœ€è¦æ ¹æ®ä¸Šæ¬¡å¾ªçŽ¯è¿­ä»£ä¸­å‘ç”Ÿçš„æƒ…å†µç»™ LLM ä¸åŒçš„æç¤ºï¼šæ‰€ä»¥ä½ éœ€è¦æŸç§è®°å¿†èƒ½åŠ›ã€‚

çœ‹åˆ°äº†å—ï¼Ÿé€šè¿‡è¿™ä¸¤ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å·²ç»å‘çŽ°éœ€è¦ä¸€äº›é¡¹ç›®æ¥å¸®åŠ©æˆ‘ä»¬ï¼š

- å½“ç„¶ï¼Œä¸€ä¸ªä½œä¸ºç³»ç»Ÿå¼•æ“Žçš„ LLM
- agent å¯ä»¥è®¿é—®çš„å·¥å…·åˆ—è¡¨
- ä»Ž LLM è¾“å‡ºä¸­æå–å·¥å…·è°ƒç”¨çš„è§£æžå™¨
- ä¸Žè§£æžå™¨åŒæ­¥çš„ç³»ç»Ÿæç¤º
- è®°å¿†èƒ½åŠ›

ä½†æ˜¯ç­‰ç­‰ï¼Œæ—¢ç„¶æˆ‘ä»¬ç»™ LLM åœ¨å†³ç­–ä¸­ç•™å‡ºäº†ç©ºé—´ï¼Œå®ƒä»¬è‚¯å®šä¼šçŠ¯é”™è¯¯ï¼šæ‰€ä»¥æˆ‘ä»¬éœ€è¦é”™è¯¯æ—¥å¿—è®°å½•å’Œé‡è¯•æœºåˆ¶ã€‚

æ‰€æœ‰è¿™äº›å…ƒç´ éƒ½éœ€è¦ç´§å¯†è€¦åˆæ‰èƒ½å½¢æˆä¸€ä¸ªåŠŸèƒ½è‰¯å¥½çš„ç³»ç»Ÿã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å†³å®šéœ€è¦åˆ¶ä½œåŸºæœ¬æž„å»ºå—æ¥è®©æ‰€æœ‰è¿™äº›ä¸œè¥¿ååŒå·¥ä½œã€‚

## ä»£ç  agent

åœ¨å¤šæ­¥ agent ä¸­ï¼Œæ¯ä¸€æ­¥ LLM éƒ½å¯ä»¥ç¼–å†™ä¸€ä¸ªåŠ¨ä½œï¼Œå½¢å¼ä¸ºè°ƒç”¨å¤–éƒ¨å·¥å…·ã€‚ç¼–å†™è¿™äº›åŠ¨ä½œçš„å¸¸è§æ ¼å¼ï¼ˆç”± Anthropicã€OpenAI ç­‰ä½¿ç”¨ï¼‰é€šå¸¸æ˜¯"å°†åŠ¨ä½œç¼–å†™ä¸ºå·¥å…·åç§°å’Œè¦ä½¿ç”¨çš„å‚æ•°çš„ JSONï¼Œç„¶åŽè§£æžä»¥çŸ¥é“è¦æ‰§è¡Œå“ªä¸ªå·¥å…·ä»¥åŠä½¿ç”¨å“ªäº›å‚æ•°"çš„ä¸åŒå˜ä½“ã€‚

[å¤šé¡¹](https://huggingface.co/papers/2402.01030) [ç ”ç©¶](https://huggingface.co/papers/2411.01747) [è®ºæ–‡](https://huggingface.co/papers/2401.00812) è¡¨æ˜Žï¼Œåœ¨ä»£ç ä¸­è¿›è¡Œå·¥å…·è°ƒç”¨çš„ LLM è¦å¥½å¾—å¤šã€‚

åŽŸå› å¾ˆç®€å•ï¼Œ_æˆ‘ä»¬ä¸“é—¨è®¾è®¡äº†æˆ‘ä»¬çš„ä»£ç è¯­è¨€ï¼Œä½¿å…¶æˆä¸ºè¡¨è¾¾è®¡ç®—æœºæ‰§è¡ŒåŠ¨ä½œçš„æœ€ä½³æ–¹å¼_ã€‚å¦‚æžœ JSON ç‰‡æ®µæ˜¯æ›´å¥½çš„è¡¨è¾¾æ–¹å¼ï¼ŒJSON å°†æˆä¸ºé¡¶çº§ç¼–ç¨‹è¯­è¨€ï¼Œç¼–ç¨‹å°†å˜å¾—éžå¸¸å›°éš¾ã€‚

ä¸‹å›¾å–è‡ª [Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030)ï¼Œè¯´æ˜Žäº†ç”¨ä»£ç ç¼–å†™åŠ¨ä½œçš„ä¸€äº›ä¼˜åŠ¿ï¼š

<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/code_vs_json_actions.png">

ä¸Ž JSON ç‰‡æ®µç›¸æ¯”ï¼Œç”¨ä»£ç ç¼–å†™åŠ¨ä½œæä¾›äº†æ›´å¥½çš„ï¼š

- **å¯ç»„åˆæ€§ï¼š** ä½ èƒ½åƒå®šä¹‰ python å‡½æ•°ä¸€æ ·ï¼Œå°† JSON åŠ¨ä½œåµŒå¥—åœ¨ä¸€èµ·ï¼Œæˆ–å®šä¹‰ä¸€ç»„ JSON åŠ¨ä½œä»¥ä¾›é‡ç”¨å—ï¼Ÿ
- **å¯¹è±¡ç®¡ç†ï¼š** ä½ å¦‚ä½•åœ¨ JSON ä¸­å­˜å‚¨åƒ `generate_image` è¿™æ ·çš„åŠ¨ä½œçš„è¾“å‡ºï¼Ÿ
- **é€šç”¨æ€§ï¼š** ä»£ç è¢«æž„å»ºä¸ºç®€å•åœ°è¡¨è¾¾ä»»ä½•ä½ å¯ä»¥è®©è®¡ç®—æœºåšçš„äº‹æƒ…ã€‚
- **LLM è®­ç»ƒæ•°æ®ä¸­çš„è¡¨ç¤ºï¼š** å¤§é‡é«˜è´¨é‡çš„ä»£ç åŠ¨ä½œå·²ç»åŒ…å«åœ¨ LLM çš„è®­ç»ƒæ•°æ®ä¸­ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å·²ç»ä¸ºæ­¤è¿›è¡Œäº†è®­ç»ƒï¼



================================================
FILE: docs/source/zh/conceptual_guides/react.md
================================================
# å¤šæ­¥éª¤ agent æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

ReAct æ¡†æž¶ï¼ˆ[Yao et al., 2022](https://huggingface.co/papers/2210.03629)ï¼‰æ˜¯ç›®å‰æž„å»º agent çš„ä¸»è¦æ–¹æ³•ã€‚

è¯¥åç§°åŸºäºŽä¸¤ä¸ªè¯çš„ç»„åˆï¼š"Reason" ï¼ˆæŽ¨ç†ï¼‰å’Œ "Act" ï¼ˆè¡ŒåŠ¨ï¼‰ã€‚å®žé™…ä¸Šï¼Œéµå¾ªæ­¤æž¶æž„çš„ agent å°†æ ¹æ®éœ€è¦å°½å¯èƒ½å¤šçš„æ­¥éª¤æ¥è§£å†³å…¶ä»»åŠ¡ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…æ‹¬ä¸€ä¸ªæŽ¨ç†æ­¥éª¤ï¼Œç„¶åŽæ˜¯ä¸€ä¸ªè¡ŒåŠ¨æ­¥éª¤ï¼Œåœ¨è¯¥æ­¥éª¤ä¸­ï¼Œå®ƒåˆ¶å®šå·¥å…·è°ƒç”¨ï¼Œä½¿å…¶æ›´æŽ¥è¿‘è§£å†³æ‰‹å¤´çš„ä»»åŠ¡ã€‚

ReAct è¿‡ç¨‹æ¶‰åŠä¿ç•™è¿‡åŽ»æ­¥éª¤çš„è®°å¿†ã€‚

> [!TIP]
> é˜…è¯» [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) åšå®¢æ–‡ç« ä»¥äº†è§£æ›´å¤šå…³äºŽå¤šæ­¥ agent çš„ä¿¡æ¯ã€‚

ä»¥ä¸‹æ˜¯å…¶å·¥ä½œåŽŸç†çš„è§†é¢‘æ¦‚è¿°ï¼š

<div class="flex justify-center">
    <img
        class="block dark:hidden"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
    <img
        class="hidden dark:block"
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif"
    />
</div>

![ReAct agent çš„æ¡†æž¶](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/open-source-llms-as-agents/ReAct.png)

æˆ‘ä»¬å®žçŽ°äº†ä¸¤ä¸ªç‰ˆæœ¬çš„ ToolCallingAgentï¼š
- [`ToolCallingAgent`] åœ¨å…¶è¾“å‡ºä¸­ç”Ÿæˆ JSON æ ¼å¼çš„å·¥å…·è°ƒç”¨ã€‚
- [`CodeAgent`] æ˜¯ä¸€ç§æ–°åž‹çš„ ToolCallingAgentï¼Œå®ƒç”Ÿæˆä»£ç å—å½¢å¼çš„å·¥å…·è°ƒç”¨ï¼Œè¿™å¯¹äºŽå…·æœ‰å¼ºå¤§ç¼–ç æ€§èƒ½çš„ LLM éžå¸¸æœ‰æ•ˆã€‚



================================================
FILE: docs/source/zh/examples/multiagents.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/examples/rag.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/examples/text_to_sql.md
================================================
# Text-to-SQL

[[open-in-colab]]

åœ¨æ­¤æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ `smolagents` å®žçŽ°ä¸€ä¸ªåˆ©ç”¨ SQL çš„ agentã€‚

> è®©æˆ‘ä»¬ä»Žç»å…¸é—®é¢˜å¼€å§‹ï¼šä¸ºä»€ä¹ˆä¸ç®€å•åœ°ä½¿ç”¨æ ‡å‡†çš„ text-to-SQL pipeline å‘¢ï¼Ÿ

æ ‡å‡†çš„ text-to-SQL pipeline å¾ˆè„†å¼±ï¼Œå› ä¸ºç”Ÿæˆçš„ SQL æŸ¥è¯¢å¯èƒ½ä¼šå‡ºé”™ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼ŒæŸ¥è¯¢å¯èƒ½å‡ºé”™å´ä¸å¼•å‘é”™è¯¯è­¦æŠ¥ï¼Œä»Žè€Œè¿”å›žä¸€äº›ä¸æ­£ç¡®æˆ–æ— ç”¨çš„ç»“æžœã€‚

ðŸ‘‰ ç›¸åï¼Œagent ç³»ç»Ÿåˆ™å¯ä»¥æ£€è§†è¾“å‡ºç»“æžœå¹¶å†³å®šæŸ¥è¯¢æ˜¯å¦éœ€è¦è¢«æ›´æ”¹ï¼Œå› æ­¤å¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ã€‚

è®©æˆ‘ä»¬æ¥ä¸€èµ·æž„å»ºè¿™ä¸ª agent! ðŸ’ª

é¦–å…ˆï¼Œæˆ‘ä»¬æž„å»ºä¸€ä¸ª SQL çš„çŽ¯å¢ƒï¼š
```py
from sqlalchemy import (
    create_engine,
    MetaData,
    Table,
    Column,
    String,
    Integer,
    Float,
    insert,
    inspect,
    text,
)

engine = create_engine("sqlite:///:memory:")
metadata_obj = MetaData()

# create city SQL table
table_name = "receipts"
receipts = Table(
    table_name,
    metadata_obj,
    Column("receipt_id", Integer, primary_key=True),
    Column("customer_name", String(16), primary_key=True),
    Column("price", Float),
    Column("tip", Float),
)
metadata_obj.create_all(engine)

rows = [
    {"receipt_id": 1, "customer_name": "Alan Payne", "price": 12.06, "tip": 1.20},
    {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
    {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
    {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
]
for row in rows:
    stmt = insert(receipts).values(**row)
    with engine.begin() as connection:
        cursor = connection.execute(stmt)
```

### æž„å»º agent

çŽ°åœ¨ï¼Œæˆ‘ä»¬æž„å»ºä¸€ä¸ª agentï¼Œå®ƒå°†ä½¿ç”¨ SQL æŸ¥è¯¢æ¥å›žç­”é—®é¢˜ã€‚å·¥å…·çš„ description å±žæ€§å°†è¢« agent ç³»ç»ŸåµŒå…¥åˆ° LLM çš„æç¤ºä¸­ï¼šå®ƒä¸º LLM æä¾›æœ‰å…³å¦‚ä½•ä½¿ç”¨è¯¥å·¥å…·çš„ä¿¡æ¯ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬æè¿° SQL è¡¨çš„åœ°æ–¹ã€‚

```py
inspector = inspect(engine)
columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]

table_description = "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
print(table_description)
```

```text
Columns:
  - receipt_id: INTEGER
  - customer_name: VARCHAR(16)
  - price: FLOAT
  - tip: FLOAT
```

çŽ°åœ¨è®©æˆ‘ä»¬æž„å»ºæˆ‘ä»¬çš„å·¥å…·ã€‚å®ƒéœ€è¦ä»¥ä¸‹å†…å®¹ï¼šï¼ˆæ›´å¤šç»†èŠ‚è¯·å‚é˜…[å·¥å…·æ–‡æ¡£](../tutorials/tools)ï¼‰

- ä¸€ä¸ªå¸¦æœ‰ `Args:` éƒ¨åˆ†åˆ—å‡ºå‚æ•°çš„ docstringã€‚
- è¾“å…¥å’Œè¾“å‡ºçš„type hintsã€‚

```py
from smolagents import tool

@tool
def sql_engine(query: str) -> str:
    """
    Allows you to perform SQL queries on the table. Returns a string representation of the result.
    The table is named 'receipts'. Its description is as follows:
        Columns:
        - receipt_id: INTEGER
        - customer_name: VARCHAR(16)
        - price: FLOAT
        - tip: FLOAT

    Args:
        query: The query to perform. This should be correct SQL.
    """
    output = ""
    with engine.connect() as con:
        rows = con.execute(text(query))
        for row in rows:
            output += "\n" + str(row)
    return output
```

æˆ‘ä»¬çŽ°åœ¨ä½¿ç”¨è¿™ä¸ªå·¥å…·æ¥åˆ›å»ºä¸€ä¸ª agentã€‚æˆ‘ä»¬ä½¿ç”¨ `CodeAgent`ï¼Œè¿™æ˜¯ smolagent çš„ä¸»è¦ agent ç±»ï¼šä¸€ä¸ªåœ¨ä»£ç ä¸­ç¼–å†™æ“ä½œå¹¶æ ¹æ® ReAct æ¡†æž¶è¿­ä»£å…ˆå‰è¾“å‡ºçš„ agentã€‚

è¿™ä¸ªæ¨¡åž‹æ˜¯é©±åŠ¨ agent ç³»ç»Ÿçš„ LLMã€‚`InferenceClientModel` å…è®¸ä½ ä½¿ç”¨ HF  Inference API è°ƒç”¨ LLMï¼Œæ— è®ºæ˜¯é€šè¿‡ Serverless è¿˜æ˜¯ Dedicated endpointï¼Œä½†ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä»»ä½•ä¸“æœ‰ APIã€‚

```py
from smolagents import CodeAgent, InferenceClientModel

agent = CodeAgent(
    tools=[sql_engine],
    model=InferenceClientModel(model_id="meta-llama/Meta-Llama-3.1-8B-Instruct"),
)
agent.run("Can you give me the name of the client who got the most expensive receipt?")
```

### Level 2: è¡¨è¿žæŽ¥

çŽ°åœ¨è®©æˆ‘ä»¬å¢žåŠ ä¸€äº›æŒ‘æˆ˜ï¼æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ agent èƒ½å¤Ÿå¤„ç†è·¨å¤šä¸ªè¡¨çš„è¿žæŽ¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°è¡¨ï¼Œè®°å½•æ¯ä¸ª receipt_id çš„æœåŠ¡å‘˜åå­—ï¼

```py
table_name = "waiters"
receipts = Table(
    table_name,
    metadata_obj,
    Column("receipt_id", Integer, primary_key=True),
    Column("waiter_name", String(16), primary_key=True),
)
metadata_obj.create_all(engine)

rows = [
    {"receipt_id": 1, "waiter_name": "Corey Johnson"},
    {"receipt_id": 2, "waiter_name": "Michael Watts"},
    {"receipt_id": 3, "waiter_name": "Michael Watts"},
    {"receipt_id": 4, "waiter_name": "Margaret James"},
]
for row in rows:
    stmt = insert(receipts).values(**row)
    with engine.begin() as connection:
        cursor = connection.execute(stmt)
```

å› ä¸ºæˆ‘ä»¬æ”¹å˜äº†è¡¨ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ–° `SQLExecutorTool`ï¼Œè®© LLM èƒ½å¤Ÿæ­£ç¡®åˆ©ç”¨è¿™ä¸ªè¡¨çš„ä¿¡æ¯ã€‚

```py
updated_description = """Allows you to perform SQL queries on the table. Beware that this tool's output is a string representation of the execution output.
It can use the following tables:"""

inspector = inspect(engine)
for table in ["receipts", "waiters"]:
    columns_info = [(col["name"], col["type"]) for col in inspector.get_columns(table)]

    table_description = f"Table '{table}':\n"

    table_description += "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
    updated_description += "\n\n" + table_description

print(updated_description)
```

å› ä¸ºè¿™ä¸ªrequest æ¯”ä¹‹å‰çš„è¦éš¾ä¸€äº›ï¼Œæˆ‘ä»¬å°† LLM å¼•æ“Žåˆ‡æ¢åˆ°æ›´å¼ºå¤§çš„ [Qwen/Qwen3-Next-80B-A3B-Thinking](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking)ï¼

```py
sql_engine.description = updated_description

agent = CodeAgent(
    tools=[sql_engine],
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
)

agent.run("Which waiter got more total money from tips?")
```

å®ƒç›´æŽ¥å°±èƒ½å·¥ä½œï¼è®¾ç½®è¿‡ç¨‹éžå¸¸ç®€å•ï¼Œéš¾é“ä¸æ˜¯å—ï¼Ÿ

è¿™ä¸ªä¾‹å­åˆ°æ­¤ç»“æŸï¼æˆ‘ä»¬æ¶µç›–äº†è¿™äº›æ¦‚å¿µï¼š

- æž„å»ºæ–°å·¥å…·ã€‚
- æ›´æ–°å·¥å…·çš„æè¿°ã€‚
- åˆ‡æ¢åˆ°æ›´å¼ºå¤§çš„ LLM æœ‰åŠ©äºŽ agent æŽ¨ç†ã€‚

âœ… çŽ°åœ¨ä½ å¯ä»¥æž„å»ºä½ ä¸€ç›´æ¢¦å¯ä»¥æ±‚çš„ text-to-SQL ç³»ç»Ÿäº†ï¼âœ¨



================================================
FILE: docs/source/zh/examples/web_browser.md
================================================
# ä½¿ç”¨Agentå®žçŽ°ç½‘é¡µæµè§ˆå™¨è‡ªåŠ¨åŒ– ðŸ¤–ðŸŒ

[[open-in-colab]]

åœ¨æœ¬notebookä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª**åŸºäºŽAgentçš„ç½‘é¡µæµè§ˆå™¨è‡ªåŠ¨åŒ–ç³»ç»Ÿ**ï¼è¯¥ç³»ç»Ÿå¯ä»¥è‡ªåŠ¨å¯¼èˆªç½‘ç«™ã€ä¸Žç½‘é¡µå…ƒç´ äº¤äº’å¹¶æå–ä¿¡æ¯ã€‚

è¯¥Agentå°†èƒ½å¤Ÿï¼š

- [x] å¯¼èˆªåˆ°ç½‘é¡µ
- [x] ç‚¹å‡»å…ƒç´ 
- [x] åœ¨é¡µé¢å†…æœç´¢
- [x] å¤„ç†å¼¹å‡ºçª—å£å’Œæ¨¡æ€æ¡†
- [x] æå–ä¿¡æ¯

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ­å»ºè¿™ä¸ªç³»ç»Ÿï¼

é¦–å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€ä¾èµ–ï¼š

```bash
pip install smolagents selenium helium pillow -q
```

è®©æˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„åº“å¹¶è®¾ç½®çŽ¯å¢ƒå˜é‡ï¼š

```python
from io import BytesIO
from time import sleep

import helium
from dotenv import load_dotenv
from PIL import Image
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from smolagents import CodeAgent, tool
from smolagents.agents import ActionStep

# Load environment variables
load_dotenv()
```

çŽ°åœ¨æˆ‘ä»¬æ¥åˆ›å»ºæ ¸å¿ƒçš„æµè§ˆå™¨äº¤äº’å·¥å…·ï¼Œä½¿æˆ‘ä»¬çš„Agentèƒ½å¤Ÿå¯¼èˆªå¹¶ä¸Žç½‘é¡µäº¤äº’ï¼š

```python
@tool
def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
    """
    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.
    Args:
        text: The text to search for
        nth_result: Which occurrence to jump to (default: 1)
    """
    elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{text}')]")
    if nth_result > len(elements):
        raise Exception(f"Match nÂ°{nth_result} not found (only {len(elements)} matches found)")
    result = f"Found {len(elements)} matches for '{text}'."
    elem = elements[nth_result - 1]
    driver.execute_script("arguments[0].scrollIntoView(true);", elem)
    result += f"Focused on element {nth_result} of {len(elements)}"
    return result

@tool
def go_back() -> None:
    """Goes back to previous page."""
    driver.back()

@tool
def close_popups() -> str:
    """
    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows!
    This does not work on cookie consent banners.
    """
    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()
```

è®©æˆ‘ä»¬é…ç½®ä½¿ç”¨Chromeæµè§ˆå™¨å¹¶è®¾ç½®æˆªå›¾åŠŸèƒ½ï¼š

```python
# Configure Chrome options
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--force-device-scale-factor=1")
chrome_options.add_argument("--window-size=1000,1350")
chrome_options.add_argument("--disable-pdf-viewer")
chrome_options.add_argument("--window-position=0,0")

# Initialize the browser
driver = helium.start_chrome(headless=False, options=chrome_options)

# Set up screenshot callback
def save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    driver = helium.get_driver()
    current_step = memory_step.step_number
    if driver is not None:
        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots for lean processing
            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:
                previous_memory_step.observations_images = None
        png_bytes = driver.get_screenshot_as_png()
        image = Image.open(BytesIO(png_bytes))
        print(f"Captured a browser screenshot: {image.size} pixels")
        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists

    # Update observations with current URL
    url_info = f"Current url: {driver.current_url}"
    memory_step.observations = (
        url_info if memory_step.observations is None else memory_step.observations + "\n" + url_info
    )
```

çŽ°åœ¨æˆ‘ä»¬æ¥åˆ›å»ºç½‘é¡µè‡ªåŠ¨åŒ–Agentï¼š

```python
from smolagents import InferenceClientModel

# Initialize the model
model_id = "meta-llama/Llama-3.3-70B-Instruct"  # You can change this to your preferred model
model = InferenceClientModel(model_id=model_id)

# Create the agent
agent = CodeAgent(
    tools=[go_back, close_popups, search_item_ctrl_f],
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[save_screenshot],
    max_steps=20,
    verbosity_level=2,
)

# Import helium for the agent
agent.python_executor("from helium import *", agent.state)
```

Agentéœ€è¦èŽ·å¾—å…³äºŽå¦‚ä½•ä½¿ç”¨Heliumè¿›è¡Œç½‘é¡µè‡ªåŠ¨åŒ–çš„æŒ‡å¯¼ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†æä¾›çš„æ“ä½œè¯´æ˜Žï¼š

```python
helium_instructions = """
You can use helium to access websites. Don't bother about the helium driver, it's already managed.
We've already ran "from helium import *"
Then you can go to pages!
Code:
```py
go_to('github.com/trending')
```<end_code>

You can directly click clickable elements by inputting the text that appears on them.
Code:
```py
click("Top products")
```<end_code>

If it's a link:
Code:
```py
click(Link("Top products"))
```<end_code>

If you try to interact with an element and it's not found, you'll get a LookupError.
In general stop your action after each button click to see what happens on your screenshot.
Never try to login in a page.

To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
Code:
```py
scroll_down(num_pixels=1200) # This will scroll one viewport down
```<end_code>

When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
Just use your built-in tool `close_popups` to close them:
Code:
```py
close_popups()
```<end_code>

You can use .exists() to check for the existence of an element. For example:
Code:
```py
if Text('Accept cookies?').exists():
    click('I accept')
```<end_code>
"""
```

çŽ°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡ŒAgentæ‰§è¡Œä»»åŠ¡äº†ï¼è®©æˆ‘ä»¬å°è¯•åœ¨ç»´åŸºç™¾ç§‘ä¸ŠæŸ¥æ‰¾ä¿¡æ¯ï¼š

```python
search_request = """
Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
"""

agent_output = agent.run(search_request + helium_instructions)
print("Final output:")
print(agent_output)
```

æ‚¨å¯ä»¥é€šè¿‡ä¿®æ”¹è¯·æ±‚å‚æ•°æ‰§è¡Œä¸åŒä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹è¯·æ±‚å¯å¸®åŠ©æˆ‘åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´åŠ åŠªåŠ›å·¥ä½œï¼š

```python
github_request = """
I'm trying to find how hard I have to work to get a repo in github.com/trending.
Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
"""

agent_output = agent.run(github_request + helium_instructions)
print("Final output:")
print(agent_output)
```

è¯¥ç³»ç»Ÿåœ¨ä»¥ä¸‹ä»»åŠ¡ä¸­å°¤ä¸ºæœ‰æ•ˆï¼š

- ä»Žç½‘ç«™æå–æ•°æ®
- ç½‘é¡µç ”ç©¶è‡ªåŠ¨åŒ–
- ç”¨æˆ·ç•Œé¢æµ‹è¯•ä¸ŽéªŒè¯
- å†…å®¹ç›‘æŽ§


================================================
FILE: docs/source/zh/reference/agents.md
================================================
# Agentsï¼ˆæ™ºèƒ½ä½“ï¼‰

<Tip warning={true}>

Smolagents æ˜¯ä¸€ä¸ªå®žéªŒæ€§çš„ APIï¼Œå¯èƒ½ä¼šéšæ—¶å‘ç”Ÿå˜åŒ–ã€‚ç”±äºŽ API æˆ–åº•å±‚æ¨¡åž‹å¯èƒ½å‘ç”Ÿå˜åŒ–ï¼Œä»£ç†è¿”å›žçš„ç»“æžœä¹Ÿå¯èƒ½æœ‰æ‰€ä¸åŒã€‚

</Tip>

è¦äº†è§£æœ‰å…³æ™ºèƒ½ä½“å’Œå·¥å…·çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·åŠ¡å¿…é˜…è¯»[å…¥é—¨æŒ‡å—](../index)ã€‚æœ¬é¡µé¢åŒ…å«åŸºç¡€ç±»çš„ API æ–‡æ¡£ã€‚

## æ™ºèƒ½ä½“ï¼ˆAgentsï¼‰

æˆ‘ä»¬çš„æ™ºèƒ½ä½“ç»§æ‰¿è‡ª [`MultiStepAgent`]ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥æ‰§è¡Œå¤šæ­¥æ“ä½œï¼Œæ¯ä¸€æ­¥åŒ…å«ä¸€ä¸ªæ€è€ƒï¼ˆthoughtï¼‰ï¼Œç„¶åŽæ˜¯ä¸€ä¸ªå·¥å…·è°ƒç”¨å’Œæ‰§è¡Œã€‚è¯·é˜…è¯»[æ¦‚å¿µæŒ‡å—](../conceptual_guides/react)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚

æˆ‘ä»¬æä¾›ä¸¤ç§ç±»åž‹çš„ä»£ç†ï¼Œå®ƒä»¬åŸºäºŽä¸»è¦çš„ [`Agent`] ç±»ï¼š
  - [`CodeAgent`] æ˜¯é»˜è®¤ä»£ç†ï¼Œå®ƒä»¥ Python ä»£ç ç¼–å†™å·¥å…·è°ƒç”¨ã€‚
  - [`ToolCallingAgent`] ä»¥ JSON ç¼–å†™å·¥å…·è°ƒç”¨ã€‚

ä¸¤è€…åœ¨åˆå§‹åŒ–æ—¶éƒ½éœ€è¦æä¾›å‚æ•° `model` å’Œå·¥å…·åˆ—è¡¨ `tools`ã€‚

### æ™ºèƒ½ä½“ç±»

[[autodoc]] MultiStepAgent

[[autodoc]] CodeAgent

[[autodoc]] ToolCallingAgent

### stream_to_gradio

[[autodoc]] stream_to_gradio

### GradioUI

> [!TIP]
> æ‚¨å¿…é¡»å®‰è£… `gradio` æ‰èƒ½ä½¿ç”¨ UIã€‚å¦‚æžœå°šæœªå®‰è£…ï¼Œè¯·è¿è¡Œ `pip install 'smolagents[gradio]'`ã€‚

[[autodoc]] GradioUI

## æç¤ºï¼ˆPromptsï¼‰

[[autodoc]] smolagents.agents.PromptTemplates

[[autodoc]] smolagents.agents.PlanningPromptTemplate

[[autodoc]] smolagents.agents.ManagedAgentPromptTemplate

[[autodoc]] smolagents.agents.FinalAnswerPromptTemplate



================================================
FILE: docs/source/zh/reference/models.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/reference/tools.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/tutorials/building_good_agents.md
================================================
# æž„å»ºå¥½ç”¨çš„ agent

[[open-in-colab]]

èƒ½è‰¯å¥½å·¥ä½œçš„ agent å’Œä¸èƒ½å·¥ä½œçš„ agent ä¹‹é—´ï¼Œæœ‰å¤©å£¤ä¹‹åˆ«ã€‚
æˆ‘ä»¬æ€Žä¹ˆæ ·æ‰èƒ½æž„å»ºå‡ºå±žäºŽå‰è€…çš„ agent å‘¢ï¼Ÿ
åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æž„å»º agent çš„æœ€ä½³å®žè·µã€‚

> [!TIP]
> å¦‚æžœä½ æ˜¯ agent æž„å»ºçš„æ–°æ‰‹ï¼Œè¯·ç¡®ä¿é¦–å…ˆé˜…è¯» [agent ä»‹ç»](../conceptual_guides/intro_agents) å’Œ [smolagents å¯¼è§ˆ](../guided_tour)ã€‚

### æœ€å¥½çš„ agent ç³»ç»Ÿæ˜¯æœ€ç®€å•çš„ï¼šå°½å¯èƒ½ç®€åŒ–å·¥ä½œæµ

åœ¨ä½ çš„å·¥ä½œæµä¸­èµ‹äºˆ LLM ä¸€äº›è‡ªä¸»æƒï¼Œä¼šå¼•å…¥ä¸€äº›é”™è¯¯é£Žé™©ã€‚

ç»è¿‡è‰¯å¥½ç¼–ç¨‹çš„ agent ç³»ç»Ÿï¼Œé€šå¸¸å…·æœ‰è‰¯å¥½çš„é”™è¯¯æ—¥å¿—è®°å½•å’Œé‡è¯•æœºåˆ¶ï¼Œå› æ­¤ LLM å¼•æ“Žæœ‰æœºä¼šè‡ªæˆ‘çº é”™ã€‚ä½†ä¸ºäº†æœ€å¤§é™åº¦åœ°é™ä½Ž LLM é”™è¯¯çš„é£Žé™©ï¼Œä½ åº”è¯¥ç®€åŒ–ä½ çš„å·¥ä½œæµï¼

è®©æˆ‘ä»¬å›žé¡¾ä¸€ä¸‹ [agent ä»‹ç»](../conceptual_guides/intro_agents) ä¸­çš„ä¾‹å­ï¼šä¸€ä¸ªä¸ºå†²æµªæ—…è¡Œå…¬å¸å›žç­”ç”¨æˆ·å’¨è¯¢çš„æœºå™¨äººã€‚
ä¸Žå…¶è®© agent æ¯æ¬¡è¢«é—®åŠæ–°çš„å†²æµªåœ°ç‚¹æ—¶ï¼Œéƒ½åˆ†åˆ«è°ƒç”¨ "æ—…è¡Œè·ç¦» API" å’Œ "å¤©æ°” API"ï¼Œä½ å¯ä»¥åªåˆ›å»ºä¸€ä¸ªç»Ÿä¸€çš„å·¥å…· "return_spot_information"ï¼Œä¸€ä¸ªåŒæ—¶è°ƒç”¨è¿™ä¸¤ä¸ª APIï¼Œå¹¶è¿”å›žå®ƒä»¬è¿žæŽ¥è¾“å‡ºçš„å‡½æ•°ã€‚

è¿™å¯ä»¥é™ä½Žæˆæœ¬ã€å»¶è¿Ÿå’Œé”™è¯¯é£Žé™©ï¼

ä¸»è¦çš„æŒ‡å¯¼åŽŸåˆ™æ˜¯ï¼šå°½å¯èƒ½å‡å°‘ LLM è°ƒç”¨çš„æ¬¡æ•°ã€‚

è¿™å¯ä»¥å¸¦æ¥ä¸€äº›å¯å‘ï¼š
- å°½å¯èƒ½æŠŠä¸¤ä¸ªå·¥å…·åˆå¹¶ä¸ºä¸€ä¸ªï¼Œå°±åƒæˆ‘ä»¬ä¸¤ä¸ª API çš„ä¾‹å­ã€‚
- å°½å¯èƒ½åŸºäºŽç¡®å®šæ€§å‡½æ•°ï¼Œè€Œä¸æ˜¯ agent å†³ç­–ï¼Œæ¥å®žçŽ°é€»è¾‘ã€‚

### æ”¹å–„æµå‘ LLM å¼•æ“Žçš„ä¿¡æ¯æµ

è®°ä½ï¼Œä½ çš„ LLM å¼•æ“Žå°±åƒä¸€ä¸ª ~æ™ºèƒ½~ æœºå™¨äººï¼Œè¢«å…³åœ¨ä¸€ä¸ªæˆ¿é—´é‡Œï¼Œä¸Žå¤–ç•Œå”¯ä¸€çš„äº¤æµæ–¹å¼æ˜¯é€šè¿‡é—¨ç¼ä¼ é€’çš„çº¸æ¡ã€‚

å¦‚æžœä½ æ²¡æœ‰æ˜Žç¡®åœ°å°†ä¿¡æ¯æ”¾å…¥å…¶æç¤ºä¸­ï¼Œå®ƒå°†ä¸çŸ¥é“å‘ç”Ÿçš„ä»»ä½•äº‹æƒ…ã€‚

æ‰€ä»¥é¦–å…ˆè¦è®©ä½ çš„ä»»åŠ¡éžå¸¸æ¸…æ™°ï¼
ç”±äºŽ agent ç”± LLM é©±åŠ¨ï¼Œä»»åŠ¡è¡¨è¿°çš„å¾®å°å˜åŒ–å¯èƒ½ä¼šäº§ç”Ÿå®Œå…¨ä¸åŒçš„ç»“æžœã€‚

ç„¶åŽï¼Œæ”¹å–„å·¥å…·ä½¿ç”¨ä¸­æµå‘ agent çš„ä¿¡æ¯æµã€‚

éœ€è¦éµå¾ªçš„å…·ä½“æŒ‡å—ï¼š
- æ¯ä¸ªå·¥å…·éƒ½åº”è¯¥è®°å½•ï¼ˆåªéœ€åœ¨å·¥å…·çš„ `forward` æ–¹æ³•ä¸­ä½¿ç”¨ `print` è¯­å¥ï¼‰å¯¹ LLM å¼•æ“Žå¯èƒ½æœ‰ç”¨çš„æ‰€æœ‰ä¿¡æ¯ã€‚
  - ç‰¹åˆ«æ˜¯ï¼Œè®°å½•å·¥å…·æ‰§è¡Œé”™è¯¯çš„è¯¦ç»†ä¿¡æ¯ä¼šå¾ˆæœ‰å¸®åŠ©ï¼

ä¾‹å¦‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªæ ¹æ®ä½ç½®å’Œæ—¥æœŸæ—¶é—´æ£€ç´¢å¤©æ°”æ•°æ®çš„å·¥å…·ï¼š

é¦–å…ˆï¼Œè¿™æ˜¯ä¸€ä¸ªç³Ÿç³•çš„ç‰ˆæœ¬ï¼š
```python
import datetime
from smolagents import tool

def get_weather_report_at_coordinates(coordinates, date_time):
    # è™šæ‹Ÿå‡½æ•°ï¼Œè¿”å›ž [æ¸©åº¦ï¼ˆÂ°Cï¼‰ï¼Œé™é›¨é£Žé™©ï¼ˆ0-1ï¼‰ï¼Œæµªé«˜ï¼ˆmï¼‰]
    return [28.0, 0.35, 0.85]

def get_coordinates_from_location(location):
    # è¿”å›žè™šæ‹Ÿåæ ‡
    return [3.3, -42.0]

@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for.
        date_time: the date and time for which you want the report.
    """
    lon, lat = convert_location_to_coordinates(location)
    date_time = datetime.strptime(date_time)
    return str(get_weather_report_at_coordinates((lon, lat), date_time))
```

ä¸ºä»€ä¹ˆå®ƒä¸å¥½ï¼Ÿ
- æ²¡æœ‰è¯´æ˜Ž `date_time` åº”è¯¥ä½¿ç”¨çš„æ ¼å¼
- æ²¡æœ‰è¯´æ˜Žä½ç½®åº”è¯¥å¦‚ä½•æŒ‡å®š
- æ²¡æœ‰è®°å½•æœºåˆ¶æ¥å¤„ç†æ˜Žç¡®çš„æŠ¥é”™æƒ…å†µï¼Œå¦‚ä½ç½®æ ¼å¼ä¸æ­£ç¡®æˆ– date_time æ ¼å¼ä¸æ­£ç¡®
- è¾“å‡ºæ ¼å¼éš¾ä»¥ç†è§£

å¦‚æžœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå†…å­˜ä¸­è®°å½•çš„é”™è¯¯è·Ÿè¸ªï¼Œå¯ä»¥å¸®åŠ© LLM é€†å‘å·¥ç¨‹å·¥å…·æ¥ä¿®å¤é”™è¯¯ã€‚ä½†ä¸ºä»€ä¹ˆè¦è®©å®ƒåšè¿™ä¹ˆå¤šç¹é‡çš„å·¥ä½œå‘¢ï¼Ÿ

æž„å»ºè¿™ä¸ªå·¥å…·çš„æ›´å¥½æ–¹å¼å¦‚ä¸‹ï¼š
```python
@tool
def get_weather_api(location: str, date_time: str) -> str:
    """
    Returns the weather report.

    Args:
        location: the name of the place that you want the weather for. Should be a place name, followed by possibly a city name, then a country, like "Anchor Point, Taghazout, Morocco".
        date_time: the date and time for which you want the report, formatted as '%m/%d/%y %H:%M:%S'.
    """
    lon, lat = convert_location_to_coordinates(location)
    try:
        date_time = datetime.strptime(date_time)
    except Exception as e:
        raise ValueError("Conversion of `date_time` to datetime format failed, make sure to provide a string in format '%m/%d/%y %H:%M:%S'. Full trace:" + str(e))
    temperature_celsius, risk_of_rain, wave_height = get_weather_report_at_coordinates((lon, lat), date_time)
    return f"Weather report for {location}, {date_time}: Temperature will be {temperature_celsius}Â°C, risk of rain is {risk_of_rain*100:.0f}%, wave height is {wave_height}m."
```

ä¸€èˆ¬æ¥è¯´ï¼Œä¸ºäº†å‡è½» LLM çš„è´Ÿæ‹…ï¼Œè¦é—®è‡ªå·±çš„å¥½é—®é¢˜æ˜¯ï¼š"å¦‚æžœæˆ‘æ˜¯ä¸€ä¸ªç¬¬ä¸€æ¬¡ä½¿ç”¨è¿™ä¸ªå·¥å…·çš„å‚»ç“œï¼Œä½¿ç”¨è¿™ä¸ªå·¥å…·ç¼–ç¨‹å¹¶çº æ­£è‡ªå·±çš„é”™è¯¯æœ‰å¤šå®¹æ˜“ï¼Ÿ"ã€‚

### ç»™ agent æ›´å¤šå‚æ•°

é™¤äº†ç®€å•çš„ä»»åŠ¡æè¿°å­—ç¬¦ä¸²å¤–ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨ `additional_args` å‚æ•°ä¼ é€’ä»»ä½•ç±»åž‹çš„å¯¹è±¡ï¼š

```py
from smolagents import CodeAgent, InferenceClientModel

model_id = "meta-llama/Llama-3.3-70B-Instruct"

agent = CodeAgent(tools=[], model=InferenceClientModel(model_id=model_id), add_base_tools=True)

agent.run(
    "Why does Mike not know many people in New York?",
    additional_args={"mp3_sound_file_url":'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3'}
)
```
ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ª `additional_args` å‚æ•°ä¼ é€’ä½ å¸Œæœ› agent åˆ©ç”¨çš„å›¾åƒæˆ–å­—ç¬¦ä¸²ã€‚


## å¦‚ä½•è°ƒè¯•ä½ çš„ agent

### 1. ä½¿ç”¨æ›´å¼ºå¤§çš„ LLM

åœ¨ agent å·¥ä½œæµä¸­ï¼Œæœ‰äº›é”™è¯¯æ˜¯å®žé™…é”™è¯¯ï¼Œæœ‰äº›åˆ™æ˜¯ä½ çš„ LLM å¼•æ“Žæ²¡æœ‰æ­£ç¡®æŽ¨ç†çš„ç»“æžœã€‚
ä¾‹å¦‚ï¼Œå‚è€ƒè¿™ä¸ªæˆ‘è¦æ±‚åˆ›å»ºä¸€ä¸ªæ±½è½¦å›¾ç‰‡çš„ `CodeAgent` çš„è¿è¡Œè®°å½•ï¼š
```text
==================================================================================================== New task ====================================================================================================
Make me a cool car picture
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
image_generator(prompt="A cool, futuristic sports car with LED headlights, aerodynamic design, and vibrant color, high-res, photorealistic")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Step 1:

- Time taken: 16.35 seconds
- Input tokens: 1,383
- Output tokens: 77
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New step â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent is executing the code below: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
final_answer("/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png")
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Print outputs:

Last output from code snippet: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
Final answer:
/var/folders/6m/9b1tts6d5w960j80wbw9tx3m0000gn/T/tmpx09qfsdd/652f0007-3ee9-44e2-94ac-90dae6bb89a4.png
```
ç”¨æˆ·çœ‹åˆ°çš„æ˜¯è¿”å›žäº†ä¸€ä¸ªè·¯å¾„ï¼Œè€Œä¸æ˜¯å›¾åƒã€‚
è¿™çœ‹èµ·æ¥åƒæ˜¯ç³»ç»Ÿçš„é”™è¯¯ï¼Œä½†å®žé™…ä¸Š agent ç³»ç»Ÿå¹¶æ²¡æœ‰å¯¼è‡´é”™è¯¯ï¼šåªæ˜¯ LLM å¤§è„‘çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œæ²¡æœ‰æŠŠå›¾åƒè¾“å‡ºï¼Œä¿å­˜åˆ°å˜é‡ä¸­ã€‚
å› æ­¤ï¼Œå®ƒæ— æ³•å†æ¬¡è®¿é—®å›¾åƒï¼Œåªèƒ½åˆ©ç”¨ä¿å­˜å›¾åƒæ—¶è®°å½•çš„è·¯å¾„ï¼Œæ‰€ä»¥å®ƒè¿”å›žçš„æ˜¯è·¯å¾„ï¼Œè€Œä¸æ˜¯å›¾åƒã€‚

è°ƒè¯• agent çš„ç¬¬ä¸€æ­¥æ˜¯"ä½¿ç”¨æ›´å¼ºå¤§çš„ LLM"ã€‚åƒ `Qwen2.5-72B-Instruct` è¿™æ ·çš„æ›¿ä»£æ–¹æ¡ˆä¸ä¼šçŠ¯è¿™ç§é”™è¯¯ã€‚

### 2. æä¾›æ›´å¤šæŒ‡å¯¼/æ›´å¤šä¿¡æ¯

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸å¤ªå¼ºå¤§çš„æ¨¡åž‹ï¼Œåªè¦ä½ æ›´æœ‰æ•ˆåœ°æŒ‡å¯¼å®ƒä»¬ã€‚

ç«™åœ¨æ¨¡åž‹çš„è§’åº¦æ€è€ƒï¼šå¦‚æžœä½ æ˜¯æ¨¡åž‹åœ¨è§£å†³ä»»åŠ¡ï¼Œä½ ä¼šå› ä¸ºç³»ç»Ÿæç¤º+ä»»åŠ¡è¡¨è¿°+å·¥å…·æè¿°ä¸­æä¾›çš„ä¿¡æ¯è€ŒæŒ£æ‰Žå—ï¼Ÿ

ä½ éœ€è¦ä¸€äº›é¢å¤–çš„è¯´æ˜Žå—ï¼Ÿ

ä¸ºäº†æä¾›é¢å¤–ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¸å»ºè®®ç«‹å³æ›´æ”¹ç³»ç»Ÿæç¤ºï¼šé»˜è®¤ç³»ç»Ÿæç¤ºæœ‰è®¸å¤šè°ƒæ•´ï¼Œé™¤éžä½ éžå¸¸äº†è§£æç¤ºï¼Œå¦åˆ™ä½ å¾ˆå®¹æ˜“ç¿»è½¦ã€‚
æ›´å¥½çš„æŒ‡å¯¼ LLM å¼•æ“Žçš„æ–¹æ³•æ˜¯ï¼š
- å¦‚æžœæ˜¯å…³äºŽè¦è§£å†³çš„ä»»åŠ¡ï¼šæŠŠæ‰€æœ‰ç»†èŠ‚æ·»åŠ åˆ°ä»»åŠ¡ä¸­ã€‚ä»»åŠ¡å¯ä»¥æœ‰å‡ ç™¾é¡µé•¿ã€‚
- å¦‚æžœæ˜¯å…³äºŽå¦‚ä½•ä½¿ç”¨å·¥å…·ï¼šä½ çš„å·¥å…·çš„ description å±žæ€§ã€‚


### 3. æ›´æ”¹ç³»ç»Ÿæç¤ºï¼ˆé€šå¸¸ä¸å»ºè®®ï¼‰

å¦‚æžœä¸Šè¿°è¯´æ˜Žä¸å¤Ÿï¼Œä½ å¯ä»¥æ›´æ”¹ç³»ç»Ÿæç¤ºã€‚

è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ [`CodeAgent`] çš„é»˜è®¤ç³»ç»Ÿæç¤ºï¼ˆä¸‹é¢çš„ç‰ˆæœ¬é€šè¿‡è·³è¿‡é›¶æ ·æœ¬ç¤ºä¾‹è¿›è¡Œäº†ç¼©çŸ­ï¼‰ã€‚

```python
print(agent.prompt_templates["system_prompt"])
```
ä½ ä¼šå¾—åˆ°ï¼š
```text
You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.

At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '<end_code>' sequence.
During each intermediate step, you can use 'print()' to save whatever important information you will then need.
These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
In the end you have to return a final answer using the `final_answer` tool.

Here are a few examples using notional tools:
---
Task: "Generate an image of the oldest person in this document."

Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.
Code:
```py
answer = document_qa(document=document, question="Who is the oldest person mentioned?")
print(answer)
```<end_code>
Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

Thought: I will now generate an image showcasing the oldest person.
Code:
```py
image = image_generator("A portrait of John Doe, a 55-year-old man living in Canada.")
final_answer(image)
```<end_code>

---
Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

Thought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool
Code:
```py
result = 5 + 3 + 1294.678
final_answer(result)
```<end_code>

---
Task:
"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.
You have been provided with these additional arguments, that you can access using the keys as variables in your python code:
{'question': 'Quel est l'animal sur l'image?', 'image': 'path/to/image.jpg'}"

Thought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.
Code:
```py
translated_question = translator(question=question, src_lang="French", tgt_lang="English")
print(f"The translated question is {translated_question}.")
answer = image_qa(image=image, question=translated_question)
final_answer(f"The answer is {answer}")
```<end_code>

---
Task:
In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.
What does he say was the consequence of Einstein learning too much math on his creativity, in one word?

Thought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.
Code:
```py
pages = search(query="1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein")
print(pages)
```<end_code>
Observation:
No result found for query "1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein".

Thought: The query was maybe too restrictive and did not find any results. Let's try again with a broader query.
Code:
```py
pages = search(query="1979 interview Stanislaus Ulam")
print(pages)
```<end_code>
Observation:
Found 6 pages:
[Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)

[Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)

(truncated)

Thought: I will read the first 2 pages to know more.
Code:
```py
for url in ["https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/", "https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/"]:
    whole_page = visit_webpage(url)
    print(whole_page)
    print("\n" + "="*80 + "\n")  # Print separator between pages
```<end_code>
Observation:
Manhattan Project Locations:
Los Alamos, NM
Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at
(truncated)

Thought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: "He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity." Let's answer in one word.
Code:
```py
final_answer("diminished")
```<end_code>

---
Task: "Which city has the highest population: Guangzhou or Shanghai?"

Thought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.
Code:
```py
for city in ["Guangzhou", "Shanghai"]:
    print(f"Population {city}:", search(f"{city} population")
```<end_code>
Observation:
Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
Population Shanghai: '26 million (2019)'

Thought: Now I know that Shanghai has the highest population.
Code:
```py
final_answer("Shanghai")
```<end_code>

---
Task: "What is the current age of the pope, raised to the power 0.36?"

Thought: I will use the tool `wiki` to get the age of the pope, and confirm that with a web search.
Code:
```py
pope_age_wiki = wiki(query="current pope age")
print("Pope age as per wikipedia:", pope_age_wiki)
pope_age_search = web_search(query="current pope age")
print("Pope age as per google search:", pope_age_search)
```<end_code>
Observation:
Pope age: "The pope Francis is currently 88 years old."

Thought: I know that the pope is 88 years old. Let's compute the result using python code.
Code:
```py
pope_current_age = 88 ** 0.36
final_answer(pope_current_age)
```<end_code>

Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools:
{%- for tool in tools.values() %}
- {{ tool.to_tool_calling_prompt() }}
{%- endfor %}

{%- if managed_agents and managed_agents.values() | list %}
You can also give tasks to team members.
Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
You can also include any relevant variables or context using the 'additional_args' argument.
Here is a list of the team members that you can call:
{%- for agent in managed_agents.values() %}
- {{ agent.name }}: {{ agent.description }}
{%- endfor %}
{%- endif %}

Here are the rules you should always follow to solve your task:
1. Always provide a 'Thought:' sequence, and a 'Code:\n```py' sequence ending with '```<end_code>' sequence, else you will fail.
2. Use only variables that you have defined!
3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wiki({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = wiki(query="What is the place where James Bond lives?")'.
4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.
5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.
6. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.
7. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.
8. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}
9. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.
10. Don't give up! You're in charge of solving the task, not providing directions to solve it.

Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.
```

å¦‚ä½ æ‰€è§ï¼Œæœ‰ä¸€äº›å ä½ç¬¦ï¼Œå¦‚ `"{{ tool.description }}"`ï¼šè¿™äº›å°†åœ¨ agent åˆå§‹åŒ–æ—¶ç”¨äºŽæ’å…¥æŸäº›è‡ªåŠ¨ç”Ÿæˆçš„å·¥å…·æˆ–ç®¡ç† agent çš„æè¿°ã€‚

å› æ­¤ï¼Œè™½ç„¶ä½ å¯ä»¥é€šè¿‡å°†è‡ªå®šä¹‰æç¤ºä½œä¸ºå‚æ•°ä¼ é€’ç»™ `system_prompt` å‚æ•°æ¥è¦†ç›–æ­¤ç³»ç»Ÿæç¤ºæ¨¡æ¿ï¼Œä½†ä½ çš„æ–°ç³»ç»Ÿæç¤ºå¿…é¡»åŒ…å«ä»¥ä¸‹å ä½ç¬¦ï¼š
- ç”¨äºŽæ’å…¥å·¥å…·æè¿°ã€‚
  ```
  {%- for tool in tools.values() %}
  - {{ tool.to_tool_calling_prompt() }}
  {%- endfor %}
  ```
- ç”¨äºŽæ’å…¥ managed agent çš„æè¿°ï¼ˆå¦‚æžœæœ‰ï¼‰ã€‚
  ```
  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  {%- for agent in managed_agents.values() %}
  - {{ agent.name }}: {{ agent.description }}
  {%- endfor %}
  {%- endif %}
  ```
- ä»…é™ `CodeAgent`ï¼š`"{{authorized_imports}}"` ç”¨äºŽæ’å…¥æŽˆæƒå¯¼å…¥åˆ—è¡¨ã€‚

ç„¶åŽä½ å¯ä»¥æ ¹æ®å¦‚ä¸‹ï¼Œæ›´æ”¹ç³»ç»Ÿæç¤ºï¼š

```py
agent.prompt_templates["system_prompt"] = agent.prompt_templates["system_prompt"] + "\nHere you go!"
```

è¿™ä¹Ÿé€‚ç”¨äºŽ [`ToolCallingAgent`]ã€‚


### 4. é¢å¤–è§„åˆ’

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç”¨äºŽè¡¥å……è§„åˆ’æ­¥éª¤çš„æ¨¡åž‹ï¼Œagent å¯ä»¥åœ¨æ­£å¸¸æ“ä½œæ­¥éª¤ä¹‹é—´å®šæœŸè¿è¡Œã€‚åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼ŒLLM åªæ˜¯è¢«è¦æ±‚æ›´æ–°å®ƒçŸ¥é“çš„äº‹å®žåˆ—è¡¨ï¼Œå¹¶æ ¹æ®è¿™äº›äº‹å®žåæŽ¨å®ƒåº”è¯¥é‡‡å–çš„ä¸‹ä¸€æ­¥ã€‚

```py
from smolagents import load_tool, CodeAgent, InferenceClientModel, WebSearchTool
from dotenv import load_dotenv

load_dotenv()

# ä»Ž Hub å¯¼å…¥å·¥å…·
image_generation_tool = load_tool("m-ric/text-to-image", trust_remote_code=True)

search_tool = WebSearchTool()

agent = CodeAgent(
    tools=[search_tool],
    model=InferenceClientModel(model_id="Qwen/Qwen2.5-72B-Instruct"),
    planning_interval=3 # è¿™æ˜¯ä½ æ¿€æ´»è§„åˆ’çš„åœ°æ–¹ï¼
)

# è¿è¡Œå®ƒï¼
result = agent.run(
    "How long would a cheetah at full speed take to run the length of Pont Alexandre III?",
)
```


================================================
FILE: docs/source/zh/tutorials/inspect_runs.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/tutorials/memory.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/tutorials/secure_code_execution.md
================================================
[Binary file]


================================================
FILE: docs/source/zh/tutorials/tools.md
================================================
[Binary file]


================================================
FILE: examples/agent_from_any_llm.py
================================================
from smolagents import (
    CodeAgent,
    InferenceClientModel,
    LiteLLMModel,
    OpenAIModel,
    ToolCallingAgent,
    TransformersModel,
    tool,
)


# Choose which inference type to use!

available_inferences = ["inference_client", "transformers", "ollama", "litellm", "openai"]
chosen_inference = "inference_client"

print(f"Chose model: '{chosen_inference}'")

if chosen_inference == "inference_client":
    model = InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct", provider="nebius")

elif chosen_inference == "transformers":
    model = TransformersModel(model_id="HuggingFaceTB/SmolLM2-1.7B-Instruct", device_map="auto", max_new_tokens=1000)

elif chosen_inference == "ollama":
    model = LiteLLMModel(
        model_id="ollama_chat/llama3.2",
        api_base="http://localhost:11434",  # replace with remote open-ai compatible server if necessary
        api_key="your-api-key",  # replace with API key if necessary
        num_ctx=8192,  # ollama default is 2048 which will often fail horribly. 8192 works for easy tasks, more is better. Check https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator to calculate how much VRAM this will need for the selected model.
    )

elif chosen_inference == "litellm":
    # For anthropic: change model_id below to 'anthropic/claude-3-5-sonnet-latest'
    model = LiteLLMModel(model_id="gpt-4o")

elif chosen_inference == "openai":
    # For anthropic: change model_id below to 'anthropic/claude-3-5-sonnet-latest'
    model = OpenAIModel(model_id="gpt-4o")


@tool
def get_weather(location: str, celsius: bool | None = False) -> str:
    """
    Get weather in the next days at given location.
    Secretly this tool does not care about the location, it hates the weather everywhere.

    Args:
        location: the location
        celsius: the temperature
    """
    return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"


agent = ToolCallingAgent(tools=[get_weather], model=model, verbosity_level=2)

print("ToolCallingAgent:", agent.run("What's the weather like in Paris?"))

agent = CodeAgent(tools=[get_weather], model=model, verbosity_level=2, stream_outputs=True)

print("CodeAgent:", agent.run("What's the weather like in Paris?"))



================================================
FILE: examples/gradio_ui.py
================================================
from smolagents import CodeAgent, GradioUI, InferenceClientModel, WebSearchTool


agent = CodeAgent(
    tools=[WebSearchTool()],
    model=InferenceClientModel(model_id="meta-llama/Llama-3.3-70B-Instruct", provider="fireworks-ai"),
    verbosity_level=1,
    planning_interval=3,
    name="example_agent",
    description="This is an example agent.",
    step_callbacks=[],
    stream_outputs=True,
    # use_structured_outputs_internally=True,
)

GradioUI(agent, file_upload_folder="./data").launch()



================================================
FILE: examples/inspect_multiagent_run.py
================================================
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
from phoenix.otel import register


register()
SmolagentsInstrumentor().instrument(skip_dep_check=True)


from smolagents import (
    CodeAgent,
    InferenceClientModel,
    ToolCallingAgent,
    VisitWebpageTool,
    WebSearchTool,
)


# Then we run the agentic part!
model = InferenceClientModel(provider="nebius")

search_agent = ToolCallingAgent(
    tools=[WebSearchTool(), VisitWebpageTool()],
    model=model,
    name="search_agent",
    description="This is an agent that can do web search.",
    return_full_result=True,
)

manager_agent = CodeAgent(
    tools=[],
    model=model,
    managed_agents=[search_agent],
    return_full_result=True,
)
run_result = manager_agent.run(
    "If the US keeps it 2024 growth rate, how many years would it take for the GDP to double?"
)
print("Here is the token usage for the manager agent", run_result.token_usage)
print("Here are the timing informations for the manager agent:", run_result.timing)



================================================
FILE: examples/multi_llm_agent.py
================================================
import os

from smolagents import CodeAgent, LiteLLMRouterModel, WebSearchTool


# Make sure to setup the necessary environment variables!

llm_loadbalancer_model_list = [
    {
        "model_name": "model-group-1",
        "litellm_params": {
            "model": "gpt-4o-mini",
            "api_key": os.getenv("OPENAI_API_KEY"),
        },
    },
    {
        "model_name": "model-group-1",
        "litellm_params": {
            "model": "bedrock/anthropic.claude-3-sonnet-20240229-v1:0",
            "aws_access_key_id": os.getenv("AWS_ACCESS_KEY_ID"),
            "aws_secret_access_key": os.getenv("AWS_SECRET_ACCESS_KEY"),
            "aws_region_name": os.getenv("AWS_REGION"),
        },
    },
    # {
    #     "model_name": "model-group-2",
    #     "litellm_params": {
    #         "model": "bedrock/anthropic.claude-3-sonnet-20240229-v1:0",
    #         "aws_access_key_id": os.getenv("AWS_ACCESS_KEY_ID"),
    #         "aws_secret_access_key": os.getenv("AWS_SECRET_ACCESS_KEY"),
    #         "aws_region_name": os.getenv("AWS_REGION"),
    #     },
    # },
]


model = LiteLLMRouterModel(
    model_id="model-group-1",
    model_list=llm_loadbalancer_model_list,
    client_kwargs={"routing_strategy": "simple-shuffle"},
)
agent = CodeAgent(tools=[WebSearchTool()], model=model, stream_outputs=True, return_full_result=True)

full_result = agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")

print(full_result)



================================================
FILE: examples/multiple_tools.py
================================================
import requests

# from smolagents.agents import ToolCallingAgent
from smolagents import CodeAgent, InferenceClientModel, tool


# Choose which LLM engine to use!
model = InferenceClientModel()
# model = TransformersModel(model_id="meta-llama/Llama-3.2-2B-Instruct")

# For anthropic: change model_id below to 'anthropic/claude-3-5-sonnet-20240620'
# model = LiteLLMModel(model_id="gpt-5")


@tool
def get_weather(location: str, celsius: bool | None = False) -> str:
    """
    Get the current weather at the given location using the WeatherStack API.

    Args:
        location: The location (city name).
        celsius: Whether to return the temperature in Celsius (default is False, which returns Fahrenheit).

    Returns:
        A string describing the current weather at the location.
    """
    api_key = "your_api_key"  # Replace with your API key from https://weatherstack.com/
    units = "m" if celsius else "f"  # 'm' for Celsius, 'f' for Fahrenheit

    url = f"http://api.weatherstack.com/current?access_key={api_key}&query={location}&units={units}"

    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for HTTP errors

        data = response.json()

        if data.get("error"):  # Check if there's an error in the response
            return f"Error: {data['error'].get('info', 'Unable to fetch weather data.')}"

        weather = data["current"]["weather_descriptions"][0]
        temp = data["current"]["temperature"]
        temp_unit = "Â°C" if celsius else "Â°F"

        return f"The current weather in {location} is {weather} with a temperature of {temp} {temp_unit}."

    except requests.exceptions.RequestException as e:
        return f"Error fetching weather data: {str(e)}"


@tool
def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:
    """
    Converts a specified amount from one currency to another using the ExchangeRate-API.

    Args:
        amount: The amount of money to convert.
        from_currency: The currency code of the currency to convert from (e.g., 'USD').
        to_currency: The currency code of the currency to convert to (e.g., 'EUR').

    Returns:
        str: A string describing the converted amount in the target currency, or an error message if the conversion fails.

    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request to the ExchangeRate-API.
    """
    api_key = "your_api_key"  # Replace with your actual API key from https://www.exchangerate-api.com/
    url = f"https://v6.exchangerate-api.com/v6/{api_key}/latest/{from_currency}"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        exchange_rate = data["conversion_rates"].get(to_currency)

        if not exchange_rate:
            return f"Error: Unable to find exchange rate for {from_currency} to {to_currency}."

        converted_amount = amount * exchange_rate
        return f"{amount} {from_currency} is equal to {converted_amount} {to_currency}."

    except requests.exceptions.RequestException as e:
        return f"Error fetching conversion data: {str(e)}"


@tool
def get_news_headlines() -> str:
    """
    Fetches the top news headlines from the News API for the United States.
    This function makes a GET request to the News API to retrieve the top news headlines
    for the United States. It returns the titles and sources of the top 5 articles as a
    formatted string. If no articles are available, it returns a message indicating that
    no news is available. In case of a request error, it returns an error message.
    Returns:
        str: A string containing the top 5 news headlines and their sources, or an error message.
    """
    api_key = "your_api_key"  # Replace with your actual API key from https://newsapi.org/
    url = f"https://newsapi.org/v2/top-headlines?country=us&apiKey={api_key}"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        articles = data["articles"]

        if not articles:
            return "No news available at the moment."

        headlines = [f"{article['title']} - {article['source']['name']}" for article in articles[:5]]
        return "\n".join(headlines)

    except requests.exceptions.RequestException as e:
        return f"Error fetching news data: {str(e)}"


@tool
def get_joke() -> str:
    """
    Fetches a random joke from the JokeAPI.
    This function sends a GET request to the JokeAPI to retrieve a random joke.
    It handles both single jokes and two-part jokes (setup and delivery).
    If the request fails or the response does not contain a joke, an error message is returned.
    Returns:
        str: The joke as a string, or an error message if the joke could not be fetched.
    """
    url = "https://v2.jokeapi.dev/joke/Any?type=single"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        if "joke" in data:
            return data["joke"]
        elif "setup" in data and "delivery" in data:
            return f"{data['setup']} - {data['delivery']}"
        else:
            return "Error: Unable to fetch joke."

    except requests.exceptions.RequestException as e:
        return f"Error fetching joke: {str(e)}"


@tool
def get_time_in_timezone(location: str) -> str:
    """
    Fetches the current time for a given location using the World Time API.
    Args:
        location: The location for which to fetch the current time, formatted as 'Region/City'.
    Returns:
        str: A string indicating the current time in the specified location, or an error message if the request fails.
    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request.
    """
    url = f"http://worldtimeapi.org/api/timezone/{location}.json"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        current_time = data["datetime"]

        return f"The current time in {location} is {current_time}."

    except requests.exceptions.RequestException as e:
        return f"Error fetching time data: {str(e)}"


@tool
def get_random_fact() -> str:
    """
    Fetches a random fact from the "uselessfacts.jsph.pl" API.
    Returns:
        str: A string containing the random fact or an error message if the request fails.
    """
    url = "https://uselessfacts.jsph.pl/random.json?language=en"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()

        return f"Random Fact: {data['text']}"

    except requests.exceptions.RequestException as e:
        return f"Error fetching random fact: {str(e)}"


@tool
def search_wikipedia(query: str) -> str:
    """
    Fetches a summary of a Wikipedia page for a given query.
    Args:
        query: The search term to look up on Wikipedia.
    Returns:
        str: A summary of the Wikipedia page if successful, or an error message if the request fails.
    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request.
    """
    url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"

    try:
        response = requests.get(url)
        response.raise_for_status()

        data = response.json()
        title = data["title"]
        extract = data["extract"]

        return f"Summary for {title}: {extract}"

    except requests.exceptions.RequestException as e:
        return f"Error fetching Wikipedia data: {str(e)}"


# If you want to use the ToolCallingAgent instead, uncomment the following lines as they both will work

# agent = ToolCallingAgent(
#     tools=[
#         convert_currency,
#         get_weather,
#         get_news_headlines,
#         get_joke,
#         get_random_fact,
#         search_wikipedia,
#     ],
#     model=model,
# )


agent = CodeAgent(
    tools=[
        convert_currency,
        get_weather,
        get_news_headlines,
        get_joke,
        get_random_fact,
        search_wikipedia,
    ],
    model=model,
    stream_outputs=True,
)

# Uncomment the line below to run the agent with a specific query

agent.run("Convert 5000 dollars to Euros")
# agent.run("What is the weather in New York?")
# agent.run("Give me the top news headlines")
# agent.run("Tell me a joke")
# agent.run("Tell me a Random Fact")
# agent.run("who is Elon Musk?")



================================================
FILE: examples/rag.py
================================================
# from huggingface_hub import login

# login()
import datasets
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.retrievers import BM25Retriever


knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")
knowledge_base = knowledge_base.filter(lambda row: row["source"].startswith("huggingface/transformers"))

source_docs = [
    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
]

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    add_start_index=True,
    strip_whitespace=True,
    separators=["\n\n", "\n", ".", " ", ""],
)
docs_processed = text_splitter.split_documents(source_docs)

from smolagents import Tool


class RetrieverTool(Tool):
    name = "retriever"
    description = "Uses lexical search to retrieve the parts of transformers documentation that could be most relevant to answer your query."
    inputs = {
        "query": {
            "type": "string",
            "description": "The query to perform. This should be lexically close to your target documents. Use the affirmative form rather than a question.",
        }
    }
    output_type = "string"

    def __init__(self, docs, **kwargs):
        super().__init__(**kwargs)
        self.retriever = BM25Retriever.from_documents(docs, k=10)

    def forward(self, query: str) -> str:
        assert isinstance(query, str), "Your search query must be a string"

        docs = self.retriever.invoke(
            query,
        )
        return "\nRetrieved documents:\n" + "".join(
            [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
        )


from smolagents import CodeAgent, InferenceClientModel


retriever_tool = RetrieverTool(docs_processed)
agent = CodeAgent(
    tools=[retriever_tool],
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
    max_steps=4,
    verbosity_level=2,
    stream_outputs=True,
)

agent_output = agent.run("For a transformers model training, which is slower, the forward or the backward pass?")

print("Final output:")
print(agent_output)



================================================
FILE: examples/rag_using_chromadb.py
================================================
import os

import datasets
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

# from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from tqdm import tqdm
from transformers import AutoTokenizer

# from langchain_openai import OpenAIEmbeddings
from smolagents import LiteLLMModel, Tool
from smolagents.agents import CodeAgent


# from smolagents.agents import ToolCallingAgent


knowledge_base = datasets.load_dataset("m-ric/huggingface_doc", split="train")

source_docs = [
    Document(page_content=doc["text"], metadata={"source": doc["source"].split("/")[1]}) for doc in knowledge_base
]

## For your own PDFs, you can use the following code to load them into source_docs
# pdf_directory = "pdfs"
# pdf_files = [
#     os.path.join(pdf_directory, f)
#     for f in os.listdir(pdf_directory)
#     if f.endswith(".pdf")
# ]
# source_docs = []

# for file_path in pdf_files:
#     loader = PyPDFLoader(file_path)
#     docs.extend(loader.load())

text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
    AutoTokenizer.from_pretrained("thenlper/gte-small"),
    chunk_size=200,
    chunk_overlap=20,
    add_start_index=True,
    strip_whitespace=True,
    separators=["\n\n", "\n", ".", " ", ""],
)

# Split docs and keep only unique ones
print("Splitting documents...")
docs_processed = []
unique_texts = {}
for doc in tqdm(source_docs):
    new_docs = text_splitter.split_documents([doc])
    for new_doc in new_docs:
        if new_doc.page_content not in unique_texts:
            unique_texts[new_doc.page_content] = True
            docs_processed.append(new_doc)


print("Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)")
# Initialize embeddings and ChromaDB vector store
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


# embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

vector_store = Chroma.from_documents(docs_processed, embeddings, persist_directory="./chroma_db")


class RetrieverTool(Tool):
    name = "retriever"
    description = (
        "Uses semantic search to retrieve the parts of documentation that could be most relevant to answer your query."
    )
    inputs = {
        "query": {
            "type": "string",
            "description": "The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.",
        }
    }
    output_type = "string"

    def __init__(self, vector_store, **kwargs):
        super().__init__(**kwargs)
        self.vector_store = vector_store

    def forward(self, query: str) -> str:
        assert isinstance(query, str), "Your search query must be a string"
        docs = self.vector_store.similarity_search(query, k=3)
        return "\nRetrieved documents:\n" + "".join(
            [f"\n\n===== Document {str(i)} =====\n" + doc.page_content for i, doc in enumerate(docs)]
        )


retriever_tool = RetrieverTool(vector_store)

# Choose which LLM engine to use!

# from smolagents import InferenceClientModel
# model = InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking")

# from smolagents import TransformersModel
# model = TransformersModel(model_id="Qwen/Qwen3-4B-Instruct-2507")

# For anthropic: change model_id below to 'anthropic/claude-4-sonnet-latest' and also change 'os.environ.get("ANTHROPIC_API_KEY")'
model = LiteLLMModel(
    model_id="groq/openai/gpt-oss-120b",
    api_key=os.environ.get("GROQ_API_KEY"),
)

# # You can also use the ToolCallingAgent class
# agent = ToolCallingAgent(
#     tools=[retriever_tool],
#     model=model,
#     verbose=True,
# )

agent = CodeAgent(
    tools=[retriever_tool],
    model=model,
    max_steps=4,
    verbosity_level=2,
    stream_outputs=True,
)

agent_output = agent.run("How can I push a model to the Hub?")


print("Final output:")
print(agent_output)



================================================
FILE: examples/sandboxed_execution.py
================================================
from smolagents import CodeAgent, InferenceClientModel, WebSearchTool


model = InferenceClientModel()

# Blaxel executor example
with CodeAgent(tools=[WebSearchTool()], model=model, executor_type="blaxel") as agent:
    output = agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")
print("Blaxel executor result:", output)

# Docker executor example
with CodeAgent(tools=[WebSearchTool()], model=model, executor_type="docker") as agent:
    output = agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")
print("Docker executor result:", output)

# E2B executor example
with CodeAgent(tools=[WebSearchTool()], model=model, executor_type="e2b") as agent:
    output = agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")
print("E2B executor result:", output)

# Modal executor example
with CodeAgent(tools=[WebSearchTool()], model=model, executor_type="modal") as agent:
    output = agent.run("How many seconds would it take for a leopard at full speed to run through Pont des Arts?")
print("Modal executor result:", output)

# WebAssembly executor example
with CodeAgent(tools=[], model=model, executor_type="wasm") as agent:
    output = agent.run("Calculate the square root of 125.")
print("Wasm executor result:", output)
# TODO: Support tools
# with CodeAgent(tools=[VisitWebpageTool()], model=model, executor_type="wasm") as agent:
#     output = agent.run("What is the content of the Wikipedia page at https://en.wikipedia.org/wiki/Intelligent_agent?")



================================================
FILE: examples/structured_output_tool.py
================================================
# How to run with uv:
#   uv run structured_output_tool.py
#
# Modify the smolagents dependency to point to the local smolagents repo or
# remove `@ file:///<path-to-smolagents>`
#
# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "smolagents[mcp,litellm] @ file:///<path-to-smolagents>",
#   "pydantic",
# ]
# ///

from textwrap import dedent

from mcp import StdioServerParameters

from smolagents import CodeAgent, InferenceClientModel, LiteLLMModel, MCPClient  # noqa: F401


def weather_server_script() -> str:
    """Return an inline MCP server script that exposes a weather tool."""
    return dedent(
        '''
        from pydantic import BaseModel, Field
        from mcp.server.fastmcp import FastMCP

        mcp = FastMCP("Weather Service")

        class WeatherInfo(BaseModel):
            location: str = Field(description="The location name")
            temperature: float = Field(description="Temperature in Celsius")
            conditions: str = Field(description="Weather conditions")
            humidity: int = Field(description="Humidity percentage", ge=0, le=100)

        @mcp.tool(
            name="get_weather_info",
            description="Get weather information for a location as structured data.",
        )
        def get_weather_info(city: str) -> WeatherInfo:
            """Get weather information for a city."""
            return WeatherInfo(
                location=city,
                temperature=22.5,
                conditions="partly cloudy",
                humidity=65
            )

        mcp.run()
        '''
    )


def main() -> None:
    # Configure your inference model
    # model = InferenceClientModel()
    model = LiteLLMModel(
        model_id="mistral/mistral-small-latest",
        # model_id="openai/gpt-4o-mini",
    )

    # Start the Weather MCP server from an inline script in this same file
    serverparams = StdioServerParameters(command="python", args=["-c", weather_server_script()])

    # Bridge MCP tools into SmolAgents with structured outputs enabled
    with MCPClient(
        serverparams,
        structured_output=True,
    ) as tools:
        agent = CodeAgent(tools=tools, model=model)
        # Example query that encourages tool use and unit conversion
        agent.run("What is the temperature in Tokyo in Fahrenheit?")


if __name__ == "__main__":
    main()



================================================
FILE: examples/text_to_sql.py
================================================
from sqlalchemy import (
    Column,
    Float,
    Integer,
    MetaData,
    String,
    Table,
    create_engine,
    insert,
    inspect,
    text,
)


engine = create_engine("sqlite:///:memory:")
metadata_obj = MetaData()

# create city SQL table
table_name = "receipts"
receipts = Table(
    table_name,
    metadata_obj,
    Column("receipt_id", Integer, primary_key=True),
    Column("customer_name", String(16), primary_key=True),
    Column("price", Float),
    Column("tip", Float),
)
metadata_obj.create_all(engine)

rows = [
    {"receipt_id": 1, "customer_name": "Alan Payne", "price": 12.06, "tip": 1.20},
    {"receipt_id": 2, "customer_name": "Alex Mason", "price": 23.86, "tip": 0.24},
    {"receipt_id": 3, "customer_name": "Woodrow Wilson", "price": 53.43, "tip": 5.43},
    {"receipt_id": 4, "customer_name": "Margaret James", "price": 21.11, "tip": 1.00},
]
for row in rows:
    stmt = insert(receipts).values(**row)
    with engine.begin() as connection:
        cursor = connection.execute(stmt)

inspector = inspect(engine)
columns_info = [(col["name"], col["type"]) for col in inspector.get_columns("receipts")]

table_description = "Columns:\n" + "\n".join([f"  - {name}: {col_type}" for name, col_type in columns_info])
print(table_description)

from smolagents import tool


@tool
def sql_engine(query: str) -> str:
    """
    Allows you to perform SQL queries on the table. Returns a string representation of the result.
    The table is named 'receipts'. Its description is as follows:
        Columns:
        - receipt_id: INTEGER
        - customer_name: VARCHAR(16)
        - price: FLOAT
        - tip: FLOAT

    Args:
        query: The query to perform. This should be correct SQL.
    """
    output = ""
    with engine.connect() as con:
        rows = con.execute(text(query))
        for row in rows:
            output += "\n" + str(row)
    return output


from smolagents import CodeAgent, InferenceClientModel


agent = CodeAgent(
    tools=[sql_engine],
    model=InferenceClientModel(model_id="meta-llama/Meta-Llama-3.1-8B-Instruct"),
)
agent.run("Can you give me the name of the client who got the most expensive receipt?")



================================================
FILE: examples/async_agent/README.md
================================================
# Async Applications with Agents

This example demonstrates how to use a `CodeAgent` from the `smolagents` library in an asynchronous Starlette web application.
The agent is executed in a background thread using `anyio.to_thread.run_sync`, allowing you to integrate synchronous agent logic into an async web server.

## Key Concepts

- **Starlette**: A lightweight ASGI framework for building async web apps.
- **anyio.to_thread.run_sync**: Runs blocking (sync) code in a thread, so it doesn't block the async event loop.
- **CodeAgent**: An agent from the `smolagents` library that can be used to solve tasks programmatically.

## How it works

- The Starlette app exposes a `/run-agent` endpoint that accepts a JSON payload with a `task` string.
- When a request is received, the agent is run in a background thread using `anyio.to_thread.run_sync`.
- The result is returned as a JSON response.

## Implementation Note

**Why use a background thread?** 

`CodeAgent.run()` executes Python code synchronously, which would block Starlette's async event loop if called directly. By offloading this synchronous operation to a separate thread with `anyio.to_thread.run_sync`, we maintain the application's responsiveness while the agent processes requests, ensuring optimal performance in high-concurrency scenarios.

## Usage

1. **Install dependencies**:
   ```bash
   pip install smolagents starlette anyio uvicorn
   ```

2. **Run the app**:
   ```bash
   uvicorn async_codeagent_starlette.main:app --reload
   ```

3. **Test the endpoint**:
   ```bash
   curl -X POST http://localhost:8000/run-agent -H 'Content-Type: application/json' -d '{"task": "What is 2+2?"}'
   ```

## Files

- `main.py`: Main Starlette application with async endpoint using CodeAgent.
- `README.md`: This file.

---
This example is designed to be clear and didactic for users new to async Python and agent integration.



================================================
FILE: examples/async_agent/main.py
================================================
"""
Async CodeAgent Example with Starlette

This example demonstrates how to use a CodeAgent in an async Starlette app,
running the agent in a background thread using anyio.to_thread.run_sync.
"""

import anyio.to_thread
from starlette.applications import Starlette
from starlette.requests import Request
from starlette.responses import JSONResponse
from starlette.routing import Route

from smolagents import CodeAgent, InferenceClientModel


# Create a simple agent instance (customize as needed)
def get_agent():
    # You can set custom model, or tools as needed
    return CodeAgent(
        model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
        tools=[],
    )


async def run_agent_in_thread(task: str):
    agent = get_agent()
    # The agent's run method is synchronous
    result = await anyio.to_thread.run_sync(agent.run, task)
    return result


async def run_agent_endpoint(request: Request):
    data = await request.json()
    task = data.get("task")
    if not task:
        return JSONResponse({"error": 'Missing "task" in request body.'}, status_code=400)
    try:
        result = await run_agent_in_thread(task)
        return JSONResponse({"result": result})
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


routes = [
    Route("/run-agent", run_agent_endpoint, methods=["POST"]),
]

app = Starlette(debug=True, routes=routes)



================================================
FILE: examples/async_agent/requirements.txt
================================================
smolagents
starlette
anyio
uvicorn



================================================
FILE: examples/open_deep_research/README.md
================================================
# Open Deep Research

Welcome to this open replication of [OpenAI's Deep Research](https://openai.com/index/introducing-deep-research/)! This agent attempts to replicate OpenAI's model and achieve similar performance on research tasks.

Read more about this implementation's goal and methods in our [blog post](https://huggingface.co/blog/open-deep-research).


This agent achieves **55% pass@1** on the GAIA validation set, compared to **67%** for the original Deep Research.

## Setup

To get started, follow the steps below:

### Clone the repository

```bash
git clone https://github.com/huggingface/smolagents.git
cd smolagents/examples/open_deep_research
```

### Install dependencies

Run the following command to install the required dependencies from the `requirements.txt` file:

```bash
pip install -r requirements.txt
```

### Install the development version of `smolagents`

```bash
pip install -e ../../.[dev]
```

### Set up environment variables

The agent uses the `GoogleSearchTool` for web search, which requires an environment variable with the corresponding API key, based on the selected provider:
- `SERPAPI_API_KEY` for SerpApi: [Sign up here to get a key](https://serpapi.com/users/sign_up)
- `SERPER_API_KEY` for Serper: [Sign up here to get a key](https://serper.dev/signup)

Depending on the model you want to use, you may need to set environment variables.
For example, to use the default `o1` model, you need to set the `OPENAI_API_KEY` environment variable.
[Sign up here to get a key](https://platform.openai.com/signup).

> [!WARNING]
> The use of the default `o1` model is restricted to tier-3 access: https://help.openai.com/en/articles/10362446-api-access-to-o1-and-o3-mini


## Usage

Then you're good to go! Run the run.py script, as in:
```bash
python run.py --model-id "o1" "Your question here!"
```

## Full reproducibility of results

The data used in our submissions to GAIA was augmented in this way:
 -  For each single-page .pdf or .xls file, it was opened in a file reader (MacOS Sonoma Numbers or Preview), and a ".png" screenshot was taken and added to the folder.
- Then for any file used in a question, the file loading system checks if there is a ".png" extension version of the file, and loads it instead of the original if it exists.

This process was done manually but could be automatized.

After processing, the annotated was uploaded to a [new dataset](https://huggingface.co/datasets/smolagents/GAIA-annotated). You need to request access (granted instantly).


================================================
FILE: examples/open_deep_research/analysis.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install plotly kaleido datasets nbformat -U -q

import os

import datasets
import pandas as pd
from dotenv import load_dotenv
from huggingface_hub import login


load_dotenv(override=True)
login(os.getenv("HF_TOKEN"))

pd.set_option("max_colwidth", None)

OUTPUT_DIR = "output"

eval_ds = datasets.load_dataset("gaia-benchmark/GAIA", "2023_all")["validation"]
eval_ds = eval_ds.rename_columns({"Question": "question", "Final answer": "true_answer", "Level": "task"})
eval_df = pd.DataFrame(eval_ds)

"""
# 1. Load all results
"""

import glob


results = []
for f in glob.glob(f"{OUTPUT_DIR}/validation/*.jsonl"):
    df = pd.read_json(f, lines=True)
    df["agent_name"] = f.split("/")[-1].split(".")[0]
    results.append(df)

result_df = pd.concat(results)
result_df["prediction"] = result_df["prediction"].fillna("No prediction")

import re
from collections import Counter

from scripts.gaia_scorer import check_close_call, question_scorer


result_df["is_correct"] = result_df.apply(lambda x: question_scorer(x["prediction"], x["true_answer"]), axis=1)
result_df["is_near_correct"] = result_df.apply(
    lambda x: check_close_call(x["prediction"], x["true_answer"], x["is_correct"]),
    axis=1,
)

result_df["count_steps"] = result_df["intermediate_steps"].apply(len)


def find_attachment(question):
    matches = eval_df.loc[eval_df["question"].apply(lambda x: x in question), "file_name"]

    if len(matches) == 0:
        return "Not found"
    file_path = matches.values[0]

    if isinstance(file_path, str) and len(file_path) > 0:
        return file_path.split(".")[-1]
    else:
        return "None"


result_df["attachment_type"] = result_df["question"].apply(find_attachment)


def extract_tool_calls(code):
    regex = r"\b(\w+)\("
    function_calls = [el for el in re.findall(regex, code) if el.islower()]

    function_call_counter = Counter(function_calls)
    return function_call_counter


def sum_tool_calls(steps):
    total_count = Counter()
    for step in steps:
        if "llm_output" in step:
            total_count += extract_tool_calls(step["llm_output"])

    return total_count


def get_durations(row):
    # start_datetime = datetime.strptime(row['start_time'], "%Y-%m-%d %H:%M:%S")
    # end_datetime = datetime.strptime(row['end_time'], "%Y-%m-%d %H:%M:%S")

    duration_timedelta = row["end_time"] - row["start_time"]
    return int(duration_timedelta.total_seconds())


result_df["duration"] = result_df.apply(get_durations, axis=1)
# result_df["tool_calls"] = result_df["intermediate_steps"].apply(sum_tool_calls)

result_df["agent_name"].value_counts()

"""
# 2. Inspect specific runs
"""

sel_df = result_df
# sel_df = sel_df.loc[
#     (result_df["agent_name"].isin(list_versions))
# ]
sel_df = sel_df.reset_index(drop=True)
display(sel_df["agent_name"].value_counts())
sel_df = sel_df.drop_duplicates(subset=["agent_name", "question"])
display(sel_df.groupby("agent_name")[["task"]].value_counts())
print("Total length:", len(sel_df), "- is complete:", len(sel_df) == 165)

display("Average score:", sel_df.groupby("agent_name")[["is_correct"]].mean().round(3))
display(
    sel_df.groupby(["agent_name", "task"])[["is_correct", "is_near_correct", "count_steps", "question", "duration"]]
    .agg(
        {
            "is_correct": "mean",
            "is_near_correct": "mean",
            "count_steps": "mean",
            "question": "count",
            "duration": "mean",
        }
    )
    .rename(columns={"question": "count"})
)

import plotly.express as px


cumulative_df = (
    (
        sel_df.groupby("agent_name")[["is_correct", "is_near_correct"]]
        .expanding(min_periods=1, axis=0, method="single")
        .agg({"is_correct": "mean", "is_near_correct": "count"})
        .reset_index()
    )
    .copy()
    .rename(columns={"is_near_correct": "index"})
)
cumulative_df["index"] = cumulative_df["index"].astype(int) - 1


def find_question(row):
    try:
        res = sel_df.loc[sel_df["agent_name"] == row["agent_name"], "question"].iloc[row["index"]][:50]
        return res
    except Exception:
        return ""


cumulative_df["question"] = cumulative_df.apply(find_question, axis=1)

px.line(
    cumulative_df,
    color="agent_name",
    x="index",
    y="is_correct",
    hover_data="question",
)

"""
# 3. Dive deeper into one run
"""

sel_df = result_df.loc[result_df["agent_name"] == "o1"]
print(len(sel_df))

"""
### Count errors
"""

import numpy as np


error_types = [
    "AgentParsingError",
    "AgentExecutionError",
    "AgentMaxIterationsError",
    "AgentGenerationError",
]
sel_df[error_types] = 0
sel_df["Count steps"] = np.nan


def count_errors(row):
    if isinstance(row["intermediate_steps"], list):
        row["Count steps"] = len(row["intermediate_steps"])
        for step in row["intermediate_steps"]:
            if isinstance(step, dict) and "error" in step:
                try:
                    row[str(step["error"]["error_type"])] += 1
                except Exception:
                    pass
    return row


sel_df = sel_df.apply(count_errors, axis=1)

import plotly.express as px


aggregate_errors = (
    sel_df.groupby(["is_correct"])[error_types + ["Count steps"]].mean().reset_index().melt(id_vars=["is_correct"])
)

fig = px.bar(
    aggregate_errors,
    y="value",
    x="variable",
    color="is_correct",
    labels={
        "agent_name": "<b>Model</b>",
        "task": "<b>Level</b>",
        "aggregate_score": "<b>Performance</b>",
        "value": "<b>Average count</b>",
        "eval_score_GPT4": "<b>Score</b>",
    },
)
fig.update_layout(
    height=500,
    width=800,
    barmode="group",
    bargroupgap=0.0,
)
fig.update_traces(textposition="outside")
fig.write_image("aggregate_errors.png", scale=3)
fig.show()

"""
### Inspect result by file extension type
"""

display(
    result_df.groupby(["attachment_type"])[["is_correct", "count_steps", "question"]].agg(
        {"is_correct": "mean", "count_steps": "mean", "question": "count"}
    )
)

"""
# 4. Ensembling methods
"""

counts = result_df["agent_name"].value_counts()
long_series = result_df.loc[result_df["agent_name"].isin(counts[counts > 140].index)]

def majority_vote(df):
    df = df[(df["prediction"] != "Unable to determine") & (~df["prediction"].isna()) & (df["prediction"] != "None")]

    answer_modes = df.groupby("question")["prediction"].agg(lambda x: x.mode()[0]).reset_index()
    first_occurrences = (
        df.groupby(["question", "prediction"]).agg({"task": "first", "is_correct": "first"}).reset_index()
    )
    result = answer_modes.merge(first_occurrences, on=["question", "prediction"], how="left")

    return result


def oracle(df):
    def get_first_correct_or_first_wrong(group):
        correct_answers = group[group["is_correct"]]
        if len(correct_answers) > 0:
            return correct_answers.iloc[0]
        return group.iloc[0]

    result = df.groupby("question").apply(get_first_correct_or_first_wrong)

    return result.reset_index(drop=True)


display((long_series.groupby("agent_name")["is_correct"].mean() * 100).round(2))
print(f"Majority score: {majority_vote(long_series)['is_correct'].mean() * 100:.2f}")
print(f"Oracle score: {oracle(long_series)['is_correct'].mean() * 100:.2f}")

"""
### Submit
"""

agent_run = "code_o1_04_february_submission5.jsonl"
df = pd.read_json(f"output/validation/{agent_run}", lines=True)
df = df[["task_id", "prediction", "intermediate_steps"]]
df = df.rename(columns={"prediction": "model_answer", "intermediate_steps": "reasoning_trace"})

df.to_json("submission.jsonl", orient="records", lines=True)



================================================
FILE: examples/open_deep_research/app.py
================================================
from run import create_agent

from smolagents.gradio_ui import GradioUI


agent = create_agent()

demo = GradioUI(agent)

if __name__ == "__main__":
    demo.launch()



================================================
FILE: examples/open_deep_research/requirements.txt
================================================
anthropic>=0.37.1
audioop-lts<1.0; python_version >= "3.13" # required to use pydub in Python >=3.13; LTS port of the removed Python builtin module audioop
beautifulsoup4>=4.12.3
datasets>=2.21.0
google_search_results>=2.4.2
huggingface_hub>=0.23.4
mammoth>=1.8.0
markdownify>=0.13.1
numexpr>=2.10.1
numpy>=2.1.2
openai>=1.52.2
openpyxl
pandas>=2.2.3
pathvalidate>=3.2.1
pdfminer>=20191125
pdfminer.six>=20240706
Pillow>=11.0.0
puremagic>=1.28
pypdf>=5.1.0
python-dotenv>=1.0.1
python_pptx>=1.0.2
Requests>=2.32.3
tqdm>=4.66.4
torch>=2.2.2
torchvision>=0.17.2
transformers>=4.46.0
youtube_transcript_api>=0.6.2
chess
sympy
pubchempy
Bio
scikit-learn
scipy
pydub
PyPDF2
python-pptx
torch
xlrd
SpeechRecognition



================================================
FILE: examples/open_deep_research/run.py
================================================
import argparse
import os
import threading

from dotenv import load_dotenv
from huggingface_hub import login
from scripts.text_inspector_tool import TextInspectorTool
from scripts.text_web_browser import (
    ArchiveSearchTool,
    FinderTool,
    FindNextTool,
    PageDownTool,
    PageUpTool,
    SimpleTextBrowser,
    VisitTool,
)
from scripts.visual_qa import visualizer

from smolagents import (
    CodeAgent,
    GoogleSearchTool,
    # InferenceClientModel,
    LiteLLMModel,
    ToolCallingAgent,
)


load_dotenv(override=True)
login(os.getenv("HF_TOKEN"))

append_answer_lock = threading.Lock()


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "question", type=str, help="for example: 'How many studio albums did Mercedes Sosa release before 2007?'"
    )
    parser.add_argument("--model-id", type=str, default="o1")
    return parser.parse_args()


custom_role_conversions = {"tool-call": "assistant", "tool-response": "user"}

user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"

BROWSER_CONFIG = {
    "viewport_size": 1024 * 5,
    "downloads_folder": "downloads_folder",
    "request_kwargs": {
        "headers": {"User-Agent": user_agent},
        "timeout": 300,
    },
    "serpapi_key": os.getenv("SERPAPI_API_KEY"),
}

os.makedirs(f"./{BROWSER_CONFIG['downloads_folder']}", exist_ok=True)


def create_agent(model_id="o1"):
    model_params = {
        "model_id": model_id,
        "custom_role_conversions": custom_role_conversions,
        "max_completion_tokens": 8192,
    }
    if model_id == "o1":
        model_params["reasoning_effort"] = "high"
    model = LiteLLMModel(**model_params)

    text_limit = 100000
    browser = SimpleTextBrowser(**BROWSER_CONFIG)
    WEB_TOOLS = [
        GoogleSearchTool(provider="serper"),
        VisitTool(browser),
        PageUpTool(browser),
        PageDownTool(browser),
        FinderTool(browser),
        FindNextTool(browser),
        ArchiveSearchTool(browser),
        TextInspectorTool(model, text_limit),
    ]
    text_webbrowser_agent = ToolCallingAgent(
        model=model,
        tools=WEB_TOOLS,
        max_steps=20,
        verbosity_level=2,
        planning_interval=4,
        name="search_agent",
        description="""A team member that will search the internet to answer your question.
    Ask him for all your questions that require browsing the web.
    Provide him as much context as possible, in particular if you need to search on a specific timeframe!
    And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.
    Your request must be a real sentence, not a google search! Like "Find me this information (...)" rather than a few keywords.
    """,
        provide_run_summary=True,
    )
    text_webbrowser_agent.prompt_templates["managed_agent"]["task"] += """You can navigate to .txt online files.
    If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.
    Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information."""

    manager_agent = CodeAgent(
        model=model,
        tools=[visualizer, TextInspectorTool(model, text_limit)],
        max_steps=12,
        verbosity_level=2,
        additional_authorized_imports=["*"],
        planning_interval=4,
        managed_agents=[text_webbrowser_agent],
    )

    return manager_agent


def main():
    args = parse_args()

    agent = create_agent(model_id=args.model_id)

    answer = agent.run(args.question)

    print(f"Got this answer: {answer}")


if __name__ == "__main__":
    main()



================================================
FILE: examples/open_deep_research/run_gaia.py
================================================
# EXAMPLE COMMAND: from folder examples/open_deep_research, run: python run_gaia.py --concurrency 32 --run-name generate-traces-03-apr-noplanning --model-id gpt-4o
import argparse
import json
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any

import datasets
import pandas as pd
from dotenv import load_dotenv
from huggingface_hub import login, snapshot_download
from scripts.reformulator import prepare_response
from scripts.run_agents import (
    get_single_file_description,
    get_zip_description,
)
from scripts.text_inspector_tool import TextInspectorTool
from scripts.text_web_browser import (
    ArchiveSearchTool,
    FinderTool,
    FindNextTool,
    PageDownTool,
    PageUpTool,
    SimpleTextBrowser,
    VisitTool,
)
from scripts.visual_qa import visualizer
from tqdm import tqdm

from smolagents import (
    CodeAgent,
    GoogleSearchTool,
    LiteLLMModel,
    Model,
    TokenUsage,
    ToolCallingAgent,
)


load_dotenv(override=True)
login(os.getenv("HF_TOKEN"))

append_answer_lock = threading.Lock()


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--concurrency", type=int, default=8)
    parser.add_argument("--model-id", type=str, default="o1")
    parser.add_argument("--run-name", type=str, required=True)
    parser.add_argument("--set-to-run", type=str, default="validation")
    parser.add_argument("--use-open-models", type=bool, default=False)
    parser.add_argument("--use-raw-dataset", action="store_true")
    return parser.parse_args()


### IMPORTANT: EVALUATION SWITCHES

print("Make sure you deactivated any VPN like Tailscale, else some URLs will be blocked!")

custom_role_conversions = {"tool-call": "assistant", "tool-response": "user"}


user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"

BROWSER_CONFIG = {
    "viewport_size": 1024 * 5,
    "downloads_folder": "downloads_folder",
    "request_kwargs": {
        "headers": {"User-Agent": user_agent},
        "timeout": 300,
    },
    "serpapi_key": os.getenv("SERPAPI_API_KEY"),
}

os.makedirs(f"./{BROWSER_CONFIG['downloads_folder']}", exist_ok=True)


def create_agent_team(model: Model, token_counts: TokenUsage):
    text_limit = 100000
    ti_tool = TextInspectorTool(model, text_limit)

    browser = SimpleTextBrowser(**BROWSER_CONFIG)

    WEB_TOOLS = [
        GoogleSearchTool(provider="serper"),
        VisitTool(browser),
        PageUpTool(browser),
        PageDownTool(browser),
        FinderTool(browser),
        FindNextTool(browser),
        ArchiveSearchTool(browser),
        TextInspectorTool(model, text_limit),
    ]

    def increment_web_agent_token_counts(final_answer, memory_step, agent):
        token_counts_web = agent.monitor.get_total_token_counts()
        token_counts.input_tokens += token_counts_web["input"]
        token_counts.output_tokens += token_counts_web["output"]
        return True

    text_webbrowser_agent = ToolCallingAgent(
        model=model,
        tools=WEB_TOOLS,
        max_steps=20,
        verbosity_level=2,
        planning_interval=4,
        name="search_agent",
        description="""A team member that will search the internet to answer your question.
    Ask him for all your questions that require browsing the web.
    Provide him as much context as possible, in particular if you need to search on a specific timeframe!
    And don't hesitate to provide him with a complex search task, like finding a difference between two webpages.
    Your request must be a real sentence, not a google search! Like "Find me this information (...)" rather than a few keywords.
    """,
        provide_run_summary=True,
        final_answer_checks=[increment_web_agent_token_counts],
    )
    text_webbrowser_agent.prompt_templates["managed_agent"]["task"] += """You can navigate to .txt online files.
    If a non-html page is in another format, especially .pdf or a Youtube video, use tool 'inspect_file_as_text' to inspect it.
    Additionally, if after some searching you find out that you need more information to answer the question, you can use `final_answer` with your request for clarification as argument to request for more information."""

    manager_agent = CodeAgent(
        model=model,
        tools=[visualizer, ti_tool],
        max_steps=12,
        verbosity_level=2,
        additional_authorized_imports=["*"],
        planning_interval=4,
        managed_agents=[text_webbrowser_agent],
    )
    return manager_agent


def load_gaia_dataset(use_raw_dataset: bool, set_to_run: str) -> datasets.Dataset:
    if not os.path.exists("data/gaia"):
        if use_raw_dataset:
            snapshot_download(
                repo_id="gaia-benchmark/GAIA",
                repo_type="dataset",
                local_dir="data/gaia",
                ignore_patterns=[".gitattributes", "README.md"],
            )
        else:
            # WARNING: this dataset is gated: make sure you visit the repo to require access.
            snapshot_download(
                repo_id="smolagents/GAIA-annotated",
                repo_type="dataset",
                local_dir="data/gaia",
                ignore_patterns=[".gitattributes", "README.md"],
            )

    def preprocess_file_paths(row):
        if len(row["file_name"]) > 0:
            row["file_name"] = f"data/gaia/{set_to_run}/" + row["file_name"]
        return row

    eval_ds = datasets.load_dataset(
        "data/gaia/GAIA.py",
        name="2023_all",
        split=set_to_run,
        # data_files={"validation": "validation/metadata.jsonl", "test": "test/metadata.jsonl"},
    )

    eval_ds = eval_ds.rename_columns({"Question": "question", "Final answer": "true_answer", "Level": "task"})
    eval_ds = eval_ds.map(preprocess_file_paths)
    return eval_ds


def append_answer(entry: dict, jsonl_file: str) -> None:
    jsonl_path = Path(jsonl_file)
    jsonl_path.parent.mkdir(parents=True, exist_ok=True)
    with append_answer_lock, open(jsonl_file, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(entry) + "\n")
    assert jsonl_path.exists(), "File not found!"
    print("Answer exported to file:", jsonl_path.resolve())


def answer_single_question(
    example: dict, model_id: str, answers_file: str, visual_inspection_tool: TextInspectorTool
) -> None:
    model_params: dict[str, Any] = {
        "model_id": model_id,
        "custom_role_conversions": custom_role_conversions,
    }
    if model_id == "o1":
        model_params["reasoning_effort"] = "high"
        model_params["max_completion_tokens"] = 8192
    else:
        model_params["max_tokens"] = 4096
    model = LiteLLMModel(**model_params)
    # model = InferenceClientModel(model_id="Qwen/Qwen3-32B", provider="novita", max_tokens=4096)
    document_inspection_tool = TextInspectorTool(model, 100000)

    total_token_counts: TokenUsage = {
        "input": 0,
        "output": 0,
    }
    agent = create_agent_team(model, total_token_counts)

    augmented_question = """You have one question to answer. It is paramount that you provide a correct answer.
Give it all you can: I know for a fact that you have access to all the relevant tools to solve it and find the correct answer (the answer does exist).
Failure or 'I cannot answer' or 'None found' will not be tolerated, success will be rewarded.
Run verification steps if that's needed, you must make sure you find the correct answer! Here is the task:

""" + example["question"]

    if example["file_name"]:
        if ".zip" in example["file_name"]:
            prompt_use_files = "\n\nTo solve the task above, you will have to use these attached files:\n"
            prompt_use_files += get_zip_description(
                example["file_name"], example["question"], visual_inspection_tool, document_inspection_tool
            )
        else:
            prompt_use_files = "\n\nTo solve the task above, you will have to use this attached file:\n"
            prompt_use_files += get_single_file_description(
                example["file_name"], example["question"], visual_inspection_tool, document_inspection_tool
            )
        augmented_question += prompt_use_files

    start_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        # Run agent ðŸš€
        final_result = agent.run(augmented_question)

        agent_memory = agent.write_memory_to_messages()

        final_result = prepare_response(augmented_question, agent_memory, reformulation_model=model)

        output = str(final_result)
        for memory_step in agent.memory.steps:
            memory_step.model_input_messages = None
        intermediate_steps = agent_memory

        # Check for parsing errors which indicate the LLM failed to follow the required format
        parsing_error = True if any(["AgentParsingError" in step for step in intermediate_steps]) else False

        # check if iteration limit exceeded
        iteration_limit_exceeded = True if "Agent stopped due to iteration limit or time limit." in output else False
        raised_exception = False

    except Exception as e:
        print("Error on ", augmented_question, e)
        output = None
        intermediate_steps = []
        parsing_error = False
        iteration_limit_exceeded = False
        exception = e
        raised_exception = True
    end_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    token_counts_manager = agent.monitor.get_total_token_counts()
    total_token_counts.input_tokens += token_counts_manager["input"]
    total_token_counts.output_tokens += token_counts_manager["output"]
    annotated_example = {
        "agent_name": model.model_id,
        "question": example["question"],
        "augmented_question": augmented_question,
        "prediction": output,
        "intermediate_steps": intermediate_steps,
        "parsing_error": parsing_error,
        "iteration_limit_exceeded": iteration_limit_exceeded,
        "agent_error": str(exception) if raised_exception else None,
        "task": example["task"],
        "task_id": example["task_id"],
        "true_answer": example["true_answer"],
        "start_time": start_time,
        "end_time": end_time,
        "token_counts": total_token_counts,
    }
    append_answer(annotated_example, answers_file)


def get_examples_to_answer(answers_file: str, eval_ds: datasets.Dataset) -> list[dict]:
    print(f"Loading answers from {answers_file}...")
    try:
        done_questions = pd.read_json(answers_file, lines=True)["question"].tolist()
        print(f"Found {len(done_questions)} previous results!")
    except Exception as e:
        print("Error when loading records: ", e)
        print("No usable records! â–¶ï¸ Starting new.")
        done_questions = []
    return [line for line in eval_ds.to_list() if line["question"] not in done_questions and line["file_name"]]


def main():
    args = parse_args()
    print(f"Starting run with arguments: {args}")

    eval_ds = load_gaia_dataset(args.use_raw_dataset, args.set_to_run)
    print("Loaded evaluation dataset:")
    print(pd.DataFrame(eval_ds)["task"].value_counts())

    answers_file = f"output/{args.set_to_run}/{args.run_name}.jsonl"
    tasks_to_run = get_examples_to_answer(answers_file, eval_ds)

    with ThreadPoolExecutor(max_workers=args.concurrency) as exe:
        futures = [
            exe.submit(answer_single_question, example, args.model_id, answers_file, visualizer)
            for example in tasks_to_run
        ]
        for f in tqdm(as_completed(futures), total=len(tasks_to_run), desc="Processing tasks"):
            f.result()

    # for example in tasks_to_run:
    #     answer_single_question(example, args.model_id, answers_file, visualizer)
    print("All tasks processed.")


if __name__ == "__main__":
    main()



================================================
FILE: examples/open_deep_research/visual_vs_text_browser.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Compare a text-based vs a vision-based browser

Warning: this notebook is experimental, it probably won't work out of the box!
"""

!pip install "smolagents[litellm,toolkit]" -q

import datasets


eval_ds = datasets.load_dataset("gaia-benchmark/GAIA", "2023_all")["validation"]

to_keep = [
    "What's the last line of the rhyme under the flavor",
    'Of the authors (First M. Last) that worked on the paper "Pie Menus or Linear Menus',
    "In Series 9, Episode 11 of Doctor Who, the Doctor is trapped inside an ever-shifting maze. What is this location called in the official script for the episode? Give the setting exactly as it appears in the first scene heading.",
    "Which contributor to the version of OpenCV where support was added for the Mask-RCNN model has the same name as a former Chinese head of government when the names are transliterated to the Latin alphabet?",
    "The photograph in the Whitney Museum of American Art's collection with accession number 2022.128 shows a person holding a book. Which military unit did the author of this book join in 1813? Answer without using articles.",
    "I went to Virtue restaurant & bar in Chicago for my birthday on March 22, 2021 and the main course I had was delicious! Unfortunately, when I went back about a month later on April 21, it was no longer on the dinner menu.",
    "In Emily Midkiff's June 2014 article in a journal named for the one of Hreidmar's ",
    "Under DDC 633 on Bielefeld University Library's BASE, as of 2020",
    "In the 2018 VSCode blog post on replit.com, what was the command they clicked on in the last video to remove extra lines?",
    "The Metropolitan Museum of Art has a portrait in its collection with an accession number of 29.100.5. Of the consecrators and co-consecrators",
    "In Nature journal's Scientific Reports conference proceedings from 2012, in the article that did not mention plasmons or plasmonics, what nano-compound is studied?",
    'In the year 2022, and before December, what does "R" stand for in the three core policies of the type of content',
    "Who nominated the only Featured Article on English Wikipedia about a dinosaur that was promoted in November 2016?",
]
eval_ds = eval_ds.filter(lambda row: any([el in row["Question"] for el in to_keep]))
eval_ds = eval_ds.rename_columns({"Question": "question", "Final answer": "true_answer", "Level": "task"})

import os

from dotenv import load_dotenv
from huggingface_hub import login


load_dotenv(override=True)

login(os.getenv("HF_TOKEN"))

"""
### Text browser
"""

from scripts.run_agents import answer_questions
from scripts.text_inspector_tool import TextInspectorTool
from scripts.text_web_browser import (
    ArchiveSearchTool,
    FinderTool,
    FindNextTool,
    NavigationalSearchTool,
    PageDownTool,
    PageUpTool,
    SearchInformationTool,
    VisitTool,
)
from scripts.visual_qa import VisualQAGPT4Tool

from smolagents import CodeAgent, LiteLLMModel


proprietary_model = LiteLLMModel(model_id="gpt-4o")

### BUILD AGENTS & TOOLS

WEB_TOOLS = [
    SearchInformationTool(),
    NavigationalSearchTool(),
    VisitTool(),
    PageUpTool(),
    PageDownTool(),
    FinderTool(),
    FindNextTool(),
    ArchiveSearchTool(),
]


surfer_agent = CodeAgent(
    model=proprietary_model,
    tools=WEB_TOOLS,
    max_steps=20,
    verbosity_level=2,
)

results_text = answer_questions(
    eval_ds,
    surfer_agent,
    "code_gpt4o_27-01_text",
    reformulation_model=proprietary_model,
    output_folder="output_browsers",
    visual_inspection_tool=VisualQAGPT4Tool(),
    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),
)

"""
### Vision browser
"""

!pip install helium -q

from scripts.visual_qa import VisualQAGPT4Tool

from smolagents import CodeAgent, LiteLLMModel, WebSearchTool
from smolagents.vision_web_browser import (
    close_popups,
    go_back,
    helium_instructions,
    initialize_agent,
    save_screenshot,
    search_item_ctrl_f,
)


proprietary_model = LiteLLMModel(model_id="gpt-4o")
vision_browser_agent = initialize_agent(proprietary_model)
### BUILD AGENTS & TOOLS

CodeAgent(
    tools=[WebSearchTool(), go_back, close_popups, search_item_ctrl_f],
    model=proprietary_model,
    additional_authorized_imports=["helium"],
    step_callbacks=[save_screenshot],
    max_steps=20,
    verbosity_level=2,
)

results_vision = answer_questions(
    eval_ds,
    vision_browser_agent,
    "code_gpt4o_27-01_vision",
    reformulation_model=proprietary_model,
    output_folder="output_browsers",
    visual_inspection_tool=VisualQAGPT4Tool(),
    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),
    postprompt=helium_instructions
    + "Any web browser controls won't work on .pdf urls, rather use the tool 'inspect_file_as_text' to read them",
)

"""
### Browser-use browser
"""

!pip install browser-use lxml_html_clean -q
!playwright install

import asyncio

import nest_asyncio


nest_asyncio.apply()

from browser_use import Agent
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI


load_dotenv()


class BrowserUseAgent:
    logs = []

    def write_inner_memory_from_logs(self, summary_mode):
        return self.results

    def run(self, task, **kwargs):
        agent = Agent(
            task=task,
            llm=ChatOpenAI(model="gpt-4o"),
        )
        self.results = asyncio.get_event_loop().run_until_complete(agent.run())
        return self.results.history[-1].result[0].extracted_content


browser_use_agent = BrowserUseAgent()

results_browseruse = answer_questions(
    eval_ds,
    browser_use_agent,
    "gpt-4o_27-01_browseruse",
    reformulation_model=proprietary_model,
    output_folder="output_browsers",
    visual_inspection_tool=VisualQAGPT4Tool(),
    text_inspector_tool=TextInspectorTool(proprietary_model, 40000),
    postprompt="",
    run_simple=True,
)

"""
### Get results
"""

import pandas as pd
from scripts.gaia_scorer import question_scorer


results_vision, results_text, results_browseruse = (
    pd.DataFrame(results_vision),
    pd.DataFrame(results_text),
    pd.DataFrame(results_browseruse),
)

results_vision["is_correct"] = results_vision.apply(
    lambda x: question_scorer(x["prediction"], x["true_answer"]), axis=1
)
results_text["is_correct"] = results_text.apply(lambda x: question_scorer(x["prediction"], x["true_answer"]), axis=1)
results_browseruse["is_correct"] = results_browseruse.apply(
    lambda x: question_scorer(x["prediction"], x["true_answer"]), axis=1
)

results = pd.concat([results_vision, results_text, results_browseruse])
results.groupby("agent_name")["is_correct"].mean()

correct_vision_results = results_vision.loc[results_vision["is_correct"]]
correct_vision_results

false_text_results = results_text.loc[~results_text["is_correct"]]
false_text_results



================================================
FILE: examples/open_deep_research/scripts/cookies.py
================================================
from requests.cookies import RequestsCookieJar


COOKIES_LIST = [
    {
        "domain": ".youtube.com",
        "expirationDate": 1718884961,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ST-xuwub9",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "session_logininfo=AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0%3AQUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753004444.745411,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-YEC",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CgtRVnI5LW1zRHlQVSjbtNCzBjIhCgJGUhIbEhcSFRMLFBUWFwwYGRobHB0eHw4PIBAREiAk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050824,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB4ezJ_bdWu46a7YwObVn44wACgYKAakSARQSFQHGX2MicJcTzecTKH6bHzqU6TMbTxoVAUF8yKqQYK-MoI6Ql3vI2oYTB3E-0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974642,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SIDCC",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWQZauHKOo8t87zoEcjaVNIYUX54ohoWXT-tX4aAhEuZzIIptxZAcNkHuG2oDXYL6t-lw",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050652,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBB6VHrZcC3gBAsFPbCQ0gF5AACgYKAYkSARQSFQHGX2Mi9kt0gHg5CxCYSkLQGHWaeBoVAUF8yKre_V6r3jZVak6JV4o2Q0FL0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397534,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDTS",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753433494.44729,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_M0180HEFCY",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718871908.1.0.1718873494.0.0.0",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050933,
        "hostOnly": False,
        "httpOnly": False,
        "name": "SAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974764,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSIDCC",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzWHDSoXGCZpZhPxRrnC7B1s8zGIUjeMVyvgtQfsm1fs92lXPtFEI_td9LBUyqVUe0xK",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050881,
        "hostOnly": False,
        "httpOnly": True,
        "name": "SSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AmlwXHnQvOQ10LVd-",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050959,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-1PAPISID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050795,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-1PSID",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "g.a000kwibeLUu8Ea9Y-vLun7u3kU5VNJVuMAZl_jdfJaNm50JyDBBrlk7lRpKQGywAHEon7WGQAACgYKAQsSARQSFQHGX2MirAmnSRdZl6GPG6KLd4hOihoVAUF8yKoV17Tcj1a_OenIOkf2wBjO0076",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050993,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__Secure-3PAPISID",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "mfeuiC-HraNJ-A03/ASXvCPNJSw7yTFgd6",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420959.974815,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDCC",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AKEyXzXM5UjKUEXwSHVmRAIo6hGHA4G63adj3EE1VdNriD0f38jZQbsUKiD4LQbA3BValmTFDg",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1750420958.397647,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__Secure-3PSIDTS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "sidts-CjIB3EgAEkYL2L-GfrEzW5Dfy62S9oefGNLgst78S_986htCnGcfkxECch_9oz-qytSsZBAA",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050908,
        "hostOnly": False,
        "httpOnly": False,
        "name": "APISID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "IlQWLPjdNqziwCrV/ANG7Z4x5FF-IBxbZk",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753434620.050855,
        "hostOnly": False,
        "httpOnly": True,
        "name": "HSID",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "AasA7hmRuTFv7vjoq",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753435873.577793,
        "hostOnly": False,
        "httpOnly": True,
        "name": "LOGIN_INFO",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "AFmmF2swRAIgf4gadACOuWOcipI1anW-dakEjtidNLkufnOC8uml7EECIDh2YisqWELDBJPTGUysCucJ3I0wjXxYjVHro1LHrdW0:QUQ3MjNmd2Jiajl3OWZYRnpFNnZlWWV5ZGJWZ0hpcmp4LVVPU280bk4zOS03Z0ozZG9fOFhWZ0dXaVo3NG1wTEg1b3hGaG10TFBlaFBnTlJfbER5bEp0aFhoNS1OLVhYNFRZT2F6ajgzOFpDbGhlUjZpMWRETlFFRjFfTTRiM0RnNTROSkdmMTFMVjFic1VuZ2trbGp4aktDa0JJUC1BWDh3",
    },
    {
        "domain": ".youtube.com",
        "expirationDate": 1753444956.555608,
        "hostOnly": False,
        "httpOnly": False,
        "name": "PREF",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "f4=4000000&f6=40000000&tz=Europe.Paris&f5=30000&f7=100",
    },
]

COOKIES_LIST += [
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "isInstIp",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1734423981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__eoi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=c26f752377373146:T=1718871981:RT=1718884914:S=AA-AfjZw-T_OOX2kW2LLaFzXImgc",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1753444909.646103,
        "hostOnly": False,
        "httpOnly": True,
        "name": "ptc",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "RG1.8947708639250500550.1718872043",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "euconsent-v2-didomi",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "CQAgmoAQAgmoAAHABBENA5EsAP_gAEPgAAYgJ2pB5G5UTWlBIG53YMskIAUFhFBoQEAgAACAAwIBSBIAIIwEAGAAIAgAICACAAIAIBIAIABAGAAAAAAAYIAAIAAIAAAQIAAKIAAAAAAAAgBQAAgIAgggEAAAgEBEABAAgAAAEIIAQNgACgAAACCAAAAAAAABAAAAAAAAQAAAAAAAYCQAAAJIAAAAACAIABAIAAAAAAAAAAAAAAAABBAAIJ2wPIAFAAXABQAFQALgAcAA8ACAAEgALwAZAA0ACIAEcAJgAUgAqgBcADEAGgAPQAfgBEACOAE4AMMAZYA0QBsgDkAHOAO4AfsBBwEIAItARwBHQC6gHUAO2Ae0A_4CHQEXgJ2AUOAo8BT4CpQFqALYAXmAwQBkgDLAGXANjAhCBG8CbAE3gJ1gTtAA.f_wACHwAAAAA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718885236,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": "www.researchgate.net",
        "expirationDate": 1721477183,
        "hostOnly": True,
        "httpOnly": False,
        "name": "_pbjs_userid_consent_data",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "3524755945110770",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gads",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "ID=eca2adb88969c830:T=1718871981:RT=1718884914:S=ALNI_MY2qZchynrhWX6hWMlaI87Pcj9riQ",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718886709.646173,
        "hostOnly": False,
        "httpOnly": True,
        "name": "__cf_bm",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "IkQ_J4ciBzKQduRvjqsfSmQu8UygDWbHeROO5JVccfo-1718884909-1.0.1.1-qvNGEdbfI0HfhFP6kwe7R7mkTqODNhFuKhs72lLly6K2BOPMG3kbahpQFGvPK0U8FUfkznkq65gngd1sWj7sDA",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1752567981,
        "hostOnly": False,
        "httpOnly": False,
        "name": "__gpi",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "UID=00000e4e9aa2e6f2:T=1718871981:RT=1718884914:S=ALNI_MYFNrgzkKn7K6Bd2y8hC6GJCvDiSg",
    },
    {
        "domain": ".researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "_cfuvid",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "_GPmGZkBymiH3UiqTqzakEpi98br3nfFUWC2_u_wqkc-1718884909785-0.0.1.1-604800000",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271667,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.1525244793.1718885177",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1753445177.271482,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_4P31SJ70EJ",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718885177.1.0.1718885177.0.0.0",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1718971576,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.854907463.1718885177",
    },
    {
        "domain": ".www.researchgate.net",
        "expirationDate": 1750407982.506505,
        "hostOnly": False,
        "httpOnly": True,
        "name": "did",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "1dWLO3C6am8l667Q4VUlBo0O1LI49Qi2Vw21SJEXHavBDYT56DI9007W5rYGVFVH",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750507578,
        "hostOnly": False,
        "httpOnly": False,
        "name": "didomi_token",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "eyJ1c2VyX2lkIjoiMTkwMzU4YTUtNWU2My02Y2UzLWJlNzAtZGFjNzVmYjdiY2ExIiwiY3JlYXRlZCI6IjIwMjQtMDYtMjBUMTI6MDY6MTYuODA2WiIsInVwZGF0ZWQiOiIyMDI0LTA2LTIwVDEyOjA2OjE4Ljc4MVoiLCJ2ZW5kb3JzIjp7ImVuYWJsZWQiOlsidHdpdHRlciIsImdvb2dsZSIsImM6bGlua2VkaW4tbWFya2V0aW5nLXNvbHV0aW9ucyIsImM6b3duZXJpcSIsImM6b21uaXR1cmUtYWRvYmUtYW5hbHl0aWNzIiwiYzp0ZWNobm9yYXRpLW1lZGlhIiwiYzppbnRlcmNvbSIsImM6aW50ZW50LWlxIiwiYzppcHJvbSIsImM6bGlua2VkaW4iLCJjOmFtYXpvbmFkdi16Y1hGTEI2WCIsImM6bWVkaWFuZXQtY1V3YUtFNnoiLCJjOmluZGV4ZXhjaC1OWkNRTTY4UCIsImM6emVvdGFwZ21iLWQ3YndtdGp3IiwiYzp0cmlwbGVsaWYtZGRKSDM0clkiLCJjOnJ0YmhvdXNlLWI4Y2RIOHRNIiwiYzptZHByaW1pcy1lYU4yOVdjUCIsImM6bG9vcG1lbGktVGRhWXRCUHEiLCJjOm1hZ25pdGVpbi05d1RZTHFSRCIsImM6Ymlkc3dpdGNoLWQ2N0V3N1c5IiwiYzpvcmFjbGVhZHYtcUhlREptQUwiLCJjOmdvb2dsZWFuYS00VFhuSmlnUiIsImM6bG90YW1lc29sLURIaTdMUmpNIiwiYzpuZXh0bWlsbGUtR0pyZlg4VWMiLCJjOm5yaWNodGVjLXFVVlEyUlFxIiwiYzpicml0ZXBvb2wtQldWeVdHeVUiLCJjOnRhcGFkaW5jLXFxY2tVN1BXIiwiYzppZDV0ZWNobi16Tk1KNGR3ZiIsImM6bWljcm9zb2Z0IiwiYzpwZXJtdXRpdmUtSjdpaHJlTWsiLCJjOm9wZXJhc29mdC1CY1hjRFZKTSIsImM6cG9zdGhvZy1Cakp4RmRGOSJdfSwicHVycG9zZXMiOnsiZW5hYmxlZCI6WyJnZW9sb2NhdGlvbl9kYXRhIiwiZGV2aWNlX2NoYXJhY3RlcmlzdGljcyJdfSwidmVuZG9yc19saSI6eyJlbmFibGVkIjpbImdvb2dsZSIsImM6b3BlcmFzb2Z0LUJjWGNEVkpNIl19LCJ2ZXJzaW9uIjoyLCJhYyI6IkRIU0FvQUZrQWNnQTVnSHFnUUhBeGdCNndEMTRJR0FRTkFqMEJJd0NTY0VyQUtCd1YtZ3MxQmgwREc0R09nQUEuREhTQW9BRmtBY2dBNWdIcWdRSEF4Z0I2d0QxNElHQVFOQWowQkl3Q1NjRXJBS0J3Vi1nczFCaDBERzRHT2dBQSJ9",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "hasPdpNext",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "False",
    },
    {
        "domain": ".researchgate.net",
        "expirationDate": 1750421183,
        "hostOnly": False,
        "httpOnly": False,
        "name": "ph_phc_ma1XTQyee96N1GML6qUTgLQRiDifnRcE9STiHTZ0CfZ_posthog",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "%7B%22distinct_id%22%3A%220190358a-56a1-7313-83b0-d13dddeac787%22%2C%22%24sesid%22%3A%5B1718885183223%2C%220190358a-56a1-7313-83b0-d13b2b87778d%22%2C1718885176993%5D%2C%22%24session_is_sampled%22%3Atrue%7D",
    },
    {
        "domain": ".www.researchgate.net",
        "hostOnly": False,
        "httpOnly": True,
        "name": "sid",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "qmH5Lc4f0CUJ3zeaxORcV0S8I8V1MuCFZtcIQqPYtv1XPejrbSLAQRbT50PL40TqeKQ1XsQDWt9gtYVzuL80bRmPjw6jn3cQ0ikNqW40maHcQ3JL2Vfa8ZZf0j7p35eJ",
    },
]

COOKIES_LIST += [
    {
        "domain": "github.com",
        "hostOnly": True,
        "httpOnly": True,
        "name": "_gh_sess",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "P%2Fmof1avuqwHaUQUIJR%2FZYn7jqbT7lgGuTGjp1BGAFIG5UpNDusEE3b8dRjz0eATE5xPdPjLYFqMs%2FI9AOalKX4YuYfSEEnxCMawU01099b4o9Xzzcv%2BmecrmO0Q8q%2Bdq1h8SIv6nvPP7HzlFesl8ysafb9b%2F0q6dTArKdSOurasza8UgLSYD08ofA50Pcm0IG7CTzF8ZCizrGgGTMi%2F%2B7L3E17jav5PM1Sf2vQKg15Gbg1QIOppJJHzlufgQoZigqFv%2BWznaws0Tt7Y2lSFCw%3D%3D--CJRhqMXJnwOaJgk4--DhUErlL4GdROikEjKD4O9g%3D%3D",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763785,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_octo",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "GH1.1.728652011.1718872875",
    },
    {
        "domain": ".github.com",
        "expirationDate": 1750408875.763926,
        "hostOnly": False,
        "httpOnly": True,
        "name": "logged_in",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": False,
        "storeId": None,
        "value": "no",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "preferred_color_mode",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "dark",
    },
    {
        "domain": ".github.com",
        "hostOnly": False,
        "httpOnly": False,
        "name": "tz",
        "path": "/",
        "sameSite": "lax",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "Europe%2FParis",
    },
]

COOKIES_LIST += [
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718886430,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gat",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718972770,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_gid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.402246368.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1753446370.315621,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.2.1301409987.1606169825",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1750422367,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjid",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "07f80263-a631-4bf4-8ffd-8fc8912085e2",
    },
    {
        "domain": ".web.archive.org",
        "expirationDate": 1718888167,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_hjFirstSeen",
        "path": "/web/20201123221659/http://orcid.org/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "1",
    },
]
COOKIES_LIST += [
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELBCORS",
        "path": "/",
        "sameSite": "no_restriction",
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.637671,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga_9R61FWK9H5",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GS1.1.1718892454.1.0.1718892454.0.0.0",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1753452454.63421,
        "hostOnly": False,
        "httpOnly": False,
        "name": "_ga",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "GA1.1.2021310691.1718892455",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "AWSELB",
        "path": "/",
        "sameSite": None,
        "secure": False,
        "session": True,
        "storeId": None,
        "value": "CBD1D7FF1216388FA48838CBCA4774FD22800B8FB548A40EF92BB0994D5B77A8410307CDEAA69C52236663F2BF89B252C17BC0FCDF790FD59771BDDF6EA8CA4CFD29D8733F",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonAlertBoxClosed",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "2024-06-20T14:07:34.583Z",
    },
    {
        "domain": ".orcid.org",
        "expirationDate": 1750428454,
        "hostOnly": False,
        "httpOnly": False,
        "name": "OptanonConsent",
        "path": "/",
        "sameSite": "lax",
        "secure": False,
        "session": False,
        "storeId": None,
        "value": "isGpcEnabled=0&datestamp=Thu+Jun+20+2024+16%3A07%3A34+GMT%2B0200+(heure+d%E2%80%99%C3%A9t%C3%A9+d%E2%80%99Europe+centrale)&version=202310.2.0&browserGpcFlag=0&isIABGlobal=False&hosts=&landingPath=NotLandingPage&groups=C0001%3A1%2CC0003%3A1%2CC0002%3A1%2CC0004%3A1",
    },
    {
        "domain": "orcid.org",
        "hostOnly": True,
        "httpOnly": False,
        "name": "XSRF-TOKEN",
        "path": "/",
        "sameSite": None,
        "secure": True,
        "session": True,
        "storeId": None,
        "value": "6957be7a-bcb4-4d59-a522-ea9b6b210ed9",
    },
]

# Create a RequestsCookieJar instance
COOKIES = RequestsCookieJar()

# Add cookies to the jar
for cookie in COOKIES_LIST:
    COOKIES.set(cookie["name"], cookie["value"], domain=cookie["domain"], path=cookie["path"])



================================================
FILE: examples/open_deep_research/scripts/gaia_scorer.py
================================================
import re
import string
import warnings


def normalize_number_str(number_str: str) -> float:
    # we replace these common units and commas to allow
    # conversion to float
    for char in ["$", "%", ","]:
        number_str = number_str.replace(char, "")
    try:
        return float(number_str)
    except ValueError:
        print(f"String {number_str} cannot be normalized to number str.")
        return float("inf")


def split_string(
    s: str,
    char_list: list[str] = [",", ";"],
) -> list[str]:
    pattern = f"[{''.join(char_list)}]"
    return re.split(pattern, s)


def is_float(element: any) -> bool:
    try:
        float(element)
        return True
    except ValueError:
        return False


def question_scorer(
    model_answer: str,
    ground_truth: str,
) -> bool:
    # if gt is a number
    if is_float(ground_truth):
        normalized_answer = normalize_number_str(str(model_answer))
        return normalized_answer == float(ground_truth)

    # if gt is a list
    elif any(char in ground_truth for char in [",", ";"]):
        # question with the fish: normalization removes punct

        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)

        # check length is the same
        if len(gt_elems) != len(ma_elems):
            warnings.warn("Answer lists have different lengths, returning False.", UserWarning)
            return False

        # compare each element as float or str
        comparisons = []
        for ma_elem, gt_elem in zip(ma_elems, gt_elems):
            if is_float(gt_elem):
                normalized_ma_elem = normalize_number_str(ma_elem)
                comparisons.append(normalized_ma_elem == float(gt_elem))
            else:
                # we do not remove punct since comparisons can include punct
                comparisons.append(
                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)
                )
        return all(comparisons)

    # if gt is a str
    else:
        return normalize_str(model_answer) == normalize_str(ground_truth)


def check_prediction_contains_answer_letters_in_order(prediction, true_answer):
    prediction = prediction.lower()
    true_answer = true_answer.lower()
    if len(prediction) > len(true_answer) * 3:
        return False
    i = 0
    for letter in true_answer:
        if letter in prediction[i:]:
            i += prediction[i:].index(letter)
        else:
            return False
    return True


def check_close_call(prediction, true_answer, is_correct):
    if is_correct:
        return True
    else:
        if is_float(true_answer):
            return is_correct
        else:
            if (
                check_prediction_contains_answer_letters_in_order(str(prediction), str(true_answer))
                and len(str(true_answer)) * 0.5 <= len(str(prediction)) <= len(str(true_answer)) * 2
            ):
                print(f"Close call: {prediction} vs {true_answer}")
                return True
            else:
                return False


def normalize_str(input_str, remove_punct=True) -> str:
    """
    Normalize a string by:
    - Removing all white spaces
    - Optionally removing punctuation (if remove_punct is True)
    - Converting to lowercase
    Parameters:
    - input_str: str, the string to normalize
    - remove_punct: bool, whether to remove punctuation (default: True)
    Returns:
    - str, the normalized string
    """
    # Remove all white spaces. Required e.g for seagull vs. sea gull
    no_spaces = re.sub(r"\s", "", input_str)

    # Remove punctuation, if specified.
    if remove_punct:
        translator = str.maketrans("", "", string.punctuation)
        return no_spaces.lower().translate(translator)
    else:
        return no_spaces.lower()



================================================
FILE: examples/open_deep_research/scripts/mdconvert.py
================================================
# This is copied from Magentic-one's great repo: https://github.com/microsoft/autogen/blob/v0.4.4/python/packages/autogen-magentic-one/src/autogen_magentic_one/markdown_browser/mdconvert.py
# Thanks to Microsoft researchers for open-sourcing this!
# type: ignore
import base64
import copy
import html
import json
import mimetypes
import os
import re
import shutil
import subprocess
import sys
import tempfile
import traceback
import zipfile
from typing import Any
from urllib.parse import parse_qs, quote, unquote, urlparse, urlunparse

import mammoth
import markdownify
import pandas as pd
import pdfminer
import pdfminer.high_level
import pptx

# File-format detection
import puremagic
import pydub
import requests
import speech_recognition as sr
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import SRTFormatter


class _CustomMarkdownify(markdownify.MarkdownConverter):
    """
    A custom version of markdownify's MarkdownConverter. Changes include:

    - Altering the default heading style to use '#', '##', etc.
    - Removing javascript hyperlinks.
    - Truncating images with large data:uri sources.
    - Ensuring URIs are properly escaped, and do not conflict with Markdown syntax
    """

    def __init__(self, **options: Any):
        options["heading_style"] = options.get("heading_style", markdownify.ATX)
        # Explicitly cast options to the expected type if necessary
        super().__init__(**options)

    def convert_hn(self, n: int, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual, but be sure to start with a new line"""
        if not convert_as_inline:
            if not re.search(r"^\n", text):
                return "\n" + super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

        return super().convert_hn(n, el, text, convert_as_inline)  # type: ignore

    def convert_a(self, el: Any, text: str, convert_as_inline: bool):
        """Same as usual converter, but removes Javascript links and escapes URIs."""
        prefix, suffix, text = markdownify.chomp(text)  # type: ignore
        if not text:
            return ""
        href = el.get("href")
        title = el.get("title")

        # Escape URIs and skip non-http or file schemes
        if href:
            try:
                parsed_url = urlparse(href)  # type: ignore
                if parsed_url.scheme and parsed_url.scheme.lower() not in ["http", "https", "file"]:  # type: ignore
                    return "%s%s%s" % (prefix, text, suffix)
                href = urlunparse(parsed_url._replace(path=quote(unquote(parsed_url.path))))  # type: ignore
            except ValueError:  # It's not clear if this ever gets thrown
                return "%s%s%s" % (prefix, text, suffix)

        # For the replacement see #29: text nodes underscores are escaped
        if (
            self.options["autolinks"]
            and text.replace(r"\_", "_") == href
            and not title
            and not self.options["default_title"]
        ):
            # Shortcut syntax
            return "<%s>" % href
        if self.options["default_title"] and not title:
            title = href
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        return "%s[%s](%s%s)%s" % (prefix, text, href, title_part, suffix) if href else text

    def convert_img(self, el: Any, text: str, convert_as_inline: bool) -> str:
        """Same as usual converter, but removes data URIs"""

        alt = el.attrs.get("alt", None) or ""
        src = el.attrs.get("src", None) or ""
        title = el.attrs.get("title", None) or ""
        title_part = ' "%s"' % title.replace('"', r"\"") if title else ""
        if convert_as_inline and el.parent.name not in self.options["keep_inline_images_in"]:
            return alt

        # Remove dataURIs
        if src.startswith("data:"):
            src = src.split(",")[0] + "..."

        return "![%s](%s%s)" % (alt, src, title_part)

    def convert_soup(self, soup: Any) -> str:
        return super().convert_soup(soup)  # type: ignore


class DocumentConverterResult:
    """The result of converting a document to text."""

    def __init__(self, title: str | None = None, text_content: str = ""):
        self.title: str | None = title
        self.text_content: str = text_content


class DocumentConverter:
    """Abstract superclass of all DocumentConverters."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        raise NotImplementedError()


class PlainTextConverter(DocumentConverter):
    """Anything with content type text/plain"""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Guess the content type from any file extension that might be around
        content_type, _ = mimetypes.guess_type("__placeholder" + kwargs.get("file_extension", ""))

        # Only accept text files
        if content_type is None:
            return None
        # elif "text/" not in content_type.lower():
        #     return None

        text_content = ""
        with open(local_path, "rt", encoding="utf-8") as fh:
            text_content = fh.read()
        return DocumentConverterResult(
            title=None,
            text_content=text_content,
        )


class HtmlConverter(DocumentConverter):
    """Anything with content type text/html"""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not html
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None

        result = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            result = self._convert(fh.read())

        return result

    def _convert(self, html_content: str) -> None | DocumentConverterResult:
        """Helper function that converts and HTML string."""

        # Parse the string
        soup = BeautifulSoup(html_content, "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("body")
        webpage_text = ""
        if body_elm:
            webpage_text = _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        assert isinstance(webpage_text, str)

        return DocumentConverterResult(
            title=None if soup.title is None else soup.title.string, text_content=webpage_text
        )


class WikipediaConverter(DocumentConverter):
    """Handle Wikipedia pages separately, focusing only on the main document content."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not Wikipedia
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not re.search(r"^https?:\/\/[a-zA-Z]{2,3}\.wikipedia.org\/", url):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Remove javascript and style blocks
        for script in soup(["script", "style"]):
            script.extract()

        # Print only the main content
        body_elm = soup.find("div", {"id": "mw-content-text"})
        title_elm = soup.find("span", {"class": "mw-page-title-main"})

        webpage_text = ""
        main_title = None if soup.title is None else soup.title.string

        if body_elm:
            # What's the title
            if title_elm and len(title_elm) > 0:
                main_title = title_elm.string  # type: ignore
                assert isinstance(main_title, str)

            # Convert the page
            webpage_text = f"# {main_title}\n\n" + _CustomMarkdownify().convert_soup(body_elm)
        else:
            webpage_text = _CustomMarkdownify().convert_soup(soup)

        return DocumentConverterResult(
            title=main_title,
            text_content=webpage_text,
        )


class YouTubeConverter(DocumentConverter):
    """Handle YouTube specially, focusing on the video title, description, and transcript."""

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not YouTube
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".html", ".htm"]:
            return None
        url = kwargs.get("url", "")
        if not url.startswith("https://www.youtube.com/watch?"):
            return None

        # Parse the file
        soup = None
        with open(local_path, "rt", encoding="utf-8") as fh:
            soup = BeautifulSoup(fh.read(), "html.parser")

        # Read the meta tags
        assert soup.title is not None and soup.title.string is not None
        metadata: dict[str, str] = {"title": soup.title.string}
        for meta in soup(["meta"]):
            for a in meta.attrs:
                if a in ["itemprop", "property", "name"]:
                    metadata[meta[a]] = meta.get("content", "")
                    break

        # We can also try to read the full description. This is more prone to breaking, since it reaches into the page implementation
        try:
            for script in soup(["script"]):
                content = script.text
                if "ytInitialData" in content:
                    lines = re.split(r"\r?\n", content)
                    obj_start = lines[0].find("{")
                    obj_end = lines[0].rfind("}")
                    if obj_start >= 0 and obj_end >= 0:
                        data = json.loads(lines[0][obj_start : obj_end + 1])
                        attrdesc = self._findKey(data, "attributedDescriptionBodyText")  # type: ignore
                        if attrdesc:
                            metadata["description"] = str(attrdesc["content"])
                    break
        except Exception:
            pass

        # Start preparing the page
        webpage_text = "# YouTube\n"

        title = self._get(metadata, ["title", "og:title", "name"])  # type: ignore
        assert isinstance(title, str)

        if title:
            webpage_text += f"\n## {title}\n"

        stats = ""
        views = self._get(metadata, ["interactionCount"])  # type: ignore
        if views:
            stats += f"- **Views:** {views}\n"

        keywords = self._get(metadata, ["keywords"])  # type: ignore
        if keywords:
            stats += f"- **Keywords:** {keywords}\n"

        runtime = self._get(metadata, ["duration"])  # type: ignore
        if runtime:
            stats += f"- **Runtime:** {runtime}\n"

        if len(stats) > 0:
            webpage_text += f"\n### Video Metadata\n{stats}\n"

        description = self._get(metadata, ["description", "og:description"])  # type: ignore
        if description:
            webpage_text += f"\n### Description\n{description}\n"

        transcript_text = ""
        parsed_url = urlparse(url)  # type: ignore
        params = parse_qs(parsed_url.query)  # type: ignore
        if "v" in params:
            assert isinstance(params["v"][0], str)
            video_id = str(params["v"][0])
            try:
                # Must be a single transcript.
                transcript = YouTubeTranscriptApi.get_transcript(video_id)  # type: ignore
                # transcript_text = " ".join([part["text"] for part in transcript])  # type: ignore
                # Alternative formatting:
                transcript_text = SRTFormatter().format_transcript(transcript)
            except Exception:
                pass
        if transcript_text:
            webpage_text += f"\n### Transcript\n{transcript_text}\n"

        title = title if title else soup.title.string
        assert isinstance(title, str)

        return DocumentConverterResult(
            title=title,
            text_content=webpage_text,
        )

    def _get(self, metadata: dict[str, str], keys: list[str], default: str | None = None) -> str | None:
        for k in keys:
            if k in metadata:
                return metadata[k]
        return default

    def _findKey(self, json: Any, key: str) -> str | None:  # TODO: Fix json type
        if isinstance(json, list):
            for elm in json:
                ret = self._findKey(elm, key)
                if ret is not None:
                    return ret
        elif isinstance(json, dict):
            for k in json:
                if k == key:
                    return json[k]
                else:
                    ret = self._findKey(json[k], key)
                    if ret is not None:
                        return ret
        return None


class PdfConverter(DocumentConverter):
    """
    Converts PDFs to Markdown. Most style information is ignored, so the results are essentially plain-text.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a PDF
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pdf":
            return None

        return DocumentConverterResult(
            title=None,
            text_content=pdfminer.high_level.extract_text(local_path),
        )


class DocxConverter(HtmlConverter):
    """
    Converts DOCX files to Markdown. Style information (e.g.m headings) and tables are preserved where possible.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a DOCX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".docx":
            return None

        result = None
        with open(local_path, "rb") as docx_file:
            result = mammoth.convert_to_html(docx_file)
            html_content = result.value
            result = self._convert(html_content)

        return result


class XlsxConverter(HtmlConverter):
    """
    Converts XLSX files to Markdown, with each sheet presented as a separate Markdown table.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".xlsx", ".xls"]:
            return None

        sheets = pd.read_excel(local_path, sheet_name=None)
        md_content = ""
        for s in sheets:
            md_content += f"## {s}\n"
            html_content = sheets[s].to_html(index=False)
            md_content += self._convert(html_content).text_content.strip() + "\n\n"

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )


class PptxConverter(HtmlConverter):
    """
    Converts PPTX files to Markdown. Supports heading, tables and images with alt text.
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a PPTX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".pptx":
            return None

        md_content = ""

        presentation = pptx.Presentation(local_path)
        slide_num = 0
        for slide in presentation.slides:
            slide_num += 1

            md_content += f"\n\n<!-- Slide number: {slide_num} -->\n"

            title = slide.shapes.title
            for shape in slide.shapes:
                # Pictures
                if self._is_picture(shape):
                    # https://github.com/scanny/python-pptx/pull/512#issuecomment-1713100069
                    alt_text = ""
                    try:
                        alt_text = shape._element._nvXxPr.cNvPr.attrib.get("descr", "")
                    except Exception:
                        pass

                    # A placeholder name
                    filename = re.sub(r"\W", "", shape.name) + ".jpg"
                    md_content += "\n![" + (alt_text if alt_text else shape.name) + "](" + filename + ")\n"

                # Tables
                if self._is_table(shape):
                    html_table = "<html><body><table>"
                    first_row = True
                    for row in shape.table.rows:
                        html_table += "<tr>"
                        for cell in row.cells:
                            if first_row:
                                html_table += "<th>" + html.escape(cell.text) + "</th>"
                            else:
                                html_table += "<td>" + html.escape(cell.text) + "</td>"
                        html_table += "</tr>"
                        first_row = False
                    html_table += "</table></body></html>"
                    md_content += "\n" + self._convert(html_table).text_content.strip() + "\n"

                # Text areas
                elif shape.has_text_frame:
                    if shape == title:
                        md_content += "# " + shape.text.lstrip() + "\n"
                    else:
                        md_content += shape.text + "\n"

            md_content = md_content.strip()

            if slide.has_notes_slide:
                md_content += "\n\n### Notes:\n"
                notes_frame = slide.notes_slide.notes_text_frame
                if notes_frame is not None:
                    md_content += notes_frame.text
                md_content = md_content.strip()

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )

    def _is_picture(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PICTURE:
            return True
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.PLACEHOLDER:
            if hasattr(shape, "image"):
                return True
        return False

    def _is_table(self, shape):
        if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.TABLE:
            return True
        return False


class MediaConverter(DocumentConverter):
    """
    Abstract class for multi-modal media (e.g., images and audio)
    """

    def _get_metadata(self, local_path):
        exiftool = shutil.which("exiftool")
        if not exiftool:
            return None
        else:
            try:
                result = subprocess.run([exiftool, "-json", local_path], capture_output=True, text=True).stdout
                return json.loads(result)[0]
            except Exception:
                return None


class WavConverter(MediaConverter):
    """
    Converts WAV files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` is installed).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".wav":
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        try:
            transcript = self._transcribe_audio(local_path)
            md_content += "\n\n### Audio Transcript:\n" + ("[No speech detected]" if transcript == "" else transcript)
        except Exception:
            md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )

    def _transcribe_audio(self, local_path) -> str:
        recognizer = sr.Recognizer()
        with sr.AudioFile(local_path) as source:
            audio = recognizer.record(source)
            return recognizer.recognize_google(audio).strip()


class Mp3Converter(WavConverter):
    """
    Converts MP3 and M4A files to markdown via extraction of metadata (if `exiftool` is installed), and speech transcription (if `speech_recognition` AND `pydub` are installed).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a MP3
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".mp3", ".m4a"]:
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "Title",
                "Artist",
                "Author",
                "Band",
                "Album",
                "Genre",
                "Track",
                "DateTimeOriginal",
                "CreateDate",
                "Duration",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Transcribe
        handle, temp_path = tempfile.mkstemp(suffix=".wav")
        os.close(handle)
        try:
            if extension.lower() == ".mp3":
                sound = pydub.AudioSegment.from_mp3(local_path)
            else:
                sound = pydub.AudioSegment.from_file(local_path, format="m4a")
            sound.export(temp_path, format="wav")

            _args = dict()
            _args.update(kwargs)
            _args["file_extension"] = ".wav"

            try:
                transcript = super()._transcribe_audio(temp_path).strip()
                md_content += "\n\n### Audio Transcript:\n" + (
                    "[No speech detected]" if transcript == "" else transcript
                )
            except Exception:
                md_content += "\n\n### Audio Transcript:\nError. Could not transcribe this audio."

        finally:
            os.unlink(temp_path)

        # Return the result
        return DocumentConverterResult(
            title=None,
            text_content=md_content.strip(),
        )


class ZipConverter(DocumentConverter):
    """
    Extracts ZIP files to a permanent local directory and returns a listing of extracted files.
    """

    def __init__(self, extract_dir: str = "downloads"):
        """
        Initialize with path to extraction directory.

        Args:
            extract_dir: The directory where files will be extracted. Defaults to "downloads"
        """
        self.extract_dir = extract_dir
        # Create the extraction directory if it doesn't exist
        os.makedirs(self.extract_dir, exist_ok=True)

    def convert(self, local_path: str, **kwargs: Any) -> None | DocumentConverterResult:
        # Bail if not a ZIP file
        extension = kwargs.get("file_extension", "")
        if extension.lower() != ".zip":
            return None

        # Verify it's actually a ZIP file
        if not zipfile.is_zipfile(local_path):
            return None

        # Extract all files and build list
        extracted_files = []
        with zipfile.ZipFile(local_path, "r") as zip_ref:
            # Extract all files
            zip_ref.extractall(self.extract_dir)
            # Get list of all files
            for file_path in zip_ref.namelist():
                # Skip directories
                if not file_path.endswith("/"):
                    extracted_files.append(self.extract_dir + "/" + file_path)

        # Sort files for consistent output
        extracted_files.sort()

        # Build the markdown content
        md_content = "Downloaded the following files:\n"
        for file in extracted_files:
            md_content += f"* {file}\n"

        return DocumentConverterResult(title="Extracted Files", text_content=md_content.strip())


class ImageConverter(MediaConverter):
    """
    Converts images to markdown via extraction of metadata (if `exiftool` is installed), OCR (if `easyocr` is installed), and description via a multimodal LLM (if an mlm_client is configured).
    """

    def convert(self, local_path, **kwargs) -> None | DocumentConverterResult:
        # Bail if not a XLSX
        extension = kwargs.get("file_extension", "")
        if extension.lower() not in [".jpg", ".jpeg", ".png"]:
            return None

        md_content = ""

        # Add metadata
        metadata = self._get_metadata(local_path)
        if metadata:
            for f in [
                "ImageSize",
                "Title",
                "Caption",
                "Description",
                "Keywords",
                "Artist",
                "Author",
                "DateTimeOriginal",
                "CreateDate",
                "GPSPosition",
            ]:
                if f in metadata:
                    md_content += f"{f}: {metadata[f]}\n"

        # Try describing the image with GPTV
        mlm_client = kwargs.get("mlm_client")
        mlm_model = kwargs.get("mlm_model")
        if mlm_client is not None and mlm_model is not None:
            md_content += (
                "\n# Description:\n"
                + self._get_mlm_description(
                    local_path, extension, mlm_client, mlm_model, prompt=kwargs.get("mlm_prompt")
                ).strip()
                + "\n"
            )

        return DocumentConverterResult(
            title=None,
            text_content=md_content,
        )

    def _get_mlm_description(self, local_path, extension, client, model, prompt=None):
        if prompt is None or prompt.strip() == "":
            prompt = "Write a detailed caption for this image."

        sys.stderr.write(f"MLM Prompt:\n{prompt}\n")

        data_uri = ""
        with open(local_path, "rb") as image_file:
            content_type, encoding = mimetypes.guess_type("_dummy" + extension)
            if content_type is None:
                content_type = "image/jpeg"
            image_base64 = base64.b64encode(image_file.read()).decode("utf-8")
            data_uri = f"data:{content_type};base64,{image_base64}"

        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": data_uri,
                        },
                    },
                ],
            }
        ]

        response = client.chat.completions.create(model=model, messages=messages)
        return response.choices[0].message.content


class FileConversionException(Exception):
    pass


class UnsupportedFormatException(Exception):
    pass


class MarkdownConverter:
    """(In preview) An extremely simple text-based document reader, suitable for LLM use.
    This reader will convert common file-types or webpages to Markdown."""

    def __init__(
        self,
        requests_session: requests.Session | None = None,
        mlm_client: Any | None = None,
        mlm_model: Any | None = None,
    ):
        if requests_session is None:
            self._requests_session = requests.Session()
        else:
            self._requests_session = requests_session

        self._mlm_client = mlm_client
        self._mlm_model = mlm_model

        self._page_converters: list[DocumentConverter] = []

        # Register converters for successful browsing operations
        # Later registrations are tried first / take higher priority than earlier registrations
        # To this end, the most specific converters should appear below the most generic converters
        self.register_page_converter(PlainTextConverter())
        self.register_page_converter(HtmlConverter())
        self.register_page_converter(WikipediaConverter())
        self.register_page_converter(YouTubeConverter())
        self.register_page_converter(DocxConverter())
        self.register_page_converter(XlsxConverter())
        self.register_page_converter(PptxConverter())
        self.register_page_converter(WavConverter())
        self.register_page_converter(Mp3Converter())
        self.register_page_converter(ImageConverter())
        self.register_page_converter(ZipConverter())
        self.register_page_converter(PdfConverter())

    def convert(
        self, source: str | requests.Response, **kwargs: Any
    ) -> DocumentConverterResult:  # TODO: deal with kwargs
        """
        Args:
            - source: can be a string representing a path or url, or a requests.response object
            - extension: specifies the file extension to use when interpreting the file. If None, infer from source (path, uri, content-type, etc.)
        """

        # Local path or url
        if isinstance(source, str):
            if source.startswith("http://") or source.startswith("https://") or source.startswith("file://"):
                return self.convert_url(source, **kwargs)
            else:
                return self.convert_local(source, **kwargs)
        # Request response
        elif isinstance(source, requests.Response):
            return self.convert_response(source, **kwargs)

    def convert_local(self, path: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Get extension alternatives from the path and puremagic
        base, ext = os.path.splitext(path)
        self._append_ext(extensions, ext)
        self._append_ext(extensions, self._guess_ext_magic(path))

        # Convert
        return self._convert(path, extensions, **kwargs)

    # TODO what should stream's type be?
    def convert_stream(self, stream: Any, **kwargs: Any) -> DocumentConverterResult:  # TODO: deal with kwargs
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Write to the temporary file
            content = stream.read()
            if isinstance(content, str):
                fh.write(content.encode("utf-8"))
            else:
                fh.write(content)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, **kwargs)
        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def convert_url(self, url: str, **kwargs: Any) -> DocumentConverterResult:  # TODO: fix kwargs type
        # Send a HTTP request to the URL
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
        response = self._requests_session.get(url, stream=True, headers={"User-Agent": user_agent})
        response.raise_for_status()
        return self.convert_response(response, **kwargs)

    def convert_response(
        self, response: requests.Response, **kwargs: Any
    ) -> DocumentConverterResult:  # TODO fix kwargs type
        # Prepare a list of extensions to try (in order of priority)
        ext = kwargs.get("file_extension")
        extensions = [ext] if ext is not None else []

        # Guess from the mimetype
        content_type = response.headers.get("content-type", "").split(";")[0]
        self._append_ext(extensions, mimetypes.guess_extension(content_type))

        # Read the content disposition if there is one
        content_disposition = response.headers.get("content-disposition", "")
        m = re.search(r"filename=([^;]+)", content_disposition)
        if m:
            base, ext = os.path.splitext(m.group(1).strip("\"'"))
            self._append_ext(extensions, ext)

        # Read from the extension from the path
        base, ext = os.path.splitext(urlparse(response.url).path)
        self._append_ext(extensions, ext)

        # Save the file locally to a temporary file. It will be deleted before this method exits
        handle, temp_path = tempfile.mkstemp()
        fh = os.fdopen(handle, "wb")
        result = None
        try:
            # Download the file
            for chunk in response.iter_content(chunk_size=512):
                fh.write(chunk)
            fh.close()

            # Use puremagic to check for more extension options
            self._append_ext(extensions, self._guess_ext_magic(temp_path))

            # Convert
            result = self._convert(temp_path, extensions, url=response.url)
        except Exception as e:
            print(f"Error in converting: {e}")

        # Clean up
        finally:
            try:
                fh.close()
            except Exception:
                pass
            os.unlink(temp_path)

        return result

    def _convert(self, local_path: str, extensions: list[str | None], **kwargs) -> DocumentConverterResult:
        error_trace = ""
        for ext in extensions + [None]:  # Try last with no extension
            for converter in self._page_converters:
                _kwargs = copy.deepcopy(kwargs)

                # Overwrite file_extension appropriately
                if ext is None:
                    if "file_extension" in _kwargs:
                        del _kwargs["file_extension"]
                else:
                    _kwargs.update({"file_extension": ext})

                # Copy any additional global options
                if "mlm_client" not in _kwargs and self._mlm_client is not None:
                    _kwargs["mlm_client"] = self._mlm_client

                if "mlm_model" not in _kwargs and self._mlm_model is not None:
                    _kwargs["mlm_model"] = self._mlm_model

                # If we hit an error log it and keep trying
                try:
                    res = converter.convert(local_path, **_kwargs)
                except Exception:
                    error_trace = ("\n\n" + traceback.format_exc()).strip()

                if res is not None:
                    # Normalize the content
                    res.text_content = "\n".join([line.rstrip() for line in re.split(r"\r?\n", res.text_content)])
                    res.text_content = re.sub(r"\n{3,}", "\n\n", res.text_content)

                    # Todo
                    return res

        # If we got this far without success, report any exceptions
        if len(error_trace) > 0:
            raise FileConversionException(
                f"Could not convert '{local_path}' to Markdown. File type was recognized as {extensions}. While converting the file, the following error was encountered:\n\n{error_trace}"
            )

        # Nothing can handle it!
        raise UnsupportedFormatException(
            f"Could not convert '{local_path}' to Markdown. The formats {extensions} are not supported."
        )

    def _append_ext(self, extensions, ext):
        """Append a unique non-None, non-empty extension to a list of extensions."""
        if ext is None:
            return
        ext = ext.strip()
        if ext == "":
            return
        # if ext not in extensions:
        if True:
            extensions.append(ext)

    def _guess_ext_magic(self, path):
        """Use puremagic (a Python implementation of libmagic) to guess a file's extension based on the first few bytes."""
        # Use puremagic to guess
        try:
            guesses = puremagic.magic_file(path)
            if len(guesses) > 0:
                ext = guesses[0].extension.strip()
                if len(ext) > 0:
                    return ext
        except FileNotFoundError:
            pass
        except IsADirectoryError:
            pass
        except PermissionError:
            pass
        return None

    def register_page_converter(self, converter: DocumentConverter) -> None:
        """Register a page text converter."""
        self._page_converters.insert(0, converter)



================================================
FILE: examples/open_deep_research/scripts/reformulator.py
================================================
# Shamelessly stolen from Microsoft Autogen team: thanks to them for this great resource!
# https://github.com/microsoft/autogen/blob/gaia_multiagent_v01_march_1st/autogen/browser_utils.py
import copy

from smolagents.models import MessageRole, Model


def prepare_response(original_task: str, inner_messages, reformulation_model: Model) -> str:
    messages = [
        {
            "role": MessageRole.SYSTEM,
            "content": [
                {
                    "type": "text",
                    "text": f"""Earlier you were asked the following:

{original_task}

Your team then worked diligently to address that request. Read below a transcript of that conversation:""",
                }
            ],
        }
    ]

    # The first message just repeats the question, so remove it
    # if len(inner_messages) > 1:
    #    del inner_messages[0]

    # copy them to this context
    try:
        for message in inner_messages:
            if not message.content:
                continue
            message = copy.deepcopy(message)
            message.role = MessageRole.USER
            messages.append(message)
    except Exception:
        messages += [{"role": MessageRole.ASSISTANT, "content": str(inner_messages)}]

    # ask for the final answer
    messages.append(
        {
            "role": MessageRole.USER,
            "content": [
                {
                    "type": "text",
                    "text": f"""
Read the above conversation and output a FINAL ANSWER to the question. The question is repeated here for convenience:

{original_task}

To output the final answer, use the following template: FINAL ANSWER: [YOUR FINAL ANSWER]
Your FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.
ADDITIONALLY, your FINAL ANSWER MUST adhere to any formatting instructions specified in the original question (e.g., alphabetization, sequencing, units, rounding, decimal places, etc.)
If you are asked for a number, express it numerically (i.e., with digits rather than words), don't use commas, and DO NOT INCLUDE UNITS such as $ or USD or percent signs unless specified otherwise.
If you are asked for a string, don't use articles or abbreviations (e.g. for cities), unless specified otherwise. Don't output any final sentence punctuation such as '.', '!', or '?'.
If you are asked for a comma separated list, apply the above rules depending on whether the elements are numbers or strings.
If you are unable to determine the final answer, output 'FINAL ANSWER: Unable to determine'
""",
                }
            ],
        }
    )

    response = reformulation_model(messages).content

    final_answer = response.split("FINAL ANSWER: ")[-1].strip()
    print("> Reformulated answer: ", final_answer)

    #     if "unable to determine" in final_answer.lower():
    #         messages.append({"role": MessageRole.ASSISTANT, "content": response })
    #         messages.append({"role": MessageRole.USER, "content": [{"type": "text", "text": """
    # I understand that a definitive answer could not be determined. Please make a well-informed EDUCATED GUESS based on the conversation.

    # To output the educated guess, use the following template: EDUCATED GUESS: [YOUR EDUCATED GUESS]
    # Your EDUCATED GUESS should be a number OR as few words as possible OR a comma separated list of numbers and/or strings. DO NOT OUTPUT 'I don't know', 'Unable to determine', etc.
    # ADDITIONALLY, your EDUCATED GUESS MUST adhere to any formatting instructions specified in the original question (e.g., alphabetization, sequencing, units, rounding, decimal places, etc.)
    # If you are asked for a number, express it numerically (i.e., with digits rather than words), don't use commas, and don't include units such as $ or percent signs unless specified otherwise.
    # If you are asked for a string, don't use articles or abbreviations (e.g. cit for cities), unless specified otherwise. Don't output any final sentence punctuation such as '.', '!', or '?'.
    # If you are asked for a comma separated list, apply the above rules depending on whether the elements are numbers or strings.
    # """.strip()}]})

    #         response = model(messages).content
    #         print("\n>>>Making an educated guess.\n", response)
    #         final_answer = response.split("EDUCATED GUESS: ")[-1].strip()
    return final_answer



================================================
FILE: examples/open_deep_research/scripts/run_agents.py
================================================
import json
import os
import shutil
import textwrap
from pathlib import Path

# import tqdm.asyncio
from smolagents.utils import AgentError


def serialize_agent_error(obj):
    if isinstance(obj, AgentError):
        return {"error_type": obj.__class__.__name__, "message": obj.message}
    else:
        return str(obj)


def get_image_description(file_name: str, question: str, visual_inspection_tool) -> str:
    prompt = f"""Write a caption of 5 sentences for this image. Pay special attention to any details that might be useful for someone answering the following question:
{question}. But do not try to answer the question directly!
Do not add any information that is not present in the image."""
    return visual_inspection_tool(image_path=file_name, question=prompt)


def get_document_description(file_path: str, question: str, document_inspection_tool) -> str:
    prompt = f"""Write a caption of 5 sentences for this document. Pay special attention to any details that might be useful for someone answering the following question:
{question}. But do not try to answer the question directly!
Do not add any information that is not present in the document."""
    return document_inspection_tool.forward_initial_exam_mode(file_path=file_path, question=prompt)


def get_single_file_description(file_path: str, question: str, visual_inspection_tool, document_inspection_tool):
    file_extension = file_path.split(".")[-1]
    if file_extension in ["png", "jpg", "jpeg"]:
        file_description = f" - Attached image: {file_path}"
        file_description += (
            f"\n     -> Image description: {get_image_description(file_path, question, visual_inspection_tool)}"
        )
        return file_description
    elif file_extension in ["pdf", "xls", "xlsx", "docx", "doc", "xml"]:
        image_path = file_path.split(".")[0] + ".png"
        if os.path.exists(image_path):
            description = get_image_description(image_path, question, visual_inspection_tool)
            file_path = image_path
        else:
            description = get_document_description(file_path, question, document_inspection_tool)
        file_description = f" - Attached document: {file_path}"
        file_description += f"\n     -> File description: {description}"
        return file_description
    elif file_extension in ["mp3", "m4a", "wav"]:
        return f" - Attached audio: {file_path}"
    else:
        return f" - Attached file: {file_path}"


def get_zip_description(file_path: str, question: str, visual_inspection_tool, document_inspection_tool):
    folder_path = file_path.replace(".zip", "")
    os.makedirs(folder_path, exist_ok=True)
    shutil.unpack_archive(file_path, folder_path)

    prompt_use_files = ""
    for root, dirs, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            prompt_use_files += "\n" + textwrap.indent(
                get_single_file_description(file_path, question, visual_inspection_tool, document_inspection_tool),
                prefix="    ",
            )
    return prompt_use_files


def get_tasks_to_run(data, total: int, base_filename: Path, tasks_ids: list[int]):
    f = base_filename.parent / f"{base_filename.stem}_answers.jsonl"
    done = set()
    if f.exists():
        with open(f, encoding="utf-8") as fh:
            done = {json.loads(line)["task_id"] for line in fh if line.strip()}

    tasks = []
    for i in range(total):
        task_id = int(data[i]["task_id"])
        if task_id not in done:
            if tasks_ids is not None:
                if task_id in tasks_ids:
                    tasks.append(data[i])
            else:
                tasks.append(data[i])
    return tasks



================================================
FILE: examples/open_deep_research/scripts/text_inspector_tool.py
================================================
from smolagents import Tool
from smolagents.models import Model


class TextInspectorTool(Tool):
    name = "inspect_file_as_text"
    description = """
You cannot load files yourself: instead call this tool to read a file as markdown text and ask questions about it.
This tool handles the following file extensions: [".html", ".htm", ".xlsx", ".pptx", ".wav", ".mp3", ".m4a", ".flac", ".pdf", ".docx"], and all other types of text files. IT DOES NOT HANDLE IMAGES."""

    inputs = {
        "file_path": {
            "description": "The path to the file you want to read as text. Must be a '.something' file, like '.pdf'. If it is an image, use the visualizer tool instead! DO NOT use this tool for an HTML webpage: use the web_search tool instead!",
            "type": "string",
        },
        "question": {
            "description": "[Optional]: Your question, as a natural language sentence. Provide as much context as possible. Do not pass this parameter if you just want to directly return the content of the file.",
            "type": "string",
            "nullable": True,
        },
    }
    output_type = "string"

    def __init__(self, model: Model = None, text_limit: int = 100000):
        super().__init__()
        self.model = model
        self.text_limit = text_limit
        from .mdconvert import MarkdownConverter

        self.md_converter = MarkdownConverter()

    def forward_initial_exam_mode(self, file_path, question):
        from smolagents.models import MessageRole

        result = self.md_converter.convert(file_path)

        if file_path[-4:] in [".png", ".jpg"]:
            raise Exception("Cannot use inspect_file_as_text tool with images: use visualizer instead!")

        if ".zip" in file_path:
            return result.text_content

        if not question:
            return result.text_content

        if len(result.text_content) < 4000:
            return "Document content: " + result.text_content

        messages = [
            {
                "role": MessageRole.SYSTEM,
                "content": [
                    {
                        "type": "text",
                        "text": "Here is a file:\n### "
                        + str(result.title)
                        + "\n\n"
                        + result.text_content[: self.text_limit],
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Now please write a short, 5 sentence caption for this document, that could help someone asking this question: "
                        + question
                        + "\n\nDon't answer the question yourself! Just provide useful notes on the document",
                    }
                ],
            },
        ]
        return self.model(messages).content

    def forward(self, file_path, question: str | None = None) -> str:
        from smolagents.models import MessageRole

        result = self.md_converter.convert(file_path)

        if file_path[-4:] in [".png", ".jpg"]:
            raise Exception("Cannot use inspect_file_as_text tool with images: use visualizer instead!")

        if ".zip" in file_path:
            return result.text_content

        if not question:
            return result.text_content

        messages = [
            {
                "role": MessageRole.SYSTEM,
                "content": [
                    {
                        "type": "text",
                        "text": "You will have to write a short caption for this file, then answer this question:"
                        + question,
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Here is the complete file:\n### "
                        + str(result.title)
                        + "\n\n"
                        + result.text_content[: self.text_limit],
                    }
                ],
            },
            {
                "role": MessageRole.USER,
                "content": [
                    {
                        "type": "text",
                        "text": "Now answer the question below. Use these three headings: '1. Short answer', '2. Extremely detailed answer', '3. Additional Context on the document and question asked'."
                        + question,
                    }
                ],
            },
        ]
        return self.model(messages).content



================================================
FILE: examples/open_deep_research/scripts/text_web_browser.py
================================================
# Shamelessly stolen from Microsoft Autogen team: thanks to them for this great resource!
# https://github.com/microsoft/autogen/blob/gaia_multiagent_v01_march_1st/autogen/browser_utils.py
import mimetypes
import os
import pathlib
import re
import time
import uuid
from typing import Any
from urllib.parse import unquote, urljoin, urlparse

import pathvalidate
import requests
from serpapi import GoogleSearch

from smolagents import Tool

from .cookies import COOKIES
from .mdconvert import FileConversionException, MarkdownConverter, UnsupportedFormatException


class SimpleTextBrowser:
    """(In preview) An extremely simple text-based web browser comparable to Lynx. Suitable for Agentic use."""

    def __init__(
        self,
        start_page: str | None = None,
        viewport_size: int | None = 1024 * 8,
        downloads_folder: str | None | None = None,
        serpapi_key: str | None | None = None,
        request_kwargs: dict[str, Any] | None | None = None,
    ):
        self.start_page: str = start_page if start_page else "about:blank"
        self.viewport_size = viewport_size  # Applies only to the standard uri types
        self.downloads_folder = downloads_folder
        self.history: list[tuple[str, float]] = list()
        self.page_title: str | None = None
        self.viewport_current_page = 0
        self.viewport_pages: list[tuple[int, int]] = list()
        self.set_address(self.start_page)
        self.serpapi_key = serpapi_key
        self.request_kwargs = request_kwargs
        self.request_kwargs["cookies"] = COOKIES
        self._mdconvert = MarkdownConverter()
        self._page_content: str = ""

        self._find_on_page_query: str | None = None
        self._find_on_page_last_result: int | None = None  # Location of the last result

    @property
    def address(self) -> str:
        """Return the address of the current page."""
        return self.history[-1][0]

    def set_address(self, uri_or_path: str, filter_year: int | None = None) -> None:
        # TODO: Handle anchors
        self.history.append((uri_or_path, time.time()))

        # Handle special URIs
        if uri_or_path == "about:blank":
            self._set_page_content("")
        elif uri_or_path.startswith("google:"):
            self._serpapi_search(uri_or_path[len("google:") :].strip(), filter_year=filter_year)
        else:
            if (
                not uri_or_path.startswith("http:")
                and not uri_or_path.startswith("https:")
                and not uri_or_path.startswith("file:")
            ):
                if len(self.history) > 1:
                    prior_address = self.history[-2][0]
                    uri_or_path = urljoin(prior_address, uri_or_path)
                    # Update the address with the fully-qualified path
                    self.history[-1] = (uri_or_path, self.history[-1][1])
            self._fetch_page(uri_or_path)

        self.viewport_current_page = 0
        self.find_on_page_query = None
        self.find_on_page_viewport = None

    @property
    def viewport(self) -> str:
        """Return the content of the current viewport."""
        bounds = self.viewport_pages[self.viewport_current_page]
        return self.page_content[bounds[0] : bounds[1]]

    @property
    def page_content(self) -> str:
        """Return the full contents of the current page."""
        return self._page_content

    def _set_page_content(self, content: str) -> None:
        """Sets the text content of the current page."""
        self._page_content = content
        self._split_pages()
        if self.viewport_current_page >= len(self.viewport_pages):
            self.viewport_current_page = len(self.viewport_pages) - 1

    def page_down(self) -> None:
        self.viewport_current_page = min(self.viewport_current_page + 1, len(self.viewport_pages) - 1)

    def page_up(self) -> None:
        self.viewport_current_page = max(self.viewport_current_page - 1, 0)

    def find_on_page(self, query: str) -> str | None:
        """Searches for the query from the current viewport forward, looping back to the start if necessary."""

        # Did we get here via a previous find_on_page search with the same query?
        # If so, map to find_next
        if query == self._find_on_page_query and self.viewport_current_page == self._find_on_page_last_result:
            return self.find_next()

        # Ok it's a new search start from the current viewport
        self._find_on_page_query = query
        viewport_match = self._find_next_viewport(query, self.viewport_current_page)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def find_next(self) -> str | None:
        """Scroll to the next viewport that matches the query"""

        if self._find_on_page_query is None:
            return None

        starting_viewport = self._find_on_page_last_result
        if starting_viewport is None:
            starting_viewport = 0
        else:
            starting_viewport += 1
            if starting_viewport >= len(self.viewport_pages):
                starting_viewport = 0

        viewport_match = self._find_next_viewport(self._find_on_page_query, starting_viewport)
        if viewport_match is None:
            self._find_on_page_last_result = None
            return None
        else:
            self.viewport_current_page = viewport_match
            self._find_on_page_last_result = viewport_match
            return self.viewport

    def _find_next_viewport(self, query: str, starting_viewport: int) -> int | None:
        """Search for matches between the starting viewport looping when reaching the end."""

        if query is None:
            return None

        # Normalize the query, and convert to a regular expression
        nquery = re.sub(r"\*", "__STAR__", query)
        nquery = " " + (" ".join(re.split(r"\W+", nquery))).strip() + " "
        nquery = nquery.replace(" __STAR__ ", "__STAR__ ")  # Merge isolated stars with prior word
        nquery = nquery.replace("__STAR__", ".*").lower()

        if nquery.strip() == "":
            return None

        idxs = list()
        idxs.extend(range(starting_viewport, len(self.viewport_pages)))
        idxs.extend(range(0, starting_viewport))

        for i in idxs:
            bounds = self.viewport_pages[i]
            content = self.page_content[bounds[0] : bounds[1]]

            # TODO: Remove markdown links and images
            ncontent = " " + (" ".join(re.split(r"\W+", content))).strip().lower() + " "
            if re.search(nquery, ncontent):
                return i

        return None

    def visit_page(self, path_or_uri: str, filter_year: int | None = None) -> str:
        """Update the address, visit the page, and return the content of the viewport."""
        self.set_address(path_or_uri, filter_year=filter_year)
        return self.viewport

    def _split_pages(self) -> None:
        # Do not split search results
        if self.address.startswith("google:"):
            self.viewport_pages = [(0, len(self._page_content))]
            return

        # Handle empty pages
        if len(self._page_content) == 0:
            self.viewport_pages = [(0, 0)]
            return

        # Break the viewport into pages
        self.viewport_pages = []
        start_idx = 0
        while start_idx < len(self._page_content):
            end_idx = min(start_idx + self.viewport_size, len(self._page_content))  # type: ignore[operator]
            # Adjust to end on a space
            while end_idx < len(self._page_content) and self._page_content[end_idx - 1] not in [" ", "\t", "\r", "\n"]:
                end_idx += 1
            self.viewport_pages.append((start_idx, end_idx))
            start_idx = end_idx

    def _serpapi_search(self, query: str, filter_year: int | None = None) -> None:
        if self.serpapi_key is None:
            raise ValueError("Missing SerpAPI key.")

        params = {
            "engine": "google",
            "q": query,
            "api_key": self.serpapi_key,
        }
        if filter_year is not None:
            params["tbs"] = f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"

        search = GoogleSearch(params)
        results = search.get_dict()
        self.page_title = f"{query} - Search"
        if "organic_results" not in results.keys():
            raise Exception(f"No results found for query: '{query}'. Use a less specific query.")
        if len(results["organic_results"]) == 0:
            year_filter_message = f" with filter year={filter_year}" if filter_year is not None else ""
            self._set_page_content(
                f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter."
            )
            return

        def _prev_visit(url):
            for i in range(len(self.history) - 1, -1, -1):
                if self.history[i][0] == url:
                    return f"You previously visited this page {round(time.time() - self.history[i][1])} seconds ago.\n"
            return ""

        web_snippets: list[str] = list()
        idx = 0
        if "organic_results" in results:
            for page in results["organic_results"]:
                idx += 1
                date_published = ""
                if "date" in page:
                    date_published = "\nDate published: " + page["date"]

                source = ""
                if "source" in page:
                    source = "\nSource: " + page["source"]

                snippet = ""
                if "snippet" in page:
                    snippet = "\n" + page["snippet"]

                redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{_prev_visit(page['link'])}{snippet}"

                redacted_version = redacted_version.replace("Your browser can't play this video.", "")
                web_snippets.append(redacted_version)

        content = (
            f"A Google search for '{query}' found {len(web_snippets)} results:\n\n## Web Results\n"
            + "\n\n".join(web_snippets)
        )

        self._set_page_content(content)

    def _fetch_page(self, url: str) -> None:
        download_path = ""
        try:
            if url.startswith("file://"):
                download_path = os.path.normcase(os.path.normpath(unquote(url[7:])))
                res = self._mdconvert.convert_local(download_path)
                self.page_title = res.title
                self._set_page_content(res.text_content)
            else:
                # Prepare the request parameters
                request_kwargs = self.request_kwargs.copy() if self.request_kwargs is not None else {}
                request_kwargs["stream"] = True

                # Send a HTTP request to the URL
                response = requests.get(url, **request_kwargs)
                response.raise_for_status()

                # If the HTTP request was successful
                content_type = response.headers.get("content-type", "")

                # Text or HTML
                if "text/" in content_type.lower():
                    res = self._mdconvert.convert_response(response)
                    self.page_title = res.title
                    self._set_page_content(res.text_content)
                # A download
                else:
                    # Try producing a safe filename
                    fname = None
                    download_path = None
                    try:
                        fname = pathvalidate.sanitize_filename(os.path.basename(urlparse(url).path)).strip()
                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))

                        suffix = 0
                        while os.path.exists(download_path) and suffix < 1000:
                            suffix += 1
                            base, ext = os.path.splitext(fname)
                            new_fname = f"{base}__{suffix}{ext}"
                            download_path = os.path.abspath(os.path.join(self.downloads_folder, new_fname))

                    except NameError:
                        pass

                    # No suitable name, so make one
                    if fname is None:
                        extension = mimetypes.guess_extension(content_type)
                        if extension is None:
                            extension = ".download"
                        fname = str(uuid.uuid4()) + extension
                        download_path = os.path.abspath(os.path.join(self.downloads_folder, fname))

                    # Open a file for writing
                    with open(download_path, "wb") as fh:
                        for chunk in response.iter_content(chunk_size=512):
                            fh.write(chunk)

                    # Render it
                    local_uri = pathlib.Path(download_path).as_uri()
                    self.set_address(local_uri)

        except UnsupportedFormatException as e:
            print(e)
            self.page_title = ("Download complete.",)
            self._set_page_content(f"# Download complete\n\nSaved file to '{download_path}'")
        except FileConversionException as e:
            print(e)
            self.page_title = ("Download complete.",)
            self._set_page_content(f"# Download complete\n\nSaved file to '{download_path}'")
        except FileNotFoundError:
            self.page_title = "Error 404"
            self._set_page_content(f"## Error 404\n\nFile not found: {download_path}")
        except requests.exceptions.RequestException as request_exception:
            try:
                self.page_title = f"Error {response.status_code}"

                # If the error was rendered in HTML we might as well render it
                content_type = response.headers.get("content-type", "")
                if content_type is not None and "text/html" in content_type.lower():
                    res = self._mdconvert.convert(response)
                    self.page_title = f"Error {response.status_code}"
                    self._set_page_content(f"## Error {response.status_code}\n\n{res.text_content}")
                else:
                    text = ""
                    for chunk in response.iter_content(chunk_size=512, decode_unicode=True):
                        text += chunk
                    self.page_title = f"Error {response.status_code}"
                    self._set_page_content(f"## Error {response.status_code}\n\n{text}")
            except NameError:
                self.page_title = "Error"
                self._set_page_content(f"## Error\n\n{str(request_exception)}")

    def _state(self) -> tuple[str, str]:
        header = f"Address: {self.address}\n"
        if self.page_title is not None:
            header += f"Title: {self.page_title}\n"

        current_page = self.viewport_current_page
        total_pages = len(self.viewport_pages)

        address = self.address
        for i in range(len(self.history) - 2, -1, -1):  # Start from the second last
            if self.history[i][0] == address:
                header += f"You previously visited this page {round(time.time() - self.history[i][1])} seconds ago.\n"
                break

        header += f"Viewport position: Showing page {current_page + 1} of {total_pages}.\n"
        return (header, self.viewport)


class SearchInformationTool(Tool):
    name = "web_search"
    description = "Perform a web search query (think a google search) and returns the search results."
    inputs = {"query": {"type": "string", "description": "The web search query to perform."}}
    inputs["filter_year"] = {
        "type": "string",
        "description": "[Optional parameter]: filter the search results to only include pages from a specific year. For example, '2020' will only include pages from 2020. Make sure to use this parameter if you're trying to search for articles from a specific date!",
        "nullable": True,
    }
    output_type = "string"

    def __init__(self, browser):
        super().__init__()
        self.browser = browser

    def forward(self, query: str, filter_year: int | None = None) -> str:
        self.browser.visit_page(f"google: {query}", filter_year=filter_year)
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content


class VisitTool(Tool):
    name = "visit_page"
    description = "Visit a webpage at a given URL and return its text. Given a url to a YouTube video, this returns the transcript."
    inputs = {"url": {"type": "string", "description": "The relative or absolute url of the webpage to visit."}}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, url: str) -> str:
        self.browser.visit_page(url)
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content


class DownloadTool(Tool):
    name = "download_file"
    description = """
Download a file at a given URL. The file should be of this format: [".xlsx", ".pptx", ".wav", ".mp3", ".m4a", ".png", ".docx"]
After using this tool, for further inspection of this page you should return the download path to your manager via final_answer, and they will be able to inspect it.
DO NOT use this tool for .pdf or .txt or .htm files: for these types of files use visit_page with the file url instead."""
    inputs = {"url": {"type": "string", "description": "The relative or absolute url of the file to be downloaded."}}
    output_type = "string"

    def __init__(self, browser):
        super().__init__()
        self.browser = browser

    def forward(self, url: str) -> str:
        import requests

        if "arxiv" in url:
            url = url.replace("abs", "pdf")
        response = requests.get(url)
        content_type = response.headers.get("content-type", "")
        extension = mimetypes.guess_extension(content_type)
        if extension and isinstance(extension, str):
            new_path = f"./downloads/file{extension}"
        else:
            new_path = "./downloads/file.object"

        with open(new_path, "wb") as f:
            f.write(response.content)

        if "pdf" in extension or "txt" in extension or "htm" in extension:
            raise Exception("Do not use this tool for pdf or txt or html files: use visit_page instead.")

        return f"File was downloaded and saved under path {new_path}."


class ArchiveSearchTool(Tool):
    name = "find_archived_url"
    description = "Given a url, searches the Wayback Machine and returns the archived version of the url that's closest in time to the desired date."
    inputs = {
        "url": {"type": "string", "description": "The url you need the archive for."},
        "date": {
            "type": "string",
            "description": "The date that you want to find the archive for. Give this date in the format 'YYYYMMDD', for instance '27 June 2008' is written as '20080627'.",
        },
    }
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, url, date) -> str:
        import requests

        no_timestamp_url = f"https://archive.org/wayback/available?url={url}"
        archive_url = no_timestamp_url + f"&timestamp={date}"
        response = requests.get(archive_url).json()
        response_notimestamp = requests.get(no_timestamp_url).json()
        if "archived_snapshots" in response and "closest" in response["archived_snapshots"]:
            closest = response["archived_snapshots"]["closest"]
            print("Archive found!", closest)

        elif "archived_snapshots" in response_notimestamp and "closest" in response_notimestamp["archived_snapshots"]:
            closest = response_notimestamp["archived_snapshots"]["closest"]
            print("Archive found!", closest)
        else:
            raise Exception(f"Your {url=} was not archived on Wayback Machine, try a different url.")
        target_url = closest["url"]
        self.browser.visit_page(target_url)
        header, content = self.browser._state()
        return (
            f"Web archive for url {url}, snapshot taken at date {closest['timestamp'][:8]}:\n"
            + header.strip()
            + "\n=======================\n"
            + content
        )


class PageUpTool(Tool):
    name = "page_up"
    description = "Scroll the viewport UP one page-length in the current webpage and return the new viewport content."
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        self.browser.page_up()
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content


class PageDownTool(Tool):
    name = "page_down"
    description = (
        "Scroll the viewport DOWN one page-length in the current webpage and return the new viewport content."
    )
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        self.browser.page_down()
        header, content = self.browser._state()
        return header.strip() + "\n=======================\n" + content


class FinderTool(Tool):
    name = "find_on_page_ctrl_f"
    description = "Scroll the viewport to the first occurrence of the search string. This is equivalent to Ctrl+F."
    inputs = {
        "search_string": {
            "type": "string",
            "description": "The string to search for on the page. This search string supports wildcards like '*'",
        }
    }
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self, search_string: str) -> str:
        find_result = self.browser.find_on_page(search_string)
        header, content = self.browser._state()

        if find_result is None:
            return (
                header.strip()
                + f"\n=======================\nThe search string '{search_string}' was not found on this page."
            )
        else:
            return header.strip() + "\n=======================\n" + content


class FindNextTool(Tool):
    name = "find_next"
    description = "Scroll the viewport to next occurrence of the search string. This is equivalent to finding the next match in a Ctrl+F search."
    inputs = {}
    output_type = "string"

    def __init__(self, browser=None):
        super().__init__()
        self.browser = browser

    def forward(self) -> str:
        find_result = self.browser.find_next()
        header, content = self.browser._state()

        if find_result is None:
            return header.strip() + "\n=======================\nThe search string was not found on this page."
        else:
            return header.strip() + "\n=======================\n" + content



================================================
FILE: examples/open_deep_research/scripts/visual_qa.py
================================================
import base64
import json
import mimetypes
import os
import uuid
from io import BytesIO

import PIL.Image
import requests
from dotenv import load_dotenv
from huggingface_hub import InferenceClient

from smolagents import Tool, tool


load_dotenv(override=True)


def process_images_and_text(image_path, query, client):
    from transformers import AutoProcessor

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": query},
            ],
        },
    ]
    idefics_processor = AutoProcessor.from_pretrained("HuggingFaceM4/idefics2-8b-chatty")
    prompt_with_template = idefics_processor.apply_chat_template(messages, add_generation_prompt=True)

    # load images from local directory

    # encode images to strings which can be sent to the endpoint
    def encode_local_image(image_path):
        # load image
        image = PIL.Image.open(image_path).convert("RGB")

        # Convert the image to a base64 string
        buffer = BytesIO()
        image.save(buffer, format="JPEG")  # Use the appropriate format (e.g., JPEG, PNG)
        base64_image = base64.b64encode(buffer.getvalue()).decode("utf-8")

        # add string formatting required by the endpoint
        image_string = f"data:image/jpeg;base64,{base64_image}"

        return image_string

    image_string = encode_local_image(image_path)
    prompt_with_images = prompt_with_template.replace("<image>", "![]({}) ").format(image_string)

    payload = {
        "inputs": prompt_with_images,
        "parameters": {
            "return_full_text": False,
            "max_new_tokens": 200,
        },
    }

    return json.loads(client.post(json=payload).decode())[0]


# Function to encode the image
def encode_image(image_path):
    if image_path.startswith("http"):
        user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
        request_kwargs = {
            "headers": {"User-Agent": user_agent},
            "stream": True,
        }

        # Send a HTTP request to the URL
        response = requests.get(image_path, **request_kwargs)
        response.raise_for_status()
        content_type = response.headers.get("content-type", "")

        extension = mimetypes.guess_extension(content_type)
        if extension is None:
            extension = ".download"

        fname = str(uuid.uuid4()) + extension
        download_path = os.path.abspath(os.path.join("downloads", fname))

        with open(download_path, "wb") as fh:
            for chunk in response.iter_content(chunk_size=512):
                fh.write(chunk)

        image_path = download_path

    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


def resize_image(image_path):
    img = PIL.Image.open(image_path)
    width, height = img.size
    img = img.resize((int(width / 2), int(height / 2)))
    new_image_path = f"resized_{image_path}"
    img.save(new_image_path)
    return new_image_path


class VisualQATool(Tool):
    name = "visualizer"
    description = "A tool that can answer questions about attached images."
    inputs = {
        "image_path": {
            "description": "The path to the image on which to answer the question",
            "type": "string",
        },
        "question": {"description": "the question to answer", "type": "string", "nullable": True},
    }
    output_type = "string"

    client = InferenceClient("HuggingFaceM4/idefics2-8b-chatty")

    def forward(self, image_path: str, question: str | None = None) -> str:
        output = ""
        add_note = False
        if not question:
            add_note = True
            question = "Please write a detailed caption for this image."
        try:
            output = process_images_and_text(image_path, question, self.client)
        except Exception as e:
            print(e)
            if "Payload Too Large" in str(e):
                new_image_path = resize_image(image_path)
                output = process_images_and_text(new_image_path, question, self.client)

        if add_note:
            output = (
                f"You did not provide a particular question, so here is a detailed caption for the image: {output}"
            )

        return output


@tool
def visualizer(image_path: str, question: str | None = None) -> str:
    """A tool that can answer questions about attached images.

    Args:
        image_path: The path to the image on which to answer the question. This should be a local path to downloaded image.
        question: The question to answer.
    """
    import mimetypes
    import os

    import requests

    from .visual_qa import encode_image

    add_note = False
    if not question:
        add_note = True
        question = "Please write a detailed caption for this image."
    if not isinstance(image_path, str):
        raise Exception("You should provide at least `image_path` string argument to this tool!")

    mime_type, _ = mimetypes.guess_type(image_path)
    base64_image = encode_image(image_path)

    payload = {
        "model": "gpt-4o",
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                    {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_image}"}},
                ],
            }
        ],
        "max_tokens": 1000,
    }
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"}
    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json=payload)
    try:
        output = response.json()["choices"][0]["message"]["content"]
    except Exception:
        raise Exception(f"Response format unexpected: {response.json()}")

    if add_note:
        output = f"You did not provide a particular question, so here is a detailed caption for the image: {output}"

    return output



================================================
FILE: examples/plan_customization/README.md
================================================
# Human-in-the-Loop: Customize Agent Plan Interactively

This example demonstrates advanced usage of the smolagents library, specifically showing how to implement Human-in-the-Loop strategies to:

1. **Interrupt agent execution after plan creation** using step callbacks
2. **Allow user interaction** to review and modify plans (Human-in-the-Loop)
3. **Resume execution** while preserving agent memory
4. **Modify plans in real-time** based on user feedback, keeping the human in control

## Human-in-the-Loop Key Features

### Interactive Plan Review
- The agent creates a plan and pauses execution
- Users can view the complete plan before execution begins
- Options to approve, modify, or cancel the plan

### Plan Modification
- Users can edit the agent's plan in real-time
- Modified plans are applied to the agent's memory
- Execution continues with the updated plan

### Memory Preservation
- Using `reset=False` preserves the agent's memory between runs
- Demonstrates how to build on previous interactions
- Shows memory state management across multiple executions
- Maintains transparency and control

## Usage

### Basic Usage
```python
python plan_customization.py
```

### Key Components

#### Step Callback Function
```python
def interrupt_after_plan(memory_step, agent):
    if isinstance(memory_step, PlanningStep):
        # Display plan and get user input
        # Modify plan if requested
        # Continue or interrupt based on user choice
```

#### Agent Configuration
```python
agent = CodeAgent(
    model=InferenceClientModel(),
    tools=[DuckDuckGoSearchTool()],
    planning_interval=5,  # Plan every 5 steps
    step_callbacks={PlanningStep: interrupt_after_plan},  # Register callback for PlanningStep
    max_steps=10,
    verbosity_level=1
)
```

#### Resuming Execution
```python
# First run - may be interrupted
agent.run(task, reset=True)

# Resume with preserved memory
agent.run(task, reset=False)  # Keeps all previous steps
```

## Example Human-in-the-Loop Workflow

1. **Agent starts** with a complex task
2. **Planning step** is created automatically
3. **Execution pauses** for human review - step callback triggers
4. **Human-in-the-Loop**:
   1. **User reviews the plan** in a formatted display
   2. **User decides** to approve, modify, or cancel the plan
   3. **User modifies the plan** (if requested) - user can edit the plan
5. **Execution resumes** with approved/modified plan
6. **Memory preservation** - all steps are maintained for future runs, maintaining transparency and control

## Interactive Elements

### Plan Display
```
============================================================
ðŸ¤– AGENT PLAN CREATED
============================================================
1. Search for recent AI developments
2. Analyze the top results
3. Summarize the 3 most significant breakthroughs
4. Include sources for each breakthrough
============================================================
```

### User Choices
```
Choose an option:
1. Approve plan
2. Modify plan
3. Cancel
Your choice (1-3):
```

### Plan Modification Interface
```
----------------------------------------
MODIFY PLAN
----------------------------------------
Current plan: [displays current plan]
----------------------------------------
Enter your modified plan (press Enter twice to finish):
```

## Advanced Features

### Memory State Inspection
The example shows how to inspect the agent's memory:
```python
print(f"Current memory contains {len(agent.memory.steps)} steps:")
for i, step in enumerate(agent.memory.steps):
    step_type = type(step).__name__
    print(f"  {i+1}. {step_type}")
```

### Error Handling
Proper error handling for:
- User cancellation
- Plan modification errors
- Resume execution failures

## Requirements

- smolagents library
- DuckDuckGoSearchTool (included with smolagents)
- Access to InferenceClientModel (requires HuggingFace API token)

## Educational Value

This example teaches:
- **Step callback implementation** for custom agent behavior
- **Memory management** in multi-step agents
- **User interaction patterns** in agentic systems
- **Plan modification techniques** for dynamic agent control
- **Error handling** in interactive agent systems

Perfect for understanding how to build interactive, user-controlled AI agents that can adapt their behavior based on human feedback.



================================================
FILE: examples/plan_customization/plan_customization.py
================================================
"""
Plan Customization Example

This example demonstrates how to use step callbacks to interrupt the agent after
plan creation, allow user interaction to approve or modify the plan, and then
resume execution while preserving agent memory.

Key concepts demonstrated:
1. Step callbacks to interrupt after PlanningStep
2. Extracting and modifying the current plan
3. Resuming execution with reset=False to preserve memory
4. User interaction for plan approval/modification
"""

from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel, PlanningStep


def display_plan(plan_content):
    """Display the plan in a formatted way"""
    print("\n" + "=" * 60)
    print("ðŸ¤– AGENT PLAN CREATED")
    print("=" * 60)
    print(plan_content)
    print("=" * 60)


def get_user_choice():
    """Get user's choice for plan approval"""
    while True:
        choice = input("\nChoose an option:\n1. Approve plan\n2. Modify plan\n3. Cancel\nYour choice (1-3): ").strip()
        if choice in ["1", "2", "3"]:
            return int(choice)
        print("Invalid choice. Please enter 1, 2, or 3.")


def get_modified_plan(original_plan):
    """Allow user to modify the plan"""
    print("\n" + "-" * 40)
    print("MODIFY PLAN")
    print("-" * 40)
    print("Current plan:")
    print(original_plan)
    print("-" * 40)
    print("Enter your modified plan (press Enter twice to finish):")

    lines = []
    empty_line_count = 0

    while empty_line_count < 2:
        line = input()
        if line.strip() == "":
            empty_line_count += 1
        else:
            empty_line_count = 0
        lines.append(line)

    # Remove the last two empty lines
    modified_plan = "\n".join(lines[:-2])
    return modified_plan if modified_plan.strip() else original_plan


def interrupt_after_plan(memory_step, agent):
    """
    Step callback that interrupts the agent after a planning step is created.
    This allows for user interaction to review and potentially modify the plan.
    """
    if isinstance(memory_step, PlanningStep):
        print("\nðŸ›‘ Agent interrupted after plan creation...")

        # Display the created plan
        display_plan(memory_step.plan)

        # Get user choice
        choice = get_user_choice()

        if choice == 1:  # Approve plan
            print("âœ… Plan approved! Continuing execution...")
            # Don't interrupt - let the agent continue
            return

        elif choice == 2:  # Modify plan
            # Get modified plan from user
            modified_plan = get_modified_plan(memory_step.plan)

            # Update the plan in the memory step
            memory_step.plan = modified_plan

            print("\nPlan updated!")
            display_plan(modified_plan)
            print("âœ… Continuing with modified plan...")
            # Don't interrupt - let the agent continue with modified plan
            return

        elif choice == 3:  # Cancel
            print("âŒ Execution cancelled by user.")
            agent.interrupt()
            return


def main():
    """Run the complete plan customization example"""
    print("ðŸš€ Starting Plan Customization Example")
    print("=" * 60)

    # Create agent with planning enabled and step callback
    agent = CodeAgent(
        model=InferenceClientModel(),
        tools=[DuckDuckGoSearchTool()],  # Add a search tool for more interesting plans
        planning_interval=5,  # Plan every 5 steps for demonstration
        step_callbacks={PlanningStep: interrupt_after_plan},
        max_steps=10,
        verbosity_level=1,  # Show agent thoughts
    )

    # Define a task that will benefit from planning
    task = """Search for recent developments in artificial intelligence and provide a summary
    of the top 3 most significant breakthroughs in 2024. Include the source of each breakthrough."""

    try:
        print(f"\nðŸ“‹ Task: {task}")
        print("\nðŸ¤– Agent starting execution...")

        # First run - will create plan and potentially get interrupted
        result = agent.run(task)

        # If we get here, the plan was approved or execution completed
        print("\nâœ… Task completed successfully!")
        print("\nðŸ“„ Final Result:")
        print("-" * 40)
        print(result)

    except Exception as e:
        if "interrupted" in str(e).lower():
            print("\nðŸ›‘ Agent execution was cancelled by user.")
            print("\nTo resume execution later, you could call:")
            print("agent.run(task, reset=False)  # This preserves the agent's memory")

            # Demonstrate resuming with reset=False
            print("\n" + "=" * 60)
            print("DEMONSTRATION: Resuming with reset=False")
            print("=" * 60)

            # Show current memory state
            print(f"\nðŸ“š Current memory contains {len(agent.memory.steps)} steps:")
            for i, step in enumerate(agent.memory.steps):
                step_type = type(step).__name__
                print(f"  {i + 1}. {step_type}")

            # Ask if user wants to see resume demonstration
            resume_choice = input("\nWould you like to see resume demonstration? (y/n): ").strip().lower()
            if resume_choice == "y":
                print("\nðŸ”„ Resuming execution...")
                try:
                    # Resume without resetting - preserves memory
                    agent.run(task, reset=False)
                    print("\nâœ… Task completed after resume!")
                    print("\nðŸ“„ Final Result:")
                    print("-" * 40)
                except Exception as resume_error:
                    print(f"\nâŒ Error during resume: {resume_error}")
                else:
                    print(f"\nâŒ An error occurred: {e}")


if __name__ == "__main__":
    # Run the main example
    main()



================================================
FILE: examples/server/README.md
================================================
# Smolagents Chat Server Demo

This is a simple web server that provides a chat interface for interacting with an AI code agent powered by `smolagents` and the Qwen3-Next-80B-A3B-Thinking model, enhanced with MCP (Model Control Protocol) tools.

## Features

- Web-based chat interface
- AI code agent powered by Qwen2.5-Coder
- Integration with MCP tools through MCPClient
- Asynchronous request handling
- Clean, responsive UI
- Graceful shutdown handling

## Requirements

- Python 3.8+
- Starlette
- AnyIO
- Smolagents with MCP support

## Installation

1. Install the required packages:

```bash
pip install starlette anyio 'smolagents[mcp]' uvicorn
```

2. Optional: If you want to use a specific model, you may need additional dependencies.

## Usage

1. Run the server:

```bash
uvicorn examples.server.main:app --reload
```

2. Open your browser and navigate to `http://localhost:8000`

3. Interact with the AI code agent through the chat interface

## How It Works

The server consists of two main routes:
- `/` - Serves the HTML page with the chat interface
- `/chat` - API endpoint that processes messages and returns responses

The server integrates with MCP tools through the following components:

1. MCPClient Configuration:
```python
mcp_server_parameters = {
    "url": "https://evalstate-hf-mcp-server.hf.space/mcp",
    "transport": "streamable-http",
}
mcp_client = MCPClient(server_parameters=mcp_server_parameters)
```

2. CodeAgent with MCP Tools:
```python
agent = CodeAgent(
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
    tools=mcp_client.get_tools(),
)
```

When a user sends a message:
1. The message is sent to the `/chat` endpoint
2. The server runs the AI code agent in a separate thread
3. The agent processes the message using MCP tools
4. The agent's response is returned to the client and displayed in the chat

The server also includes a shutdown handler that properly disconnects the MCP client when the server stops:
```python
async def shutdown():
    mcp_client.disconnect()
```

## Customization

You can modify the `CodeAgent` configuration by changing the model or MCP server parameters. For example:

```python
# Custom MCP server
mcp_server_parameters = {
    "url": "your-mcp-server-url",
    "transport": "your-transport-method",
}

# Custom agent configuration
agent = CodeAgent(
    model=InferenceClientModel(model_id="your-preferred-model"),
    tools=mcp_client.get_tools(),
)
```



================================================
FILE: examples/server/main.py
================================================
from anyio import to_thread
from starlette.applications import Starlette
from starlette.responses import HTMLResponse, JSONResponse
from starlette.routing import Route

from smolagents import CodeAgent, InferenceClientModel, MCPClient


# Create an MCP client to connect to the MCP server
mcp_server_parameters = {
    "url": "https://evalstate-hf-mcp-server.hf.space/mcp",
    "transport": "streamable-http",
}
mcp_client = MCPClient(server_parameters=mcp_server_parameters)

# Create a CodeAgent with a specific model and the tools from the MCP client
agent = CodeAgent(
    model=InferenceClientModel(model_id="Qwen/Qwen3-Next-80B-A3B-Thinking"),
    tools=mcp_client.get_tools(),
)


# Define the shutdown handler to disconnect the MCP client
async def shutdown():
    mcp_client.disconnect()


async def homepage(request):
    return HTMLResponse(
        r"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Smolagents Demo</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            border-radius: 12px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 30px;
        }
        .chat-container {
            border: 1px solid #ddd;
            border-radius: 8px;
            height: 400px;
            overflow-y: auto;
            padding: 15px;
            margin-bottom: 20px;
            background-color: #fafafa;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px;
            border-radius: 6px;
        }
        .user-message {
            background-color: #007bff;
            color: white;
            margin-left: 50px;
        }
        .agent-message {
            background-color: #e9ecef;
            color: #333;
            margin-right: 50px;
        }
        .input-container {
            display: flex;
            gap: 10px;
        }
        input[type="text"] {
            flex: 1;
            padding: 12px;
            border: 1px solid #ddd;
            border-radius: 6px;
            font-size: 16px;
        }
        button {
            padding: 12px 24px;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-size: 16px;
        }
        button:hover {
            background-color: #0056b3;
        }
        button:disabled {
            background-color: #ccc;
            cursor: not-allowed;
        }
        .loading {
            color: #666;
            font-style: italic;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸ¤– Smolagents Demo</h1>
        <div class="chat-container" id="chat-container">
            <div class="message agent-message">
                Hello! I'm a code agent with access to MCP tools. Ask me anything!
            </div>
        </div>
        <div class="input-container">
            <input type="text" id="message-input" placeholder="Ask me anything..." autofocus>
            <button onclick="sendMessage()" id="send-button">Send</button>
        </div>
    </div>

    <script>
        const chatContainer = document.getElementById('chat-container');
        const messageInput = document.getElementById('message-input');
        const sendButton = document.getElementById('send-button');

        function addMessage(content, isUser = false) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${isUser ? 'user-message' : 'agent-message'}`;
            messageDiv.textContent = content;
            chatContainer.appendChild(messageDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;
        }

        async function sendMessage() {
            const message = messageInput.value.trim();
            if (!message) return;

            // Add user message
            addMessage(message, true);
            messageInput.value = '';
            sendButton.disabled = true;
            sendButton.textContent = 'Sending...';

            // Add loading indicator
            const loadingDiv = document.createElement('div');
            loadingDiv.className = 'message agent-message loading';
            loadingDiv.textContent = 'Thinking...';
            chatContainer.appendChild(loadingDiv);
            chatContainer.scrollTop = chatContainer.scrollHeight;

            try {
                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ message }),
                });

                const data = await response.json();

                // Remove loading indicator
                chatContainer.removeChild(loadingDiv);

                // Add agent response
                addMessage(data.reply);
            } catch (error) {
                // Remove loading indicator
                chatContainer.removeChild(loadingDiv);
                addMessage(`Error: ${error.message}`);
            } finally {
                sendButton.disabled = false;
                sendButton.textContent = 'Send';
                messageInput.focus();
            }
        }

        // Send message on Enter key
        messageInput.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });
    </script>
</body>
</html>
"""
    )


async def chat(request):
    data = await request.json()
    message = data.get("message", "").strip()
    # Run in a thread to avoid blocking the event loop
    result = await to_thread.run_sync(agent.run, message)
    # Format the result if it's a complex data structure
    reply = str(result)
    return JSONResponse({"reply": reply})


app = Starlette(
    debug=True,
    routes=[
        Route("/", homepage),
        Route("/chat", chat, methods=["POST"]),
    ],
    on_shutdown=[shutdown],  # Register the shutdown handler: disconnect the MCP client
)



================================================
FILE: examples/smolagents_benchmark/run.py
================================================
import argparse
import datetime
import json
import os
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import datasets
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

from smolagents import (
    AgentError,
    CodeAgent,
    GoogleSearchTool,
    InferenceClientModel,
    LiteLLMModel,
    PythonInterpreterTool,
    ToolCallingAgent,
    VisitWebpageTool,
)


load_dotenv()
os.makedirs("output", exist_ok=True)

APPEND_ANSWER_LOCK = threading.Lock()


def parse_arguments():
    parser = argparse.ArgumentParser(description="Runs an agent powered by the given model on smolagent benchmark.")
    parser.add_argument(
        "--date",
        type=str,
        default=None,
        help="The date for the evaluation.",
    )
    parser.add_argument(
        "--eval-dataset",
        type=str,
        default="smolagents/benchmark-v1",
    )
    # The eval dataset is gated, so you must first visit its page to request access: https://huggingface.co/datasets/smolagents-benchmark/benchmark-v1
    parser.add_argument(
        "--model-type",
        type=str,
        default="InferenceClientModel",
        choices=["LiteLLMModel", "InferenceClientModel"],
        help="The model type to use (LiteLLMModel or InferenceClientModel)",
    )
    parser.add_argument(
        "--model-id",
        type=str,
        required=True,
        help="The model ID to use for the specified model type",
    )
    parser.add_argument(
        "--provider",
        type=str,
        help="The provider for InferenceClientModel - will not be used for LiteLLMModel",
    )
    parser.add_argument(
        "--agent-action-type",
        type=str,
        default="code",
        choices=["code", "tool-calling", "vanilla"],
        help="The agent action type: 'code', 'tool-calling', or 'vanilla' to use the vanilla llm",
    )
    parser.add_argument(
        "--parallel-workers",
        type=int,
        default=8,
        help="The number of processes to run in parallel",
    )
    parser.add_argument(
        "--push-answers-to-hub",
        action="store_true",
        help="Push the answers to the hub",
    )
    parser.add_argument(
        "--answers-dataset",
        type=str,
        default="smolagents/answers",
    )
    return parser.parse_args()


def load_eval_dataset(eval_dataset):
    # Choose the tasks to evaluate on:
    # tasks = ["gaia"]
    # or evaluate on all tasks: ["gaia", "math", "simpleqa"]
    tasks = datasets.get_dataset_config_names(eval_dataset)
    print(tasks)

    eval_ds = {task: datasets.load_dataset(eval_dataset, task, split="test") for task in tasks}
    print(pd.DataFrame(eval_ds["simpleqa"]).head())
    return eval_ds


def serialize_agent_error(obj):
    if isinstance(obj, AgentError):
        return {"error_type": obj.__class__.__name__, "message": obj.message}
    else:
        return str(obj)


def append_answer(entry: dict, jsonl_file: str) -> None:
    jsonl_file = Path(jsonl_file)
    jsonl_file.parent.mkdir(parents=True, exist_ok=True)

    def convert_to_serializable(obj):
        if hasattr(obj, "dict"):
            return obj.dict()
        else:
            raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

    with APPEND_ANSWER_LOCK, open(jsonl_file, "a", encoding="utf-8") as fp:
        fp.write(json.dumps(entry, default=convert_to_serializable) + "\n")
    assert os.path.exists(jsonl_file), "File not found!"


def answer_single_question(example, model, answers_file, action_type):
    if action_type == "vanilla":
        agent = model
    elif action_type == "code":
        agent = CodeAgent(
            tools=[GoogleSearchTool(provider="serper"), VisitWebpageTool()],
            model=model,
            additional_authorized_imports=["numpy", "sympy"],
            max_steps=10,
        )
    elif action_type == "tool-calling":
        agent = ToolCallingAgent(
            tools=[
                GoogleSearchTool(provider="serper"),
                VisitWebpageTool(),
                PythonInterpreterTool(authorized_imports=["numpy", "sympy"]),
            ],
            model=model,
            max_steps=10,
        )

    augmented_question = example["question"]
    if example["source"] == "SimpleQA":
        augmented_question += " Answer with only the final number."
    if example["source"] == "MATH":
        augmented_question += " Write code, not latex."

    start_time = time.time()

    try:
        if action_type == "vanilla":
            answer = agent([{"role": "user", "content": augmented_question}]).content
            token_counts = agent.monitor.get_total_token_counts()
            intermediate_steps = answer
        else:
            # Run agent ðŸš€
            answer = str(agent.run(augmented_question))
            token_counts = agent.monitor.get_total_token_counts()
            intermediate_steps = [message.dict() for message in agent.write_memory_to_messages()]

        end_time = time.time()
    except Exception as e:
        print("Error on ", augmented_question, e)
        intermediate_steps = []
        token_counts = {"input": 0, "output": 0}
        answer = str(e)
    end_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    annotated_example = {
        "model_id": model.model_id,
        "agent_action_type": action_type,
        "question": augmented_question,
        "original_question": example["question"],
        "answer": answer,
        "true_answer": example["true_answer"],
        "source": example["source"],
        "intermediate_steps": intermediate_steps,
        "start_time": start_time,
        "end_time": end_time,
        "token_counts": token_counts,
    }
    append_answer(annotated_example, answers_file)


def answer_questions(
    eval_ds,
    model,
    date,
    action_type: str = "code",
    output_dir: str = "output",
    answers_dataset: str = None,
    push_answers_to_hub: bool = False,
    parallel_workers: int = 32,
):
    date = date or datetime.date.today().isoformat()
    model_id = model.model_id

    for task in eval_ds:
        file_name = f"{output_dir}/{model_id.replace('/', '__')}__{action_type}__{task}__{date}.jsonl"
        print(f"Starting processing and writing output to '{file_name}'")
        answered_questions = []
        if os.path.exists(file_name):
            with open(file_name, "r") as f:
                for line in f:
                    answered_questions.append(json.loads(line)["original_question"])

        examples_todo = [example for example in eval_ds[task] if example["question"] not in answered_questions]
        print(f"Launching {parallel_workers} parallel workers.")

        with ThreadPoolExecutor(max_workers=parallel_workers) as exe:
            futures = [
                exe.submit(answer_single_question, example, model, file_name, action_type) for example in examples_todo
            ]
            for f in tqdm(as_completed(futures), total=len(examples_todo), desc="Processing tasks"):
                f.result()

        print("All tasks processed.")

        if push_answers_to_hub and answers_dataset:
            print("Pushing answers to hub...")
            ds = datasets.Dataset.from_pandas(pd.read_json(file_name, lines=True), split="test", preserve_index=False)
            config = f"{model_id.replace('/', '__')}__{action_type}__{task}"
            data_dir = f"{model_id}/{action_type}/{task}/{date}"
            ds.push_to_hub(
                answers_dataset,
                config_name=config,
                data_dir=data_dir,
                split="test",
                commit_message=f"Upload {config}",
            )


if __name__ == "__main__":
    args = parse_arguments()

    eval_ds = load_eval_dataset(args.eval_dataset)

    if args.model_type == "LiteLLMModel":
        model = LiteLLMModel(
            model_id=args.model_id,
            max_completion_tokens=8192,
        )
    else:
        model = InferenceClientModel(model_id=args.model_id, provider=args.provider, max_tokens=8192)

    answer_questions(
        eval_ds,
        model,
        args.date,
        action_type=args.agent_action_type,
        answers_dataset=args.answers_dataset,
        push_answers_to_hub=args.push_answers_to_hub,
        parallel_workers=args.parallel_workers,
    )



================================================
FILE: examples/smolagents_benchmark/score.ipynb
================================================
# Jupyter notebook converted to Python script.

!pip install -e .. datasets sympy numpy matplotlib seaborn -q  # Install dev version of smolagents + some packages

# Benchmark date
# - set a concrete date:
DATE = "2024-12-26"
# - or use default: today
# DATE = None

# Evaluation dataset
# - the dataset is gated, so you must first visit its page to request access: https://huggingface.co/datasets/smolagents-benchmark/benchmark-v1
EVAL_DATASET = "smolagents/benchmark-v1"

# Answers dataset: it must be a gated dataset; required to score the answers
ANSWERS_DATASET = "smolagents/answers"
# Whether to push the answers dataset to the Hub
PUSH_ANSWERS_DATASET_TO_HUB = True

# Results dataset
RESULTS_DATASET = "smolagents/results"
# Whether to push the results dataset to the Hub
PUSH_RESULTS_DATASET_TO_HUB = True

"""
## Constants and utilities/tools
"""

import datetime
import re
import string
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed

import numpy as np
from tqdm import tqdm


def normalize_number_str(number_str: str) -> float:
    # we replace these common units and commas to allow
    # conversion to float
    for char in ["$", "%", ","]:
        number_str = number_str.replace(char, "")
    try:
        return float(number_str)
    except ValueError:
        return float("inf")


def split_string(
    s: str,
    char_list: list[str] = [",", ";"],
) -> list[str]:
    pattern = f"[{''.join(char_list)}]"
    return re.split(pattern, s)


def is_float(element: any) -> bool:
    try:
        float(element)
        return True
    except ValueError:
        return False


def normalize_str(input_str, remove_punct=True) -> str:
    """
    Normalize a string by:
    - Removing all white spaces
    - Optionally removing punctuation (if remove_punct is True)
    - Converting to lowercase
    Parameters:
    - input_str: str, the string to normalize
    - remove_punct: bool, whether to remove punctuation (default: True)
    Returns:
    - str, the normalized string
    """
    # Remove all white spaces. Required e.g for seagull vs. sea gull
    no_spaces = re.sub(r"\s", "", input_str)

    # Remove punctuation, if specified.
    if remove_punct:
        translator = str.maketrans("", "", string.punctuation)
        return no_spaces.lower().translate(translator)
    else:
        return no_spaces.lower()


def extract_numbers(text: str) -> list[str]:
    """This pattern matches:
    - Optional negative sign
    - Numbers with optional comma thousand separators
    - Optional decimal points with decimal numbers
    """
    pattern = r"-?(?:\d{1,3}(?:,\d{3})+|\d+)(?:\.\d+)?"

    return [el.replace(",", "") for el in re.findall(pattern, text)]


def get_question_score_gaia(
    model_answer: str,
    ground_truth: str,
) -> bool:
    """Scoring function used to score functions from the GAIA benchmark"""
    if is_float(ground_truth):
        normalized_answer = normalize_number_str(str(model_answer))
        return normalized_answer == float(ground_truth)

    elif any(char in ground_truth for char in [",", ";"]):  # if gt is a list
        # question with the fish: normalization removes punct
        gt_elems = split_string(ground_truth)
        ma_elems = split_string(model_answer)

        if len(gt_elems) != len(ma_elems):  # check length is the same
            warnings.warn("Answer lists have different lengths, returning False.", UserWarning)
            return False

        comparisons = []
        for ma_elem, gt_elem in zip(ma_elems, gt_elems):  # compare each element as float or str
            if is_float(gt_elem):
                normalized_ma_elem = normalize_number_str(ma_elem)
                comparisons.append(normalized_ma_elem == float(gt_elem))
            else:
                # we do not remove punct since comparisons can include punct
                comparisons.append(
                    normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False)
                )
        return all(comparisons)

    else:  # if gt is a str
        return normalize_str(model_answer) == normalize_str(ground_truth)


def get_correct(row):
    if row["source"] == "MATH":  # Checks the last number in answer
        numbers_answer = extract_numbers(str(row["answer"]))
        if len(numbers_answer) == 0:
            return False
        return np.isclose(float(numbers_answer[-1]), float(row["true_answer"]), rtol=1e-5, atol=1e-7)
    else:
        return get_question_score_gaia(str(row["answer"]), str(row["true_answer"]))


def score_answers_subset(answers_dataset, answers_subset):
    try:
        print(answers_dataset, answers_subset)
        *model_id, action_type, task = answers_subset.split("__")
        model_id = "/".join(model_id)
        ds = datasets.load_dataset(answers_dataset, answers_subset, split="test")
        df = ds.to_pandas()
        df["correct"] = df.apply(get_correct, axis=1)
        assert df["correct"].notnull().sum() > 30, "Missing answers"
        acc = df["correct"].mean().item()
        result = df.loc[0, ["model_id", "agent_action_type", "source"]].to_dict()
        result["acc"] = acc
        return result
    except Exception as e:
        print(f"Error with {answers_subset}: {e}")
        return None


def score_answers(
    answers_subsets,
    answers_dataset=ANSWERS_DATASET,
    date=DATE,
    push_to_hub_dataset=RESULTS_DATASET if PUSH_RESULTS_DATASET_TO_HUB else None,
    set_default=True,
):
    """
    Score answers from the given dataset subsets.

    Parameters:
        answers_subsets: List of dataset subsets to score
        answers_dataset: Dataset containing the answers
        date: Date to use for the config name
        push_to_hub_dataset: Dataset ID to push results to, or None to skip pushing
        set_default: If True, sets this config as the default config in the Hugging Face Hub dataset.
                     This means when users load the dataset without specifying a config,
                     this version will be loaded by default.
    """
    if not answers_dataset:
        raise ValueError("Pass 'answers_dataset' to load the answers from it")
    date = date or datetime.date.today().isoformat()
    results = []
    with ThreadPoolExecutor(max_workers=16) as exe:
        futures = [
            exe.submit(score_answers_subset, answers_dataset, answers_subset) for answers_subset in answers_subsets
        ]
        for f in tqdm(as_completed(futures), total=len(answers_subsets), desc="Processing tasks"):
            result = f.result()
            if result:
                results.append(result)
    df = pd.DataFrame(results)

    if push_to_hub_dataset:
        ds = datasets.Dataset.from_pandas(df)
        config = date
        ds.push_to_hub(push_to_hub_dataset, config_name=config, commit_message=f"Upload {config} results")
    return df

"""
## Score answers
"""

import datasets
import pandas as pd


# Choose the answers subsets to score:
# answers_subsets = ["meta-llama__Llama-3.1-8B-Instruct__code__gaia"]
# or get all the answers subsets present in the ANSWERS_DATASET
answers_subsets = datasets.get_dataset_config_names(ANSWERS_DATASET)
print("Number of answers_subsets", len(answers_subsets))
print("Example of answers_subset", answers_subsets[0])

result_df = score_answers(answers_subsets)
result_df["acc"] = (result_df["acc"] * 100).round(2)
result_df.head()

pivot_df = result_df.pivot_table(
    index=["model_id", "source"],
    columns=["agent_action_type"],
    values="acc",
    fill_value=float("nan"),
).reset_index()

"""
### Display results
"""

display(pivot_df)

import matplotlib.pyplot as plt
from matplotlib.legend_handler import HandlerTuple  # Added import


# Assuming pivot_df is your original dataframe
models = pivot_df["model_id"].unique()
sources = pivot_df["source"].unique()

# Create figure and axis
plt.style.use("seaborn-v0_8-white")
fig, ax = plt.subplots(figsize=(15, 6))

# Set the width of each bar group and positions of the bars
width = 0.15  # width of each bar
spacing = 0.02  # space between bars within a group
group_spacing = 0.2  # space between model groups

# Calculate positions for the bars
num_sources = len(sources)
total_width_per_group = (width + spacing) * num_sources * 2  # *2 for agent and vanilla
x = np.arange(len(models)) * (total_width_per_group + group_spacing)

# Plot bars for each source
for i, source in enumerate(sources):
    source_data = pivot_df[pivot_df["source"] == source]
    agent_scores = [
        source_data[source_data["model_id"] == model]["code"].values[0]
        if len(source_data[source_data["model_id"] == model]) > 0
        else np.nan
        for model in models
    ]
    vanilla_scores = [
        source_data[source_data["model_id"] == model]["vanilla"].values[0]
        if len(source_data[source_data["model_id"] == model]) > 0
        else np.nan
        for model in models
    ]

    # Position calculation for each pair of bars
    pos = x + i * (width * 2 + spacing)

    agent_bars = ax.bar(pos, agent_scores, width, label=f"{source} (Agent)", alpha=0.8)
    vanilla_bars = ax.bar(
        pos + width * 0.6,
        vanilla_scores,
        width,
        hatch="////",
        alpha=0.5,
        hatch_linewidth=2,
        label=f"{source} (Vanilla)",
        color="white",
        edgecolor=agent_bars[0].get_facecolor(),
    )

# Customize the plot
ax.set_ylabel("Score")
ax.set_title("Model Performance Comparison")

# Set x-axis ticks in the middle of each group
group_centers = x + (total_width_per_group - spacing) / 2
ax.set_xticks(group_centers)

# Wrap long model names to prevent overlap
wrapped_labels = ["\n".join(model.split("/")) for model in models]
ax.set_xticklabels(wrapped_labels, rotation=0, ha="center")

# Modify legend to combine agent and vanilla entries
handles, labels = ax.get_legend_handles_labels()
unique_sources = sources
legend_elements = [
    (handles[i * 2], handles[i * 2 + 1], labels[i * 2].replace(" (Agent)", "")) for i in range(len(unique_sources))
]
custom_legend = ax.legend(
    [(agent_handle, vanilla_handle) for agent_handle, vanilla_handle, _ in legend_elements],
    [label for _, _, label in legend_elements],
    handler_map={tuple: HandlerTuple(ndivide=None)},
    bbox_to_anchor=(1.05, 1),
    loc="upper left",
)

ax.yaxis.grid(True, linestyle="--", alpha=0.3)
ax.set_ylim(bottom=0)
plt.tight_layout()
ax.spines["top"].set_visible(False)
ax.spines["right"].set_visible(False)

plt.show()



================================================
FILE: src/smolagents/__init__.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
__version__ = "1.24.0.dev0"

from .agent_types import *  # noqa: I001
from .agents import *  # Above noqa avoids a circular dependency due to cli.py
from .default_tools import *
from .gradio_ui import *
from .local_python_executor import *
from .mcp_client import *
from .memory import *
from .models import *
from .monitoring import *
from .remote_executors import *
from .tools import *
from .utils import *
from .cli import *



================================================
FILE: src/smolagents/_function_type_hints_utils.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module contains utilities exclusively taken from `transformers` repository.

Since they are not specific to `transformers` and that `transformers` is an heavy dependencies, those helpers have
been duplicated.

TODO: move them to `huggingface_hub` to avoid code duplication.
"""

import inspect
import json
import re
import types
from collections.abc import Callable
from copy import copy
from typing import (
    Any,
    Literal,
    Union,
    get_args,
    get_origin,
    get_type_hints,
)


IMPORT_TO_PACKAGE_MAPPING = {
    "wikipediaapi": "wikipedia-api",
}


def get_package_name(import_name: str) -> str:
    """
    Return the package name for a given import name.

    Args:
        import_name (`str`): Import name to get the package name for.

    Returns:
        `str`: Package name for the given import name.
    """
    return IMPORT_TO_PACKAGE_MAPPING.get(import_name, import_name)


def get_imports(code: str) -> list[str]:
    """
    Extracts all the libraries (not relative imports) that are imported in a code.

    Args:
        code (`str`): Code text to inspect.

    Returns:
        `list[str]`: List of all packages required to use the input code.
    """
    # filter out try/except block so in custom code we can have try/except imports
    code = re.sub(r"\s*try\s*:.*?except.*?:", "", code, flags=re.DOTALL)

    # filter out imports under is_flash_attn_2_available block for avoid import issues in cpu only environment
    code = re.sub(
        r"if is_flash_attn[a-zA-Z0-9_]+available\(\):\s*(from flash_attn\s*.*\s*)+",
        "",
        code,
        flags=re.MULTILINE,
    )

    # Imports of the form `import xxx` or `import xxx as yyy`
    imports = re.findall(r"^\s*import\s+(\S+?)(?:\s+as\s+\S+)?\s*$", code, flags=re.MULTILINE)
    # Imports of the form `from xxx import yyy`
    imports += re.findall(r"^\s*from\s+(\S+)\s+import", code, flags=re.MULTILINE)
    # Only keep the top-level module
    imports = [imp.split(".")[0] for imp in imports if not imp.startswith(".")]
    return [get_package_name(import_name) for import_name in set(imports)]


class TypeHintParsingException(Exception):
    """Exception raised for errors in parsing type hints to generate JSON schemas"""


class DocstringParsingException(Exception):
    """Exception raised for errors in parsing docstrings to generate JSON schemas"""


def get_json_schema(func: Callable) -> dict:
    """
    This function generates a JSON schema for a given function, based on its docstring and type hints. This is
    mostly used for passing lists of tools to a chat template. The JSON schema contains the name and description of
    the function, as well as the names, types and descriptions for each of its arguments. `get_json_schema()` requires
    that the function has a docstring, and that each argument has a description in the docstring, in the standard
    Google docstring format shown below. It also requires that all the function arguments have a valid Python type hint.

    Although it is not required, a `Returns` block can also be added, which will be included in the schema. This is
    optional because most chat templates ignore the return value of the function.

    Args:
        func: The function to generate a JSON schema for.

    Returns:
        A dictionary containing the JSON schema for the function.

    Examples:
    ```python
    >>> def multiply(x: float, y: float):
    >>>    '''
    >>>    A function that multiplies two numbers
    >>>
    >>>    Args:
    >>>        x: The first number to multiply
    >>>        y: The second number to multiply
    >>>    '''
    >>>    return x * y
    >>>
    >>> print(get_json_schema(multiply))
    {
        "name": "multiply",
        "description": "A function that multiplies two numbers",
        "parameters": {
            "type": "object",
            "properties": {
                "x": {"type": "number", "description": "The first number to multiply"},
                "y": {"type": "number", "description": "The second number to multiply"}
            },
            "required": ["x", "y"]
        }
    }
    ```

    The general use for these schemas is that they are used to generate tool descriptions for chat templates that
    support them, like so:

    ```python
    >>> from transformers import AutoTokenizer
    >>> from transformers.utils import get_json_schema
    >>>
    >>> def multiply(x: float, y: float):
    >>>    '''
    >>>    A function that multiplies two numbers
    >>>
    >>>    Args:
    >>>        x: The first number to multiply
    >>>        y: The second number to multiply
    >>>    return x * y
    >>>    '''
    >>>
    >>> multiply_schema = get_json_schema(multiply)
    >>> tokenizer = AutoTokenizer.from_pretrained("CohereForAI/c4ai-command-r-v01")
    >>> messages = [{"role": "user", "content": "What is 179 x 4571?"}]
    >>> formatted_chat = tokenizer.apply_chat_template(
    >>>     messages,
    >>>     tools=[multiply_schema],
    >>>     chat_template="tool_use",
    >>>     return_dict=True,
    >>>     return_tensors="pt",
    >>>     add_generation_prompt=True
    >>> )
    >>> # The formatted chat can now be passed to model.generate()
    ```

    Each argument description can also have an optional `(choices: ...)` block at the end, such as
    `(choices: ["tea", "coffee"])`, which will be parsed into an `enum` field in the schema. Note that this will
    only be parsed correctly if it is at the end of the line:

    ```python
    >>> def drink_beverage(beverage: str):
    >>>    '''
    >>>    A function that drinks a beverage
    >>>
    >>>    Args:
    >>>        beverage: The beverage to drink (choices: ["tea", "coffee"])
    >>>    '''
    >>>    pass
    >>>
    >>> print(get_json_schema(drink_beverage))
    ```
    {
        'name': 'drink_beverage',
        'description': 'A function that drinks a beverage',
        'parameters': {
            'type': 'object',
            'properties': {
                'beverage': {
                    'type': 'string',
                    'enum': ['tea', 'coffee'],
                    'description': 'The beverage to drink'
                    }
                },
            'required': ['beverage']
        }
    }
    """
    doc = inspect.getdoc(func)
    if not doc:
        raise DocstringParsingException(
            f"Cannot generate JSON schema for {func.__name__} because it has no docstring!"
        )
    doc = doc.strip()
    main_doc, param_descriptions, return_doc = _parse_google_format_docstring(doc)

    json_schema = _convert_type_hints_to_json_schema(func)
    if (return_dict := json_schema["properties"].pop("return", None)) is not None:
        if return_doc is not None:  # We allow a missing return docstring since most templates ignore it
            return_dict["description"] = return_doc
    for arg, schema in json_schema["properties"].items():
        if arg not in param_descriptions:
            raise DocstringParsingException(
                f"Cannot generate JSON schema for {func.__name__} because the docstring has no description for the argument '{arg}'"
            )
        desc = param_descriptions[arg]
        enum_choices = re.search(r"\(choices:\s*(.*?)\)\s*$", desc, flags=re.IGNORECASE)
        if enum_choices:
            schema["enum"] = [c.strip() for c in json.loads(enum_choices.group(1))]
            desc = enum_choices.string[: enum_choices.start()].strip()
        schema["description"] = desc

    output = {"name": func.__name__, "description": main_doc, "parameters": json_schema}
    if return_dict is not None:
        output["return"] = return_dict
    return {"type": "function", "function": output}


# Extracts the initial segment of the docstring, containing the function description
description_re = re.compile(r"^(.*?)(?=\n\s*(Args:|Returns:|Raises:)|\Z)", re.DOTALL)
# Extracts the Args: block from the docstring
args_re = re.compile(r"\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)", re.DOTALL)
# Splits the Args: block into individual arguments
args_split_re = re.compile(
    r"(?:^|\n)"  # Match the start of the args block, or a newline
    r"\s*(\w+)\s*(?:\([^)]*?\))?:\s*"  # Capture the argument name (ignore the type) and strip spacing
    r"(.*?)\s*"  # Capture the argument description, which can span multiple lines, and strip trailing spacing
    r"(?=\n\s*\w+\s*(?:\([^)]*?\))?:|\Z)",  # Stop when you hit the next argument (with or without type) or the end of the block
    re.DOTALL | re.VERBOSE,
)
# Extracts the Returns: block from the docstring, if present. Note that most chat templates ignore the return type/doc!
returns_re = re.compile(
    r"\n\s*Returns:\n\s*"
    r"(?:[^)]*?:\s*)?"  # Ignore the return type if present
    r"(.*?)"  # Capture the return description
    r"[\n\s]*(Raises:|\Z)",
    re.DOTALL,
)


def _parse_google_format_docstring(
    docstring: str,
) -> tuple[str | None, dict | None, str | None]:
    """
    Parses a Google-style docstring to extract the function description,
    argument descriptions, and return description.

    Args:
        docstring (str): The docstring to parse.

    Returns:
        The function description, arguments, and return description.
    """

    # Extract the sections
    description_match = description_re.search(docstring)
    args_match = args_re.search(docstring)
    returns_match = returns_re.search(docstring)

    # Clean and store the sections
    description = description_match.group(1).strip() if description_match else None
    docstring_args = args_match.group(1).strip() if args_match else None
    returns = returns_match.group(1).strip() if returns_match else None

    # Parsing the arguments into a dictionary
    if docstring_args is not None:
        docstring_args = "\n".join([line for line in docstring_args.split("\n") if line.strip()])  # Remove blank lines
        matches = args_split_re.findall(docstring_args)
        args_dict = {match[0]: re.sub(r"\s*\n+\s*", " ", match[1].strip()) for match in matches}
    else:
        args_dict = {}

    return description, args_dict, returns


def _convert_type_hints_to_json_schema(func: Callable, error_on_missing_type_hints: bool = True) -> dict:
    type_hints = get_type_hints(func)
    signature = inspect.signature(func)

    properties = {}
    for param_name, param_type in type_hints.items():
        properties[param_name] = _parse_type_hint(param_type)

    required = []
    for param_name, param in signature.parameters.items():
        if param.annotation == inspect.Parameter.empty and error_on_missing_type_hints:
            raise TypeHintParsingException(f"Argument {param.name} is missing a type hint in function {func.__name__}")
        if param_name not in properties:
            properties[param_name] = {}

        if param.default == inspect.Parameter.empty:
            required.append(param_name)
        else:
            properties[param_name]["nullable"] = True

    # Return: multiâ€type union -> treat as any
    if (
        "return" in properties
        and (return_type := properties["return"].get("type"))
        and not isinstance(return_type, str)
    ):
        properties["return"]["type"] = "any"

    schema = {"type": "object", "properties": properties}
    if required:
        schema["required"] = required

    return schema


def _parse_type_hint(hint: type) -> dict:
    origin = get_origin(hint)
    args = get_args(hint)

    if origin is None:
        try:
            return _get_json_schema_type(hint)
        except KeyError:
            raise TypeHintParsingException(
                "Couldn't parse this type hint, likely due to a custom class or object: ",
                hint,
            )

    elif origin is Union or (hasattr(types, "UnionType") and origin is types.UnionType):
        return _parse_union_type(args)

    elif origin is list:
        if not args:
            return {"type": "array"}
        else:
            # Lists can only have a single type argument, so recurse into it
            return {"type": "array", "items": _parse_type_hint(args[0])}

    elif origin is tuple:
        if not args:
            return {"type": "array"}
        if len(args) == 1:
            raise TypeHintParsingException(
                f"The type hint {str(hint).replace('typing.', '')} is a Tuple with a single element, which "
                "we do not automatically convert to JSON schema as it is rarely necessary. If this input can contain "
                "more than one element, we recommend "
                "using a List[] type instead, or if it really is a single element, remove the Tuple[] wrapper and just "
                "pass the element directly."
            )
        if ... in args:
            raise TypeHintParsingException(
                "Conversion of '...' is not supported in Tuple type hints. "
                "Use List[] types for variable-length"
                " inputs instead."
            )
        return {"type": "array", "prefixItems": [_parse_type_hint(t) for t in args]}

    elif origin is dict:
        # The JSON equivalent to a dict is 'object', which mandates that all keys are strings
        # However, we can specify the type of the dict values with "additionalProperties"
        out = {"type": "object"}
        if len(args) == 2:
            out["additionalProperties"] = _parse_type_hint(args[1])
        return out

    elif origin is Literal:
        literal_types = set(type(arg) for arg in args)
        final_type = _parse_union_type(literal_types)

        # None literal value is represented by 'nullable' field set by _parse_union_type
        final_type.update({"enum": [arg for arg in args if arg is not None]})
        return final_type

    raise TypeHintParsingException("Couldn't parse this type hint, likely due to a custom class or object: ", hint)


def _parse_union_type(args: tuple[Any, ...]) -> dict:
    subtypes = [_parse_type_hint(t) for t in args if t is not type(None)]
    if len(subtypes) == 1:
        # A single non-null type can be expressed directly
        return_dict = subtypes[0]
    elif all(isinstance(subtype["type"], str) for subtype in subtypes):
        # A union of basic types can be expressed as a list in the schema
        return_dict = {"type": sorted([subtype["type"] for subtype in subtypes])}
    else:
        # A union of more complex types requires "anyOf"
        return_dict = {"anyOf": subtypes}
    if type(None) in args:
        return_dict["nullable"] = True
    return return_dict


_BASE_TYPE_MAPPING = {
    int: {"type": "integer"},
    float: {"type": "number"},
    str: {"type": "string"},
    bool: {"type": "boolean"},
    list: {"type": "array"},
    dict: {"type": "object"},
    Any: {"type": "any"},
    types.NoneType: {"type": "null"},
}


def _get_json_schema_type(param_type: type) -> dict[str, str]:
    if param_type in _BASE_TYPE_MAPPING:
        return copy(_BASE_TYPE_MAPPING[param_type])
    if str(param_type) == "Image":
        from PIL.Image import Image

        if param_type == Image:
            return {"type": "image"}
    if str(param_type) == "Tensor":
        try:
            from torch import Tensor

            if param_type == Tensor:
                return {"type": "audio"}
        except ModuleNotFoundError:
            pass
    return {"type": "object"}



================================================
FILE: src/smolagents/agent_types.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
import os
import pathlib
import tempfile
import uuid
from io import BytesIO
from typing import Any

import PIL.Image
import requests

from .utils import _is_package_available


logger = logging.getLogger(__name__)


class AgentType:
    """
    Abstract class to be reimplemented to define types that can be returned by agents.

    These objects serve three purposes:

    - They behave as they were the type they're meant to be, e.g., a string for text, a PIL.Image.Image for images
    - They can be stringified: str(object) in order to return a string defining the object
    - They should be displayed correctly in ipython notebooks/colab/jupyter
    """

    def __init__(self, value):
        self._value = value

    def __str__(self):
        return self.to_string()

    def to_raw(self):
        logger.error(
            "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
        )
        return self._value

    def to_string(self) -> str:
        logger.error(
            "This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable"
        )
        return str(self._value)


class AgentText(AgentType, str):
    """
    Text type returned by the agent. Behaves as a string.
    """

    def to_raw(self):
        return self._value

    def to_string(self):
        return str(self._value)


class AgentImage(AgentType, PIL.Image.Image):
    """
    Image type returned by the agent. Behaves as a PIL.Image.Image.
    """

    def __init__(self, value):
        AgentType.__init__(self, value)
        PIL.Image.Image.__init__(self)

        self._path = None
        self._raw = None
        self._tensor = None

        if isinstance(value, AgentImage):
            self._raw, self._path, self._tensor = value._raw, value._path, value._tensor
        elif isinstance(value, PIL.Image.Image):
            self._raw = value
        elif isinstance(value, bytes):
            self._raw = PIL.Image.open(BytesIO(value))
        elif isinstance(value, (str, pathlib.Path)):
            self._path = value
        else:
            try:
                import torch

                if isinstance(value, torch.Tensor):
                    self._tensor = value
                import numpy as np

                if isinstance(value, np.ndarray):
                    self._tensor = torch.from_numpy(value)
            except ModuleNotFoundError:
                pass

        if self._path is None and self._raw is None and self._tensor is None:
            raise TypeError(f"Unsupported type for {self.__class__.__name__}: {type(value)}")

    def _ipython_display_(self, include=None, exclude=None):
        """
        Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
        """
        from IPython.display import Image, display

        display(Image(self.to_string()))

    def to_raw(self):
        """
        Returns the "raw" version of that object. In the case of an AgentImage, it is a PIL.Image.Image.
        """
        if self._raw is not None:
            return self._raw

        if self._path is not None:
            self._raw = PIL.Image.open(self._path)
            return self._raw

        if self._tensor is not None:
            import numpy as np

            array = self._tensor.cpu().detach().numpy()
            return PIL.Image.fromarray((255 - array * 255).astype(np.uint8))

    def to_string(self):
        """
        Returns the stringified version of that object. In the case of an AgentImage, it is a path to the serialized
        version of the image.
        """
        if self._path is not None:
            return self._path

        if self._raw is not None:
            directory = tempfile.mkdtemp()
            self._path = os.path.join(directory, str(uuid.uuid4()) + ".png")
            self._raw.save(self._path, format="png")
            return self._path

        if self._tensor is not None:
            import numpy as np

            array = self._tensor.cpu().detach().numpy()

            # There is likely simpler than load into image into save
            img = PIL.Image.fromarray((255 - array * 255).astype(np.uint8))

            directory = tempfile.mkdtemp()
            self._path = os.path.join(directory, str(uuid.uuid4()) + ".png")
            img.save(self._path, format="png")

            return self._path

    def save(self, output_bytes, format: str = None, **params):
        """
        Saves the image to a file.
        Args:
            output_bytes (bytes): The output bytes to save the image to.
            format (str): The format to use for the output image. The format is the same as in PIL.Image.save.
            **params: Additional parameters to pass to PIL.Image.save.
        """
        img = self.to_raw()
        img.save(output_bytes, format=format, **params)


class AgentAudio(AgentType, str):
    """
    Audio type returned by the agent.
    """

    def __init__(self, value, samplerate=16_000):
        if not _is_package_available("soundfile") or not _is_package_available("torch"):
            raise ModuleNotFoundError(
                "Please install 'audio' extra to use AgentAudio: `pip install 'smolagents[audio]'`"
            )
        import numpy as np
        import torch

        super().__init__(value)

        self._path = None
        self._tensor = None

        self.samplerate = samplerate
        if isinstance(value, (str, pathlib.Path)):
            self._path = value
        elif isinstance(value, torch.Tensor):
            self._tensor = value
        elif isinstance(value, tuple):
            self.samplerate = value[0]
            if isinstance(value[1], np.ndarray):
                self._tensor = torch.from_numpy(value[1])
            else:
                self._tensor = torch.tensor(value[1])
        else:
            raise ValueError(f"Unsupported audio type: {type(value)}")

    def _ipython_display_(self, include=None, exclude=None):
        """
        Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)
        """
        from IPython.display import Audio, display

        display(Audio(self.to_string(), rate=self.samplerate))

    def to_raw(self):
        """
        Returns the "raw" version of that object. It is a `torch.Tensor` object.
        """
        import soundfile as sf

        if self._tensor is not None:
            return self._tensor

        import torch

        if self._path is not None:
            if "://" in str(self._path):
                response = requests.get(self._path)
                response.raise_for_status()
                tensor, self.samplerate = sf.read(BytesIO(response.content))
            else:
                tensor, self.samplerate = sf.read(self._path)
            self._tensor = torch.tensor(tensor)
            return self._tensor

    def to_string(self):
        """
        Returns the stringified version of that object. In the case of an AgentAudio, it is a path to the serialized
        version of the audio.
        """
        import soundfile as sf

        if self._path is not None:
            return self._path

        if self._tensor is not None:
            directory = tempfile.mkdtemp()
            self._path = os.path.join(directory, str(uuid.uuid4()) + ".wav")
            sf.write(self._path, self._tensor, samplerate=self.samplerate)
            return self._path


_AGENT_TYPE_MAPPING = {"string": AgentText, "image": AgentImage, "audio": AgentAudio}


def handle_agent_input_types(*args, **kwargs):
    args = [(arg.to_raw() if isinstance(arg, AgentType) else arg) for arg in args]
    kwargs = {k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()}
    return args, kwargs


def handle_agent_output_types(output: Any, output_type: str | None = None) -> Any:
    if output_type in _AGENT_TYPE_MAPPING:
        # If the class has defined outputs, we can map directly according to the class definition
        decoded_outputs = _AGENT_TYPE_MAPPING[output_type](output)
        return decoded_outputs

    # If the class does not have defined output, then we map according to the type
    if isinstance(output, str):
        return AgentText(output)
    if isinstance(output, PIL.Image.Image):
        return AgentImage(output)
    try:
        import torch

        if isinstance(output, torch.Tensor):
            return AgentAudio(output)
    except ModuleNotFoundError:
        pass
    return output


__all__ = ["AgentType", "AgentImage", "AgentText", "AgentAudio"]



================================================
FILE: src/smolagents/cli.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import argparse
import os

from dotenv import load_dotenv
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Confirm, Prompt
from rich.rule import Rule
from rich.table import Table

from smolagents import (
    CodeAgent,
    InferenceClientModel,
    LiteLLMModel,
    Model,
    OpenAIModel,
    Tool,
    ToolCallingAgent,
    TransformersModel,
)
from smolagents.default_tools import TOOL_MAPPING


console = Console()

leopard_prompt = "How many seconds would it take for a leopard at full speed to run through Pont des Arts?"


def parse_arguments():
    parser = argparse.ArgumentParser(description="Run a CodeAgent with all specified parameters")
    parser.add_argument(
        "prompt",
        type=str,
        nargs="?",
        default=None,
        help="The prompt to run with the agent. If no prompt is provided, interactive mode will be launched to guide user through agent setup",
    )
    parser.add_argument(
        "--model-type",
        type=str,
        default="InferenceClientModel",
        help="The model type to use (e.g., InferenceClientModel, OpenAIModel, LiteLLMModel, TransformersModel)",
    )
    parser.add_argument(
        "--action-type",
        type=str,
        default="code",
        help="The action type to use (e.g., code, tool_calling)",
    )
    parser.add_argument(
        "--model-id",
        type=str,
        default="Qwen/Qwen3-Next-80B-A3B-Thinking",
        help="The model ID to use for the specified model type",
    )
    parser.add_argument(
        "--imports",
        nargs="*",  # accepts zero or more arguments
        default=[],
        help="Space-separated list of imports to authorize (e.g., 'numpy pandas')",
    )
    parser.add_argument(
        "--tools",
        nargs="*",
        default=["web_search"],
        help="Space-separated list of tools that the agent can use (e.g., 'tool1 tool2 tool3')",
    )
    parser.add_argument(
        "--verbosity-level",
        type=int,
        default=1,
        help="The verbosity level, as an int in [0, 1, 2].",
    )
    group = parser.add_argument_group("api options", "Options for API-based model types")
    group.add_argument(
        "--provider",
        type=str,
        default=None,
        help="The inference provider to use for the model",
    )
    group.add_argument(
        "--api-base",
        type=str,
        help="The base URL for the model",
    )
    group.add_argument(
        "--api-key",
        type=str,
        help="The API key for the model",
    )
    return parser.parse_args()


def interactive_mode():
    """Run the CLI in interactive mode"""
    console.print(
        Panel.fit(
            "[bold magenta]ðŸ¤– SmolaGents CLI[/]\n[dim]Intelligent agents at your service[/]", border_style="magenta"
        )
    )

    console.print("\n[bold yellow]Welcome to smolagents![/] Let's set up your agent step by step.\n")

    # Get user input step by step
    console.print(Rule("[bold yellow]âš™ï¸  Configuration", style="bold yellow"))

    # Get agent action type
    action_type = Prompt.ask(
        "[bold white]What action type would you like to use? 'code' or 'tool_calling'?[/]",
        default="code",
        choices=["code", "tool_calling"],
    )

    # Show available tools
    tools_table = Table(title="[bold yellow]ðŸ› ï¸  Available Tools", show_header=True, header_style="bold yellow")
    tools_table.add_column("Tool Name", style="bold yellow")
    tools_table.add_column("Description", style="white")

    for tool_name, tool_class in TOOL_MAPPING.items():
        # Get description from the tool class if available
        try:
            tool_instance = tool_class()
            description = getattr(tool_instance, "description", "No description available")
        except Exception:
            description = "Built-in tool"
        tools_table.add_row(tool_name, description)

    console.print(tools_table)
    console.print(
        "\n[dim]You can also use HuggingFace Spaces by providing the full path (e.g., 'username/spacename')[/]"
    )

    console.print("[dim]Enter tool names separated by spaces (e.g., 'web_search python_interpreter')[/]")
    tools_input = Prompt.ask("[bold white]Select tools for your agent[/]", default="web_search")
    tools = tools_input.split()

    # Get model configuration
    console.print("\n[bold yellow]Model Configuration:[/]")
    model_type = Prompt.ask(
        "[bold]Model type[/]",
        default="InferenceClientModel",
        choices=["InferenceClientModel", "OpenAIServerModel", "LiteLLMModel", "TransformersModel"],
    )

    model_id = Prompt.ask("[bold white]Model ID[/]", default="Qwen/Qwen2.5-Coder-32B-Instruct")

    # Optional configurations
    provider = None
    api_base = None
    api_key = None
    imports = []
    action_type = "code"

    if Confirm.ask("\n[bold white]Configure advanced options?[/]", default=False):
        if model_type in ["InferenceClientModel", "OpenAIServerModel", "LiteLLMModel"]:
            provider = Prompt.ask("[bold white]Provider[/]", default="")
            api_base = Prompt.ask("[bold white]API Base URL[/]", default="")
            api_key = Prompt.ask("[bold white]API Key[/]", default="", password=True)

        imports_input = Prompt.ask("[bold white]Additional imports (space-separated)[/]", default="")
        if imports_input:
            imports = imports_input.split()

    # Get prompt
    prompt = Prompt.ask(
        "[bold white]Now the final step; what task would you like the agent to perform?[/]", default=leopard_prompt
    )

    return prompt, tools, model_type, model_id, provider, api_base, api_key, imports, action_type


def load_model(
    model_type: str,
    model_id: str,
    api_base: str | None = None,
    api_key: str | None = None,
    provider: str | None = None,
) -> Model:
    if model_type == "OpenAIModel":
        return OpenAIModel(
            api_key=api_key or os.getenv("FIREWORKS_API_KEY"),
            api_base=api_base or "https://api.fireworks.ai/inference/v1",
            model_id=model_id,
        )
    elif model_type == "LiteLLMModel":
        return LiteLLMModel(
            model_id=model_id,
            api_key=api_key,
            api_base=api_base,
        )
    elif model_type == "TransformersModel":
        return TransformersModel(model_id=model_id, device_map="auto")
    elif model_type == "InferenceClientModel":
        return InferenceClientModel(
            model_id=model_id,
            token=api_key or os.getenv("HF_API_KEY"),
            provider=provider,
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")


def run_smolagent(
    prompt: str,
    tools: list[str],
    model_type: str,
    model_id: str,
    api_base: str | None = None,
    api_key: str | None = None,
    imports: list[str] | None = None,
    provider: str | None = None,
    action_type: str = "code",
) -> None:
    load_dotenv()

    model = load_model(model_type, model_id, api_base=api_base, api_key=api_key, provider=provider)

    available_tools = []

    for tool_name in tools:
        if "/" in tool_name:
            space_name = tool_name.split("/")[-1].lower().replace("-", "_").replace(".", "_")
            description = f"Tool loaded from Hugging Face Space: {tool_name}"
            available_tools.append(Tool.from_space(space_id=tool_name, name=space_name, description=description))
        else:
            if tool_name in TOOL_MAPPING:
                available_tools.append(TOOL_MAPPING[tool_name]())
            else:
                raise ValueError(f"Tool {tool_name} is not recognized either as a default tool or a Space.")

    if action_type == "code":
        agent = CodeAgent(
            tools=available_tools,
            model=model,
            additional_authorized_imports=imports,
            stream_outputs=True,
        )
    elif action_type == "tool_calling":
        agent = ToolCallingAgent(tools=available_tools, model=model, stream_outputs=True)
    else:
        raise ValueError(f"Unsupported action type: {action_type}")

    agent.run(prompt)


def main() -> None:
    args = parse_arguments()

    # Check if we should run in interactive mode
    # Interactive mode is triggered when no prompt is provided
    if args.prompt is None:
        prompt, tools, model_type, model_id, provider, api_base, api_key, imports, action_type = interactive_mode()
    else:
        prompt = args.prompt
        tools = args.tools
        model_type = args.model_type
        model_id = args.model_id
        provider = args.provider
        api_base = args.api_base
        api_key = args.api_key
        imports = args.imports
        action_type = args.action_type

    run_smolagent(
        prompt,
        tools,
        model_type,
        model_id,
        provider=provider,
        api_base=api_base,
        api_key=api_key,
        imports=imports,
        action_type=action_type,
    )


if __name__ == "__main__":
    main()



================================================
FILE: src/smolagents/default_tools.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from dataclasses import dataclass
from typing import Any

from .local_python_executor import (
    BASE_BUILTIN_MODULES,
    BASE_PYTHON_TOOLS,
    evaluate_python_code,
)
from .tools import PipelineTool, Tool


@dataclass
class PreTool:
    name: str
    inputs: dict[str, str]
    output_type: type
    task: str
    description: str
    repo_id: str


class PythonInterpreterTool(Tool):
    name = "python_interpreter"
    description = "This is a tool that evaluates python code. It can be used to perform calculations."
    inputs = {
        "code": {
            "type": "string",
            "description": "The python code to run in interpreter",
        }
    }
    output_type = "string"

    def __init__(self, *args, authorized_imports=None, **kwargs):
        if authorized_imports is None:
            self.authorized_imports = list(set(BASE_BUILTIN_MODULES))
        else:
            self.authorized_imports = list(set(BASE_BUILTIN_MODULES) | set(authorized_imports))
        self.inputs = {
            "code": {
                "type": "string",
                "description": (
                    "The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, "
                    f"else you will get an error. This code can only import the following python libraries: {self.authorized_imports}."
                ),
            }
        }
        self.base_python_tools = BASE_PYTHON_TOOLS
        self.python_evaluator = evaluate_python_code
        super().__init__(*args, **kwargs)

    def forward(self, code: str) -> str:
        state = {}
        output = str(
            self.python_evaluator(
                code,
                state=state,
                static_tools=self.base_python_tools,
                authorized_imports=self.authorized_imports,
            )[0]  # The second element is boolean is_final_answer
        )
        return f"Stdout:\n{str(state['_print_outputs'])}\nOutput: {output}"


class FinalAnswerTool(Tool):
    name = "final_answer"
    description = "Provides a final answer to the given problem."
    inputs = {"answer": {"type": "any", "description": "The final answer to the problem"}}
    output_type = "any"

    def forward(self, answer: Any) -> Any:
        return answer


class UserInputTool(Tool):
    name = "user_input"
    description = "Asks for user's input on a specific question"
    inputs = {"question": {"type": "string", "description": "The question to ask the user"}}
    output_type = "string"

    def forward(self, question):
        user_input = input(f"{question} => Type your answer here:")
        return user_input


class DuckDuckGoSearchTool(Tool):
    """Web search tool that performs searches using the DuckDuckGo search engine.

    Args:
        max_results (`int`, default `10`): Maximum number of search results to return.
        rate_limit (`float`, default `1.0`): Maximum queries per second. Set to `None` to disable rate limiting.
        **kwargs: Additional keyword arguments for the `DDGS` client.

    Examples:
        ```python
        >>> from smolagents import DuckDuckGoSearchTool
        >>> web_search_tool = DuckDuckGoSearchTool(max_results=5, rate_limit=2.0)
        >>> results = web_search_tool("Hugging Face")
        >>> print(results)
        ```
    """

    name = "web_search"
    description = """Performs a duckduckgo web search based on your query (think a Google search) then returns the top search results."""
    inputs = {"query": {"type": "string", "description": "The search query to perform."}}
    output_type = "string"

    def __init__(self, max_results: int = 10, rate_limit: float | None = 1.0, **kwargs):
        super().__init__()
        self.max_results = max_results
        self.rate_limit = rate_limit
        self._min_interval = 1.0 / rate_limit if rate_limit else 0.0
        self._last_request_time = 0.0
        try:
            from ddgs import DDGS
        except ImportError as e:
            raise ImportError(
                "You must install package `ddgs` to run this tool: for instance run `pip install ddgs`."
            ) from e
        self.ddgs = DDGS(**kwargs)

    def forward(self, query: str) -> str:
        self._enforce_rate_limit()
        results = self.ddgs.text(query, max_results=self.max_results)
        if len(results) == 0:
            raise Exception("No results found! Try a less restrictive/shorter query.")
        postprocessed_results = [f"[{result['title']}]({result['href']})\n{result['body']}" for result in results]
        return "## Search Results\n\n" + "\n\n".join(postprocessed_results)

    def _enforce_rate_limit(self) -> None:
        import time

        # No rate limit enforced
        if not self.rate_limit:
            return

        now = time.time()
        elapsed = now - self._last_request_time
        if elapsed < self._min_interval:
            time.sleep(self._min_interval - elapsed)
        self._last_request_time = time.time()


class GoogleSearchTool(Tool):
    name = "web_search"
    description = """Performs a google web search for your query then returns a string of the top search results."""
    inputs = {
        "query": {"type": "string", "description": "The search query to perform."},
        "filter_year": {
            "type": "integer",
            "description": "Optionally restrict results to a certain year",
            "nullable": True,
        },
    }
    output_type = "string"

    def __init__(self, provider: str = "serpapi"):
        super().__init__()
        import os

        self.provider = provider
        if provider == "serpapi":
            self.organic_key = "organic_results"
            api_key_env_name = "SERPAPI_API_KEY"
        else:
            self.organic_key = "organic"
            api_key_env_name = "SERPER_API_KEY"
        self.api_key = os.getenv(api_key_env_name)
        if self.api_key is None:
            raise ValueError(f"Missing API key. Make sure you have '{api_key_env_name}' in your env variables.")

    def forward(self, query: str, filter_year: int | None = None) -> str:
        import requests

        if self.provider == "serpapi":
            params = {
                "q": query,
                "api_key": self.api_key,
                "engine": "google",
                "google_domain": "google.com",
            }
            base_url = "https://serpapi.com/search.json"
        else:
            params = {
                "q": query,
                "api_key": self.api_key,
            }
            base_url = "https://google.serper.dev/search"
        if filter_year is not None:
            params["tbs"] = f"cdr:1,cd_min:01/01/{filter_year},cd_max:12/31/{filter_year}"

        response = requests.get(base_url, params=params)

        if response.status_code == 200:
            results = response.json()
        else:
            raise ValueError(response.json())

        if self.organic_key not in results.keys():
            if filter_year is not None:
                raise Exception(
                    f"No results found for query: '{query}' with filtering on year={filter_year}. Use a less restrictive query or do not filter on year."
                )
            else:
                raise Exception(f"No results found for query: '{query}'. Use a less restrictive query.")
        if len(results[self.organic_key]) == 0:
            year_filter_message = f" with filter year={filter_year}" if filter_year is not None else ""
            return f"No results found for '{query}'{year_filter_message}. Try with a more general query, or remove the year filter."

        web_snippets = []
        if self.organic_key in results:
            for idx, page in enumerate(results[self.organic_key]):
                date_published = ""
                if "date" in page:
                    date_published = "\nDate published: " + page["date"]

                source = ""
                if "source" in page:
                    source = "\nSource: " + page["source"]

                snippet = ""
                if "snippet" in page:
                    snippet = "\n" + page["snippet"]

                redacted_version = f"{idx}. [{page['title']}]({page['link']}){date_published}{source}\n{snippet}"
                web_snippets.append(redacted_version)

        return "## Search Results\n" + "\n\n".join(web_snippets)


class ApiWebSearchTool(Tool):
    """Web search tool that performs API-based searches.
    By default, it uses the Brave Search API.

    This tool implements a rate limiting mechanism to ensure compliance with API usage policies.
    By default, it limits requests to 1 query per second.

    Args:
        endpoint (`str`): API endpoint URL. Defaults to Brave Search API.
        api_key (`str`): API key for authentication.
        api_key_name (`str`): Environment variable name containing the API key. Defaults to "BRAVE_API_KEY".
        headers (`dict`, *optional*): Headers for API requests.
        params (`dict`, *optional*): Parameters for API requests.
        rate_limit (`float`, default `1.0`): Maximum queries per second. Set to `None` to disable rate limiting.

    Examples:
        ```python
        >>> from smolagents import ApiWebSearchTool
        >>> web_search_tool = ApiWebSearchTool(rate_limit=50.0)
        >>> results = web_search_tool("Hugging Face")
        >>> print(results)
        ```
    """

    name = "web_search"
    description = "Performs a web search for a query and returns a string of the top search results formatted as markdown with titles, URLs, and descriptions."
    inputs = {"query": {"type": "string", "description": "The search query to perform."}}
    output_type = "string"

    def __init__(
        self,
        endpoint: str = "",
        api_key: str = "",
        api_key_name: str = "",
        headers: dict = None,
        params: dict = None,
        rate_limit: float | None = 1.0,
    ):
        import os

        super().__init__()
        self.endpoint = endpoint or "https://api.search.brave.com/res/v1/web/search"
        self.api_key_name = api_key_name or "BRAVE_API_KEY"
        self.api_key = api_key or os.getenv(self.api_key_name)
        self.headers = headers or {"X-Subscription-Token": self.api_key}
        self.params = params or {"count": 10}
        self.rate_limit = rate_limit
        self._min_interval = 1.0 / rate_limit if rate_limit else 0.0
        self._last_request_time = 0.0

    def _enforce_rate_limit(self) -> None:
        import time

        # No rate limit enforced
        if not self.rate_limit:
            return

        now = time.time()
        elapsed = now - self._last_request_time
        if elapsed < self._min_interval:
            time.sleep(self._min_interval - elapsed)
        self._last_request_time = time.time()

    def forward(self, query: str) -> str:
        import requests

        self._enforce_rate_limit()
        params = {**self.params, "q": query}
        response = requests.get(self.endpoint, headers=self.headers, params=params)
        response.raise_for_status()
        data = response.json()
        results = self.extract_results(data)
        return self.format_markdown(results)

    def extract_results(self, data: dict) -> list:
        results = []
        for result in data.get("web", {}).get("results", []):
            results.append(
                {"title": result["title"], "url": result["url"], "description": result.get("description", "")}
            )
        return results

    def format_markdown(self, results: list) -> str:
        if not results:
            return "No results found."
        return "## Search Results\n\n" + "\n\n".join(
            [
                f"{idx}. [{result['title']}]({result['url']})\n{result['description']}"
                for idx, result in enumerate(results, start=1)
            ]
        )


class WebSearchTool(Tool):
    name = "web_search"
    description = "Performs a web search for a query and returns a string of the top search results formatted as markdown with titles, links, and descriptions."
    inputs = {"query": {"type": "string", "description": "The search query to perform."}}
    output_type = "string"

    def __init__(self, max_results: int = 10, engine: str = "duckduckgo"):
        super().__init__()
        self.max_results = max_results
        self.engine = engine

    def forward(self, query: str) -> str:
        results = self.search(query)
        if len(results) == 0:
            raise Exception("No results found! Try a less restrictive/shorter query.")
        return self.parse_results(results)

    def search(self, query: str) -> list:
        if self.engine == "duckduckgo":
            return self.search_duckduckgo(query)
        elif self.engine == "bing":
            return self.search_bing(query)
        else:
            raise ValueError(f"Unsupported engine: {self.engine}")

    def parse_results(self, results: list) -> str:
        return "## Search Results\n\n" + "\n\n".join(
            [f"[{result['title']}]({result['link']})\n{result['description']}" for result in results]
        )

    def search_duckduckgo(self, query: str) -> list:
        import requests

        response = requests.get(
            "https://lite.duckduckgo.com/lite/",
            params={"q": query},
            headers={"User-Agent": "Mozilla/5.0"},
        )
        response.raise_for_status()
        parser = self._create_duckduckgo_parser()
        parser.feed(response.text)
        return parser.results

    def _create_duckduckgo_parser(self):
        from html.parser import HTMLParser

        class SimpleResultParser(HTMLParser):
            def __init__(self):
                super().__init__()
                self.results = []
                self.current = {}
                self.capture_title = False
                self.capture_description = False
                self.capture_link = False

            def handle_starttag(self, tag, attrs):
                attrs = dict(attrs)
                if tag == "a" and attrs.get("class") == "result-link":
                    self.capture_title = True
                elif tag == "td" and attrs.get("class") == "result-snippet":
                    self.capture_description = True
                elif tag == "span" and attrs.get("class") == "link-text":
                    self.capture_link = True

            def handle_endtag(self, tag):
                if tag == "a" and self.capture_title:
                    self.capture_title = False
                elif tag == "td" and self.capture_description:
                    self.capture_description = False
                elif tag == "span" and self.capture_link:
                    self.capture_link = False
                elif tag == "tr":
                    # Store current result if all parts are present
                    if {"title", "description", "link"} <= self.current.keys():
                        self.current["description"] = " ".join(self.current["description"])
                        self.results.append(self.current)
                        self.current = {}

            def handle_data(self, data):
                if self.capture_title:
                    self.current["title"] = data.strip()
                elif self.capture_description:
                    self.current.setdefault("description", [])
                    self.current["description"].append(data.strip())
                elif self.capture_link:
                    self.current["link"] = "https://" + data.strip()

        return SimpleResultParser()

    def search_bing(self, query: str) -> list:
        import xml.etree.ElementTree as ET

        import requests

        response = requests.get(
            "https://www.bing.com/search",
            params={"q": query, "format": "rss"},
        )
        response.raise_for_status()
        root = ET.fromstring(response.text)
        items = root.findall(".//item")
        results = [
            {
                "title": item.findtext("title"),
                "link": item.findtext("link"),
                "description": item.findtext("description"),
            }
            for item in items[: self.max_results]
        ]
        return results


class VisitWebpageTool(Tool):
    name = "visit_webpage"
    description = (
        "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages."
    )
    inputs = {
        "url": {
            "type": "string",
            "description": "The url of the webpage to visit.",
        }
    }
    output_type = "string"

    def __init__(self, max_output_length: int = 40000):
        super().__init__()
        self.max_output_length = max_output_length

    def _truncate_content(self, content: str, max_length: int) -> str:
        if len(content) <= max_length:
            return content
        return (
            content[:max_length] + f"\n..._This content has been truncated to stay below {max_length} characters_...\n"
        )

    def forward(self, url: str) -> str:
        try:
            import re

            import requests
            from markdownify import markdownify
            from requests.exceptions import RequestException
        except ImportError as e:
            raise ImportError(
                "You must install packages `markdownify` and `requests` to run this tool: for instance run `pip install markdownify requests`."
            ) from e
        try:
            # Send a GET request to the URL with a 20-second timeout
            response = requests.get(url, timeout=20)
            response.raise_for_status()  # Raise an exception for bad status codes

            # Convert the HTML content to Markdown
            markdown_content = markdownify(response.text).strip()

            # Remove multiple line breaks
            markdown_content = re.sub(r"\n{3,}", "\n\n", markdown_content)

            return self._truncate_content(markdown_content, self.max_output_length)

        except requests.exceptions.Timeout:
            return "The request timed out. Please try again later or check the URL."
        except RequestException as e:
            return f"Error fetching the webpage: {str(e)}"
        except Exception as e:
            return f"An unexpected error occurred: {str(e)}"


class WikipediaSearchTool(Tool):
    """
    Search Wikipedia and return the summary or full text of the requested article, along with the page URL.

    Attributes:
        user_agent (`str`): Custom user-agent string to identify the project. This is required as per Wikipedia API policies.
            See: https://foundation.wikimedia.org/wiki/Policy:Wikimedia_Foundation_User-Agent_Policy
        language (`str`, default `"en"`): Language in which to retrieve Wikipedia article.
            See: http://meta.wikimedia.org/wiki/List_of_Wikipedias
        content_type (`Literal["summary", "text"]`, default `"text"`): Type of content to fetch. Can be "summary" for a short summary or "text" for the full article.
        extract_format (`Literal["HTML", "WIKI"]`, default `"WIKI"`): Extraction format of the output. Can be `"WIKI"` or `"HTML"`.

    Example:
        ```python
        >>> from smolagents import CodeAgent, InferenceClientModel, WikipediaSearchTool
        >>> agent = CodeAgent(
        >>>     tools=[
        >>>            WikipediaSearchTool(
        >>>                user_agent="MyResearchBot (myemail@example.com)",
        >>>                language="en",
        >>>                content_type="summary",  # or "text"
        >>>                extract_format="WIKI",
        >>>            )
        >>>        ],
        >>>     model=InferenceClientModel(),
        >>> )
        >>> agent.run("Python_(programming_language)")
        ```
    """

    name = "wikipedia_search"
    description = "Searches Wikipedia and returns a summary or full text of the given topic, along with the page URL."
    inputs = {
        "query": {
            "type": "string",
            "description": "The topic to search on Wikipedia.",
        }
    }
    output_type = "string"

    def __init__(
        self,
        user_agent: str = "Smolagents (myemail@example.com)",
        language: str = "en",
        content_type: str = "text",
        extract_format: str = "WIKI",
    ):
        super().__init__()
        try:
            import wikipediaapi
        except ImportError as e:
            raise ImportError(
                "You must install `wikipedia-api` to run this tool: for instance run `pip install wikipedia-api`"
            ) from e
        if not user_agent:
            raise ValueError("User-agent is required. Provide a meaningful identifier for your project.")

        self.user_agent = user_agent
        self.language = language
        self.content_type = content_type

        # Map string format to wikipediaapi.ExtractFormat
        extract_format_map = {
            "WIKI": wikipediaapi.ExtractFormat.WIKI,
            "HTML": wikipediaapi.ExtractFormat.HTML,
        }

        if extract_format not in extract_format_map:
            raise ValueError("Invalid extract_format. Choose between 'WIKI' or 'HTML'.")

        self.extract_format = extract_format_map[extract_format]

        self.wiki = wikipediaapi.Wikipedia(
            user_agent=self.user_agent, language=self.language, extract_format=self.extract_format
        )

    def forward(self, query: str) -> str:
        try:
            page = self.wiki.page(query)

            if not page.exists():
                return f"No Wikipedia page found for '{query}'. Try a different query."

            title = page.title
            url = page.fullurl

            if self.content_type == "summary":
                text = page.summary
            elif self.content_type == "text":
                text = page.text
            else:
                return "âš ï¸ Invalid `content_type`. Use either 'summary' or 'text'."

            return f"âœ… **Wikipedia Page:** {title}\n\n**Content:** {text}\n\nðŸ”— **Read more:** {url}"

        except Exception as e:
            return f"Error fetching Wikipedia summary: {str(e)}"


class SpeechToTextTool(PipelineTool):
    default_checkpoint = "openai/whisper-large-v3-turbo"
    description = "This is a tool that transcribes an audio into text. It returns the transcribed text."
    name = "transcriber"
    inputs = {
        "audio": {
            "type": "audio",
            "description": "The audio to transcribe. Can be a local path, an url, or a tensor.",
        }
    }
    output_type = "string"

    def __new__(cls, *args, **kwargs):
        from transformers.models.whisper import WhisperForConditionalGeneration, WhisperProcessor

        cls.pre_processor_class = WhisperProcessor
        cls.model_class = WhisperForConditionalGeneration
        return super().__new__(cls)

    def encode(self, audio):
        from .agent_types import AgentAudio

        audio = AgentAudio(audio).to_raw()
        return self.pre_processor(audio, return_tensors="pt")

    def forward(self, inputs):
        return self.model.generate(inputs["input_features"])

    def decode(self, outputs):
        return self.pre_processor.batch_decode(outputs, skip_special_tokens=True)[0]


TOOL_MAPPING = {
    tool_class.name: tool_class
    for tool_class in [
        PythonInterpreterTool,
        DuckDuckGoSearchTool,
        VisitWebpageTool,
    ]
}

__all__ = [
    "ApiWebSearchTool",
    "PythonInterpreterTool",
    "FinalAnswerTool",
    "UserInputTool",
    "WebSearchTool",
    "DuckDuckGoSearchTool",
    "GoogleSearchTool",
    "VisitWebpageTool",
    "WikipediaSearchTool",
    "SpeechToTextTool",
]



================================================
FILE: src/smolagents/gradio_ui.py
================================================
#!/usr/bin/env python
# coding=utf-8
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import re
import shutil
from pathlib import Path
from typing import Generator

from smolagents.agent_types import AgentAudio, AgentImage, AgentText
from smolagents.agents import MultiStepAgent, PlanningStep
from smolagents.memory import ActionStep, FinalAnswerStep
from smolagents.models import ChatMessageStreamDelta, MessageRole, agglomerate_stream_deltas
from smolagents.utils import _is_package_available


def get_step_footnote_content(step_log: ActionStep | PlanningStep, step_name: str) -> str:
    """Get a footnote string for a step log with duration and token information"""
    step_footnote = f"**{step_name}**"
    if step_log.token_usage is not None:
        step_footnote += f" | Input tokens: {step_log.token_usage.input_tokens:,} | Output tokens: {step_log.token_usage.output_tokens:,}"
    step_footnote += f" | Duration: {round(float(step_log.timing.duration), 2)}s" if step_log.timing.duration else ""
    step_footnote_content = f"""<span style="color: #bbbbc2; font-size: 12px;">{step_footnote}</span> """
    return step_footnote_content


def _clean_model_output(model_output: str) -> str:
    """
    Clean up model output by removing trailing tags and extra backticks.

    Args:
        model_output (`str`): Raw model output.

    Returns:
        `str`: Cleaned model output.
    """
    if not model_output:
        return ""
    model_output = model_output.strip()
    # Remove any trailing <end_code> and extra backticks, handling multiple possible formats
    model_output = re.sub(r"```\s*<end_code>", "```", model_output)  # handles ```<end_code>
    model_output = re.sub(r"<end_code>\s*```", "```", model_output)  # handles <end_code>```
    model_output = re.sub(r"```\s*\n\s*<end_code>", "```", model_output)  # handles ```\n<end_code>
    return model_output.strip()


def _format_code_content(content: str) -> str:
    """
    Format code content as Python code block if it's not already formatted.

    Args:
        content (`str`): Code content to format.

    Returns:
        `str`: Code content formatted as a Python code block.
    """
    content = content.strip()
    # Remove existing code blocks and end_code tags
    content = re.sub(r"```.*?\n", "", content)
    content = re.sub(r"\s*<end_code>\s*", "", content)
    content = content.strip()
    # Add Python code block formatting if not already present
    if not content.startswith("```python"):
        content = f"```python\n{content}\n```"
    return content


def _process_action_step(step_log: ActionStep, skip_model_outputs: bool = False) -> Generator:
    """
    Process an [`ActionStep`] and yield appropriate Gradio ChatMessage objects.

    Args:
        step_log ([`ActionStep`]): ActionStep to process.
        skip_model_outputs (`bool`): Whether to skip model outputs.

    Yields:
        `gradio.ChatMessage`: Gradio ChatMessages representing the action step.
    """
    import gradio as gr

    # Output the step number
    step_number = f"Step {step_log.step_number}"
    if not skip_model_outputs:
        yield gr.ChatMessage(role=MessageRole.ASSISTANT, content=f"**{step_number}**", metadata={"status": "done"})

    # First yield the thought/reasoning from the LLM
    if not skip_model_outputs and getattr(step_log, "model_output", ""):
        model_output = _clean_model_output(step_log.model_output)
        yield gr.ChatMessage(role=MessageRole.ASSISTANT, content=model_output, metadata={"status": "done"})

    # For tool calls, create a parent message
    if getattr(step_log, "tool_calls", []):
        first_tool_call = step_log.tool_calls[0]
        used_code = first_tool_call.name == "python_interpreter"

        # Process arguments based on type
        args = first_tool_call.arguments
        if isinstance(args, dict):
            content = str(args.get("answer", str(args)))
        else:
            content = str(args).strip()

        # Format code content if needed
        if used_code:
            content = _format_code_content(content)

        # Create the tool call message
        parent_message_tool = gr.ChatMessage(
            role=MessageRole.ASSISTANT,
            content=content,
            metadata={
                "title": f"ðŸ› ï¸ Used tool {first_tool_call.name}",
                "status": "done",
            },
        )
        yield parent_message_tool

    # Display execution logs if they exist
    if getattr(step_log, "observations", "") and step_log.observations.strip():
        log_content = step_log.observations.strip()
        if log_content:
            log_content = re.sub(r"^Execution logs:\s*", "", log_content)
            yield gr.ChatMessage(
                role=MessageRole.ASSISTANT,
                content=f"```bash\n{log_content}\n",
                metadata={"title": "ðŸ“ Execution Logs", "status": "done"},
            )

    # Display any images in observations
    if getattr(step_log, "observations_images", []):
        for image in step_log.observations_images:
            path_image = AgentImage(image).to_string()
            yield gr.ChatMessage(
                role=MessageRole.ASSISTANT,
                content={"path": path_image, "mime_type": f"image/{path_image.split('.')[-1]}"},
                metadata={"title": "ðŸ–¼ï¸ Output Image", "status": "done"},
            )

    # Handle errors
    if getattr(step_log, "error", None):
        yield gr.ChatMessage(
            role=MessageRole.ASSISTANT, content=str(step_log.error), metadata={"title": "ðŸ’¥ Error", "status": "done"}
        )

    # Add step footnote and separator
    yield gr.ChatMessage(
        role=MessageRole.ASSISTANT,
        content=get_step_footnote_content(step_log, step_number),
        metadata={"status": "done"},
    )
    yield gr.ChatMessage(role=MessageRole.ASSISTANT, content="-----", metadata={"status": "done"})


def _process_planning_step(step_log: PlanningStep, skip_model_outputs: bool = False) -> Generator:
    """
    Process a [`PlanningStep`] and yield appropriate gradio.ChatMessage objects.

    Args:
        step_log ([`PlanningStep`]): PlanningStep to process.

    Yields:
        `gradio.ChatMessage`: Gradio ChatMessages representing the planning step.
    """
    import gradio as gr

    if not skip_model_outputs:
        yield gr.ChatMessage(role=MessageRole.ASSISTANT, content="**Planning step**", metadata={"status": "done"})
        yield gr.ChatMessage(role=MessageRole.ASSISTANT, content=step_log.plan, metadata={"status": "done"})
    yield gr.ChatMessage(
        role=MessageRole.ASSISTANT,
        content=get_step_footnote_content(step_log, "Planning step"),
        metadata={"status": "done"},
    )
    yield gr.ChatMessage(role=MessageRole.ASSISTANT, content="-----", metadata={"status": "done"})


def _process_final_answer_step(step_log: FinalAnswerStep) -> Generator:
    """
    Process a [`FinalAnswerStep`] and yield appropriate gradio.ChatMessage objects.

    Args:
        step_log ([`FinalAnswerStep`]): FinalAnswerStep to process.

    Yields:
        `gradio.ChatMessage`: Gradio ChatMessages representing the final answer.
    """
    import gradio as gr

    final_answer = step_log.output
    if isinstance(final_answer, AgentText):
        yield gr.ChatMessage(
            role=MessageRole.ASSISTANT,
            content=f"**Final answer:**\n{final_answer.to_string()}\n",
            metadata={"status": "done"},
        )
    elif isinstance(final_answer, AgentImage):
        yield gr.ChatMessage(
            role=MessageRole.ASSISTANT,
            content={"path": final_answer.to_string(), "mime_type": "image/png"},
            metadata={"status": "done"},
        )
    elif isinstance(final_answer, AgentAudio):
        yield gr.ChatMessage(
            role=MessageRole.ASSISTANT,
            content={"path": final_answer.to_string(), "mime_type": "audio/wav"},
            metadata={"status": "done"},
        )
    else:
        yield gr.ChatMessage(
            role=MessageRole.ASSISTANT, content=f"**Final answer:** {str(final_answer)}", metadata={"status": "done"}
        )


def pull_messages_from_step(step_log: ActionStep | PlanningStep | FinalAnswerStep, skip_model_outputs: bool = False):
    """Extract Gradio ChatMessage objects from agent steps with proper nesting.

    Args:
        step_log: The step log to display as gr.ChatMessage objects.
        skip_model_outputs: If True, skip the model outputs when creating the gr.ChatMessage objects:
            This is used for instance when streaming model outputs have already been displayed.
    """
    if not _is_package_available("gradio"):
        raise ModuleNotFoundError(
            "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`"
        )
    if isinstance(step_log, ActionStep):
        yield from _process_action_step(step_log, skip_model_outputs)
    elif isinstance(step_log, PlanningStep):
        yield from _process_planning_step(step_log, skip_model_outputs)
    elif isinstance(step_log, FinalAnswerStep):
        yield from _process_final_answer_step(step_log)
    else:
        raise ValueError(f"Unsupported step type: {type(step_log)}")


def stream_to_gradio(
    agent,
    task: str,
    task_images: list | None = None,
    reset_agent_memory: bool = False,
    additional_args: dict | None = None,
) -> Generator:
    """Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages."""

    if not _is_package_available("gradio"):
        raise ModuleNotFoundError(
            "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`"
        )
    accumulated_events: list[ChatMessageStreamDelta] = []
    for event in agent.run(
        task, images=task_images, stream=True, reset=reset_agent_memory, additional_args=additional_args
    ):
        if isinstance(event, ActionStep | PlanningStep | FinalAnswerStep):
            for message in pull_messages_from_step(
                event,
                # If we're streaming model outputs, no need to display them twice
                skip_model_outputs=getattr(agent, "stream_outputs", False),
            ):
                yield message
            accumulated_events = []
        elif isinstance(event, ChatMessageStreamDelta):
            accumulated_events.append(event)
            text = agglomerate_stream_deltas(accumulated_events).render_as_markdown()
            yield text


class GradioUI:
    """
    Gradio interface for interacting with a [`MultiStepAgent`].

    This class provides a web interface to interact with the agent in real-time, allowing users to submit prompts, upload files, and receive responses in a chat-like format.
    It uses the modern [`gradio.ChatInterface`] component for a native chatbot experience.
    It can reset the agent's memory at the start of each interaction if desired.
    It supports file uploads via multimodal input.
    This class requires the `gradio` extra to be installed: `pip install 'smolagents[gradio]'`.

    Args:
        agent ([`MultiStepAgent`]): The agent to interact with.
        file_upload_folder (`str`, *optional*): The folder where uploaded files will be saved.
            If not provided, file uploads are disabled.
        reset_agent_memory (`bool`, *optional*, defaults to `False`): Whether to reset the agent's memory at the start of each interaction.
            If `True`, the agent will not remember previous interactions.

    Raises:
        ModuleNotFoundError: If the `gradio` extra is not installed.

    Example:
        ```python
        from smolagents import CodeAgent, GradioUI, InferenceClientModel

        model = InferenceClientModel(model_id="meta-llama/Meta-Llama-3.1-8B-Instruct")
        agent = CodeAgent(tools=[], model=model)
        gradio_ui = GradioUI(agent, file_upload_folder="uploads", reset_agent_memory=True)
        gradio_ui.launch()
        ```
    """

    def __init__(self, agent: MultiStepAgent, file_upload_folder: str | None = None, reset_agent_memory: bool = False):
        if not _is_package_available("gradio"):
            raise ModuleNotFoundError(
                "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`"
            )
        self.agent = agent
        self.file_upload_folder = Path(file_upload_folder) if file_upload_folder is not None else None
        self.reset_agent_memory = reset_agent_memory
        self.name = getattr(agent, "name") or "Agent interface"
        self.description = getattr(agent, "description", None)
        if self.file_upload_folder is not None:
            if not self.file_upload_folder.exists():
                self.file_upload_folder.mkdir(parents=True, exist_ok=True)

    def _save_uploaded_file(self, file_path: str) -> str:
        """Save an uploaded file to the upload folder and return the new path."""
        if self.file_upload_folder is None:
            return file_path

        original_name = os.path.basename(file_path)
        sanitized_name = re.sub(r"[^\w\-.]", "_", original_name)
        dest_path = os.path.join(self.file_upload_folder, sanitized_name)
        shutil.copy(file_path, dest_path)
        return dest_path

    def upload_file(self, file, file_uploads_log: list, allowed_file_types: list | None = None):
        """
        Handle file upload with validation.

        Args:
            file: The uploaded file object.
            file_uploads_log: List to track uploaded files.
            allowed_file_types: List of allowed extensions. Defaults to [".pdf", ".docx", ".txt"].

        Returns:
            Tuple of (status textbox, updated file log).
        """
        import gradio as gr

        if file is None:
            return gr.Textbox(value="No file uploaded", visible=True), file_uploads_log

        if allowed_file_types is None:
            allowed_file_types = [".pdf", ".docx", ".txt"]

        file_ext = os.path.splitext(file.name)[1].lower()
        if file_ext not in allowed_file_types:
            return gr.Textbox(value="File type disallowed", visible=True), file_uploads_log

        file_path = self._save_uploaded_file(file.name)
        return gr.Textbox(value=f"File uploaded: {file_path}", visible=True), file_uploads_log + [file_path]

    def _process_message(self, message: str | dict) -> tuple[str, list[str] | None]:
        """Process incoming message and extract text and files."""
        if isinstance(message, str):
            return message, None

        text = message.get("text", "")
        files = message.get("files", [])

        if files and self.file_upload_folder:
            saved_files = [self._save_uploaded_file(f) for f in files]
            if saved_files:
                text += f"\nYou have been provided with these files: {saved_files}"
            return text, saved_files

        return text, files if files else None

    def _stream_response(self, message: str | dict, history: list[dict]) -> Generator:  # noqa: ARG002
        """Stream agent responses for ChatInterface."""
        import gradio as gr

        task, task_files = self._process_message(message)

        all_messages: list[gr.ChatMessage] = []
        accumulated_events: list[ChatMessageStreamDelta] = []
        streaming_msg_idx: int | None = None

        for event in self.agent.run(
            task, images=task_files, stream=True, reset=self.reset_agent_memory, additional_args=None
        ):
            if isinstance(event, ActionStep | PlanningStep | FinalAnswerStep):
                # Remove streaming message if present
                if streaming_msg_idx is not None:
                    all_messages.pop(streaming_msg_idx)
                    streaming_msg_idx = None

                for msg in pull_messages_from_step(
                    event,
                    skip_model_outputs=getattr(self.agent, "stream_outputs", False),
                ):
                    all_messages.append(
                        gr.ChatMessage(
                            role=msg.role,
                            content=msg.content,
                            metadata=msg.metadata,
                        )
                    )
                    yield all_messages
                accumulated_events = []
            elif isinstance(event, ChatMessageStreamDelta):
                accumulated_events.append(event)
                text = agglomerate_stream_deltas(accumulated_events).render_as_markdown()
                text = text.replace("<", r"\<").replace(">", r"\>")
                msg = gr.ChatMessage(role="assistant", content=text)
                if streaming_msg_idx is None:
                    streaming_msg_idx = len(all_messages)
                    all_messages.append(msg)
                else:
                    all_messages[streaming_msg_idx] = msg
                yield all_messages

    def launch(self, share: bool = True, **kwargs):
        """
        Launch the Gradio app with the agent interface.

        Args:
            share (`bool`, defaults to `True`): Whether to share the app publicly.
            **kwargs: Additional keyword arguments to pass to the Gradio launch method.
        """
        self.create_app().launch(debug=True, share=share, **kwargs)

    def create_app(self):
        import gradio as gr

        # Gradio 5.x requires type="messages", but Gradio 6 removed this parameter
        type_messages_kwarg = {"type": "messages"} if gr.__version__.startswith("5") else {}

        chatbot = gr.Chatbot(
            label="Agent",
            avatar_images=(
                None,
                "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/mascot_smol.png",
            ),
            latex_delimiters=[
                {"left": r"$$", "right": r"$$", "display": True},
                {"left": r"$", "right": r"$", "display": False},
                {"left": r"\[", "right": r"\]", "display": True},
                {"left": r"\(", "right": r"\)", "display": False},
            ],
            **type_messages_kwarg,
        )

        demo = gr.ChatInterface(
            fn=self._stream_response,
            chatbot=chatbot,
            title=self.name.replace("_", " ").capitalize(),
            multimodal=self.file_upload_folder is not None,
            save_history=True,
            **type_messages_kwarg,
        )
        return demo


__all__ = ["stream_to_gradio", "GradioUI"]



================================================
FILE: src/smolagents/mcp_client.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import warnings
from types import TracebackType
from typing import TYPE_CHECKING, Any

from smolagents.tools import Tool


__all__ = ["MCPClient"]

if TYPE_CHECKING:
    from mcpadapt.core import StdioServerParameters


class MCPClient:
    """Manages the connection to an MCP server and make its tools available to SmolAgents.

    Note: tools can only be accessed after the connection has been started with the
        `connect()` method, done during the init. If you don't use the context manager
        we strongly encourage to use "try ... finally" to ensure the connection is cleaned up.

    Args:
        server_parameters (StdioServerParameters | dict[str, Any] | list[StdioServerParameters | dict[str, Any]]):
            Configuration parameters to connect to the MCP server. Can be a list if you want to connect multiple MCPs at once.

            - An instance of `mcp.StdioServerParameters` for connecting a Stdio MCP server via standard input/output using a subprocess.

            - A `dict` with at least:
              - "url": URL of the server.
              - "transport": Transport protocol to use, one of:
                - "streamable-http": Streamable HTTP transport (default).
                - "sse": Legacy HTTP+SSE transport (deprecated).
        adapter_kwargs (dict[str, Any], optional):
            Additional keyword arguments to be passed directly to `MCPAdapt`.
        structured_output (bool, optional, defaults to False):
            Whether to enable structured output features for MCP tools. If True, enables:
            - Support for outputSchema in MCP tools
            - Structured content handling (structuredContent from MCP responses)
            - JSON parsing fallback for structured data
            If False, uses the original simple text-only behavior for backwards compatibility.

    Example:
        ```python
        # fully managed context manager + stdio
        with MCPClient(...) as tools:
            # tools are now available

        # context manager + Streamable HTTP transport:
        with MCPClient({"url": "http://localhost:8000/mcp", "transport": "streamable-http"}) as tools:
            # tools are now available

        # Enable structured output for advanced MCP tools:
        with MCPClient(server_parameters, structured_output=True) as tools:
            # tools with structured output support are now available

        # manually manage the connection via the mcp_client object:
        try:
            mcp_client = MCPClient(...)
            tools = mcp_client.get_tools()

            # use your tools here.
        finally:
            mcp_client.disconnect()
        ```
    """

    def __init__(
        self,
        server_parameters: "StdioServerParameters" | dict[str, Any] | list["StdioServerParameters" | dict[str, Any]],
        adapter_kwargs: dict[str, Any] | None = None,
        structured_output: bool | None = None,
    ):
        # Handle future warning for structured_output default value change
        if structured_output is None:
            warnings.warn(
                "Parameter 'structured_output' was not specified. "
                "Currently it defaults to False, but in version 1.25, the default will change to True. "
                "To suppress this warning, explicitly set structured_output=True (new behavior) or structured_output=False (legacy behavior). "
                "See documentation at https://huggingface.co/docs/smolagents/tutorials/tools#structured-output-and-output-schema-support for more details.",
                FutureWarning,
                stacklevel=2,
            )
            structured_output = False

        try:
            from mcpadapt.core import MCPAdapt
            from mcpadapt.smolagents_adapter import SmolAgentsAdapter
        except ModuleNotFoundError:
            raise ModuleNotFoundError("Please install 'mcp' extra to use MCPClient: `pip install 'smolagents[mcp]'`")
        if isinstance(server_parameters, dict):
            transport = server_parameters.get("transport")
            if transport is None:
                transport = "streamable-http"
                server_parameters["transport"] = transport
            if transport not in {"sse", "streamable-http"}:
                raise ValueError(
                    f"Unsupported transport: {transport}. Supported transports are 'streamable-http' and 'sse'."
                )
        adapter_kwargs = adapter_kwargs or {}
        self._adapter = MCPAdapt(
            server_parameters, SmolAgentsAdapter(structured_output=structured_output), **adapter_kwargs
        )
        self._tools: list[Tool] | None = None
        self.connect()

    def connect(self):
        """Connect to the MCP server and initialize the tools."""
        self._tools: list[Tool] = self._adapter.__enter__()

    def disconnect(
        self,
        exc_type: type[BaseException] | None = None,
        exc_value: BaseException | None = None,
        exc_traceback: TracebackType | None = None,
    ):
        """Disconnect from the MCP server"""
        self._adapter.__exit__(exc_type, exc_value, exc_traceback)

    def get_tools(self) -> list[Tool]:
        """The SmolAgents tools available from the MCP server.

        Note: for now, this always returns the tools available at the creation of the session,
        but it will in a future release return also new tools available from the MCP server if
        any at call time.

        Raises:
            ValueError: If the MCP server tools is None (usually assuming the server is not started).

        Returns:
            list[Tool]: The SmolAgents tools available from the MCP server.
        """
        if self._tools is None:
            raise ValueError(
                "Couldn't retrieve tools from MCP server, run `mcp_client.connect()` first before accessing `tools`"
            )
        return self._tools

    def __enter__(self) -> list[Tool]:
        """Connect to the MCP server and return the tools directly.

        Note that because of the `.connect` in the init, the mcp_client
        is already connected at this point.
        """
        return self._tools

    def __exit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        exc_traceback: TracebackType | None,
    ):
        """Disconnect from the MCP server."""
        self.disconnect(exc_type, exc_value, exc_traceback)



================================================
FILE: src/smolagents/memory.py
================================================
import inspect
from dataclasses import asdict, dataclass
from logging import getLogger
from typing import TYPE_CHECKING, Any, Callable, Type

from smolagents.models import ChatMessage, MessageRole, get_dict_from_nested_dataclasses
from smolagents.monitoring import AgentLogger, LogLevel, Timing, TokenUsage
from smolagents.utils import AgentError, make_json_serializable


if TYPE_CHECKING:
    import PIL.Image

    from smolagents.models import ChatMessage
    from smolagents.monitoring import AgentLogger


__all__ = ["AgentMemory"]


logger = getLogger(__name__)


@dataclass
class ToolCall:
    name: str
    arguments: Any
    id: str

    def dict(self):
        return {
            "id": self.id,
            "type": "function",
            "function": {
                "name": self.name,
                "arguments": make_json_serializable(self.arguments),
            },
        }


@dataclass
class MemoryStep:
    def dict(self):
        return asdict(self)

    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        raise NotImplementedError


@dataclass
class ActionStep(MemoryStep):
    step_number: int
    timing: Timing
    model_input_messages: list[ChatMessage] | None = None
    tool_calls: list[ToolCall] | None = None
    error: AgentError | None = None
    model_output_message: ChatMessage | None = None
    model_output: str | list[dict[str, Any]] | None = None
    code_action: str | None = None
    observations: str | None = None
    observations_images: list["PIL.Image.Image"] | None = None
    action_output: Any = None
    token_usage: TokenUsage | None = None
    is_final_answer: bool = False

    def dict(self):
        # We overwrite the method to parse the tool_calls and action_output manually
        return {
            "step_number": self.step_number,
            "timing": self.timing.dict(),
            "model_input_messages": [
                make_json_serializable(get_dict_from_nested_dataclasses(msg)) for msg in self.model_input_messages
            ]
            if self.model_input_messages
            else None,
            "tool_calls": [tc.dict() for tc in self.tool_calls] if self.tool_calls else [],
            "error": self.error.dict() if self.error else None,
            "model_output_message": make_json_serializable(get_dict_from_nested_dataclasses(self.model_output_message))
            if self.model_output_message
            else None,
            "model_output": self.model_output,
            "code_action": self.code_action,
            "observations": self.observations,
            "observations_images": [image.tobytes() for image in self.observations_images]
            if self.observations_images
            else None,
            "action_output": make_json_serializable(self.action_output),
            "token_usage": asdict(self.token_usage) if self.token_usage else None,
            "is_final_answer": self.is_final_answer,
        }

    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        messages = []
        if self.model_output is not None and not summary_mode:
            messages.append(
                ChatMessage(role=MessageRole.ASSISTANT, content=[{"type": "text", "text": self.model_output.strip()}])
            )

        if self.tool_calls is not None:
            messages.append(
                ChatMessage(
                    role=MessageRole.TOOL_CALL,
                    content=[
                        {
                            "type": "text",
                            "text": "Calling tools:\n" + str([tc.dict() for tc in self.tool_calls]),
                        }
                    ],
                )
            )

        if self.observations_images:
            messages.append(
                ChatMessage(
                    role=MessageRole.USER,
                    content=[
                        {
                            "type": "image",
                            "image": image,
                        }
                        for image in self.observations_images
                    ],
                )
            )

        if self.observations is not None:
            messages.append(
                ChatMessage(
                    role=MessageRole.TOOL_RESPONSE,
                    content=[
                        {
                            "type": "text",
                            "text": f"Observation:\n{self.observations}",
                        }
                    ],
                )
            )
        if self.error is not None:
            error_message = (
                "Error:\n"
                + str(self.error)
                + "\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\n"
            )
            message_content = f"Call id: {self.tool_calls[0].id}\n" if self.tool_calls else ""
            message_content += error_message
            messages.append(
                ChatMessage(role=MessageRole.TOOL_RESPONSE, content=[{"type": "text", "text": message_content}])
            )

        return messages


@dataclass
class PlanningStep(MemoryStep):
    model_input_messages: list[ChatMessage]
    model_output_message: ChatMessage
    plan: str
    timing: Timing
    token_usage: TokenUsage | None = None

    def dict(self):
        return {
            "model_input_messages": [
                make_json_serializable(get_dict_from_nested_dataclasses(msg)) for msg in self.model_input_messages
            ],
            "model_output_message": make_json_serializable(
                get_dict_from_nested_dataclasses(self.model_output_message)
            ),
            "plan": self.plan,
            "timing": self.timing.dict(),
            "token_usage": asdict(self.token_usage) if self.token_usage else None,
        }

    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        if summary_mode:
            return []
        return [
            ChatMessage(role=MessageRole.ASSISTANT, content=[{"type": "text", "text": self.plan.strip()}]),
            ChatMessage(
                role=MessageRole.USER, content=[{"type": "text", "text": "Now proceed and carry out this plan."}]
            ),
            # This second message creates a role change to prevent models models from simply continuing the plan message
        ]


@dataclass
class TaskStep(MemoryStep):
    task: str
    task_images: list["PIL.Image.Image"] | None = None

    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        content = [{"type": "text", "text": f"New task:\n{self.task}"}]
        if self.task_images:
            content.extend([{"type": "image", "image": image} for image in self.task_images])

        return [ChatMessage(role=MessageRole.USER, content=content)]


@dataclass
class SystemPromptStep(MemoryStep):
    system_prompt: str

    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        if summary_mode:
            return []
        return [ChatMessage(role=MessageRole.SYSTEM, content=[{"type": "text", "text": self.system_prompt}])]


@dataclass
class FinalAnswerStep(MemoryStep):
    output: Any


class AgentMemory:
    """Memory for the agent, containing the system prompt and all steps taken by the agent.

    This class is used to store the agent's steps, including tasks, actions, and planning steps.
    It allows for resetting the memory, retrieving succinct or full step information, and replaying the agent's steps.

    Args:
        system_prompt (`str`): System prompt for the agent, which sets the context and instructions for the agent's behavior.

    **Attributes**:
        - **system_prompt** (`SystemPromptStep`) -- System prompt step for the agent.
        - **steps** (`list[TaskStep | ActionStep | PlanningStep]`) -- List of steps taken by the agent, which can include tasks, actions, and planning steps.
    """

    def __init__(self, system_prompt: str):
        self.system_prompt: SystemPromptStep = SystemPromptStep(system_prompt=system_prompt)
        self.steps: list[TaskStep | ActionStep | PlanningStep] = []

    def reset(self):
        """Reset the agent's memory, clearing all steps and keeping the system prompt."""
        self.steps = []

    def get_succinct_steps(self) -> list[dict]:
        """Return a succinct representation of the agent's steps, excluding model input messages."""
        return [
            {key: value for key, value in step.dict().items() if key != "model_input_messages"} for step in self.steps
        ]

    def get_full_steps(self) -> list[dict]:
        """Return a full representation of the agent's steps, including model input messages."""
        if len(self.steps) == 0:
            return []
        return [step.dict() for step in self.steps]

    def replay(self, logger: AgentLogger, detailed: bool = False):
        """Prints a pretty replay of the agent's steps.

        Args:
            logger (`AgentLogger`): The logger to print replay logs to.
            detailed (`bool`, default `False`): If True, also displays the memory at each step. Defaults to False.
                Careful: will increase log length exponentially. Use only for debugging.
        """
        logger.console.log("Replaying the agent's steps:")
        logger.log_markdown(title="System prompt", content=self.system_prompt.system_prompt, level=LogLevel.ERROR)
        for step in self.steps:
            if isinstance(step, TaskStep):
                logger.log_task(step.task, "", level=LogLevel.ERROR)
            elif isinstance(step, ActionStep):
                logger.log_rule(f"Step {step.step_number}", level=LogLevel.ERROR)
                if detailed and step.model_input_messages is not None:
                    logger.log_messages(step.model_input_messages, level=LogLevel.ERROR)
                if step.model_output is not None:
                    logger.log_markdown(title="Agent output:", content=step.model_output, level=LogLevel.ERROR)
            elif isinstance(step, PlanningStep):
                logger.log_rule("Planning step", level=LogLevel.ERROR)
                if detailed and step.model_input_messages is not None:
                    logger.log_messages(step.model_input_messages, level=LogLevel.ERROR)
                logger.log_markdown(title="Agent output:", content=step.plan, level=LogLevel.ERROR)

    def return_full_code(self) -> str:
        """Returns all code actions from the agent's steps, concatenated as a single script."""
        return "\n\n".join(
            [step.code_action for step in self.steps if isinstance(step, ActionStep) and step.code_action is not None]
        )


class CallbackRegistry:
    """Registry for callbacks that are called at each step of the agent's execution.

    Callbacks are registered by passing a step class and a callback function.
    """

    def __init__(self):
        self._callbacks: dict[Type[MemoryStep], list[Callable]] = {}

    def register(self, step_cls: Type[MemoryStep], callback: Callable):
        """Register a callback for a step class.

        Args:
            step_cls (Type[MemoryStep]): Step class to register the callback for.
            callback (Callable): Callback function to register.
        """
        if step_cls not in self._callbacks:
            self._callbacks[step_cls] = []
        self._callbacks[step_cls].append(callback)

    def callback(self, memory_step, **kwargs):
        """Call callbacks registered for a step type.

        Args:
            memory_step (MemoryStep): Step to call the callbacks for.
            **kwargs: Additional arguments to pass to callbacks that accept them.
                Typically, includes the agent instance.

        Notes:
            For backwards compatibility, callbacks with a single parameter signature
            receive only the memory_step, while callbacks with multiple parameters
            receive both the memory_step and any additional kwargs.
        """
        # For compatibility with old callbacks that only take the step as an argument
        for cls in memory_step.__class__.__mro__:
            for cb in self._callbacks.get(cls, []):
                cb(memory_step) if len(inspect.signature(cb).parameters) == 1 else cb(memory_step, **kwargs)



================================================
FILE: src/smolagents/monitoring.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
from dataclasses import dataclass, field
from enum import IntEnum

from rich import box
from rich.console import Console, Group
from rich.panel import Panel
from rich.rule import Rule
from rich.syntax import Syntax
from rich.table import Table
from rich.text import Text
from rich.tree import Tree

from smolagents.utils import escape_code_brackets


__all__ = ["AgentLogger", "LogLevel", "Monitor", "TokenUsage", "Timing"]


@dataclass
class TokenUsage:
    """
    Contains the token usage information for a given step or run.
    """

    input_tokens: int
    output_tokens: int
    total_tokens: int = field(init=False)

    def __post_init__(self):
        self.total_tokens = self.input_tokens + self.output_tokens

    def dict(self):
        return {
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "total_tokens": self.total_tokens,
        }


@dataclass
class Timing:
    """
    Contains the timing information for a given step or run.
    """

    start_time: float
    end_time: float | None = None

    @property
    def duration(self):
        return None if self.end_time is None else self.end_time - self.start_time

    def dict(self):
        return {
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration": self.duration,
        }

    def __repr__(self) -> str:
        return f"Timing(start_time={self.start_time}, end_time={self.end_time}, duration={self.duration})"


class Monitor:
    def __init__(self, tracked_model, logger):
        self.step_durations = []
        self.tracked_model = tracked_model
        self.logger = logger
        self.total_input_token_count = 0
        self.total_output_token_count = 0

    def get_total_token_counts(self) -> TokenUsage:
        return TokenUsage(
            input_tokens=self.total_input_token_count,
            output_tokens=self.total_output_token_count,
        )

    def reset(self):
        self.step_durations = []
        self.total_input_token_count = 0
        self.total_output_token_count = 0

    def update_metrics(self, step_log):
        """Update the metrics of the monitor.

        Args:
            step_log ([`MemoryStep`]): Step log to update the monitor with.
        """
        step_duration = step_log.timing.duration
        self.step_durations.append(step_duration)
        console_outputs = f"[Step {len(self.step_durations)}: Duration {step_duration:.2f} seconds"

        if step_log.token_usage is not None:
            self.total_input_token_count += step_log.token_usage.input_tokens
            self.total_output_token_count += step_log.token_usage.output_tokens
            console_outputs += (
                f"| Input tokens: {self.total_input_token_count:,} | Output tokens: {self.total_output_token_count:,}"
            )
        console_outputs += "]"
        self.logger.log(Text(console_outputs, style="dim"), level=1)


class LogLevel(IntEnum):
    OFF = -1  # No output
    ERROR = 0  # Only errors
    INFO = 1  # Normal output (default)
    DEBUG = 2  # Detailed output


YELLOW_HEX = "#d4b702"


class AgentLogger:
    def __init__(self, level: LogLevel = LogLevel.INFO, console: Console | None = None):
        self.level = level
        if console is None:
            self.console = Console(highlight=False)
        else:
            self.console = console

    def log(self, *args, level: int | str | LogLevel = LogLevel.INFO, **kwargs) -> None:
        """Logs a message to the console.

        Args:
            level (LogLevel, optional): Defaults to LogLevel.INFO.
        """
        if isinstance(level, str):
            level = LogLevel[level.upper()]
        if level <= self.level:
            self.console.print(*args, **kwargs)

    def log_error(self, error_message: str) -> None:
        self.log(escape_code_brackets(error_message), style="bold red", level=LogLevel.ERROR)

    def log_markdown(self, content: str, title: str | None = None, level=LogLevel.INFO, style=YELLOW_HEX) -> None:
        markdown_content = Syntax(
            content,
            lexer="markdown",
            theme="github-dark",
            word_wrap=True,
        )
        if title:
            self.log(
                Group(
                    Rule(
                        "[bold italic]" + title,
                        align="left",
                        style=style,
                    ),
                    markdown_content,
                ),
                level=level,
            )
        else:
            self.log(markdown_content, level=level)

    def log_code(self, title: str, content: str, level: int = LogLevel.INFO) -> None:
        self.log(
            Panel(
                Syntax(
                    content,
                    lexer="python",
                    theme="monokai",
                    word_wrap=True,
                ),
                title="[bold]" + title,
                title_align="left",
                box=box.HORIZONTALS,
            ),
            level=level,
        )

    def log_rule(self, title: str, level: int = LogLevel.INFO) -> None:
        self.log(
            Rule(
                "[bold white]" + title,
                characters="â”",
                style=YELLOW_HEX,
            ),
            level=LogLevel.INFO,
        )

    def log_task(self, content: str, subtitle: str, title: str | None = None, level: LogLevel = LogLevel.INFO) -> None:
        self.log(
            Panel(
                f"\n[bold]{escape_code_brackets(content)}\n",
                title="[bold]New run" + (f" - {title}" if title else ""),
                subtitle=subtitle,
                border_style=YELLOW_HEX,
                subtitle_align="left",
            ),
            level=level,
        )

    def log_messages(self, messages: list[dict], level: LogLevel = LogLevel.DEBUG) -> None:
        messages_as_string = "\n".join([json.dumps(message.dict(), indent=4) for message in messages])
        self.log(
            Syntax(
                messages_as_string,
                lexer="markdown",
                theme="github-dark",
                word_wrap=True,
            ),
            level=level,
        )

    def visualize_agent_tree(self, agent):
        def create_tools_section(tools_dict):
            table = Table(show_header=True, header_style="bold")
            table.add_column("Name", style="#1E90FF")
            table.add_column("Description")
            table.add_column("Arguments")

            for name, tool in tools_dict.items():
                args = [
                    f"{arg_name} (`{info.get('type', 'Any')}`{', optional' if info.get('optional') else ''}): {info.get('description', '')}"
                    for arg_name, info in getattr(tool, "inputs", {}).items()
                ]
                table.add_row(name, getattr(tool, "description", str(tool)), "\n".join(args))

            return Group("ðŸ› ï¸ [italic #1E90FF]Tools:[/italic #1E90FF]", table)

        def get_agent_headline(agent, name: str | None = None):
            name_headline = f"{name} | " if name else ""
            return f"[bold {YELLOW_HEX}]{name_headline}{agent.__class__.__name__} | {agent.model.model_id}"

        def build_agent_tree(parent_tree, agent_obj):
            """Recursively builds the agent tree."""
            parent_tree.add(create_tools_section(agent_obj.tools))

            if agent_obj.managed_agents:
                agents_branch = parent_tree.add("ðŸ¤– [italic #1E90FF]Managed agents:")
                for name, managed_agent in agent_obj.managed_agents.items():
                    agent_tree = agents_branch.add(get_agent_headline(managed_agent, name))
                    if managed_agent.__class__.__name__ == "CodeAgent":
                        agent_tree.add(
                            f"âœ… [italic #1E90FF]Authorized imports:[/italic #1E90FF] {managed_agent.additional_authorized_imports}"
                        )
                    agent_tree.add(f"ðŸ“ [italic #1E90FF]Description:[/italic #1E90FF] {managed_agent.description}")
                    build_agent_tree(agent_tree, managed_agent)

        main_tree = Tree(get_agent_headline(agent))
        if agent.__class__.__name__ == "CodeAgent":
            main_tree.add(
                f"âœ… [italic #1E90FF]Authorized imports:[/italic #1E90FF] {agent.additional_authorized_imports}"
            )
        build_agent_tree(main_tree, agent)
        self.console.print(main_tree)



================================================
FILE: src/smolagents/remote_executors.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import base64
import inspect
import json
import os
import pickle
import re
import secrets
import subprocess
import tempfile
import time
import uuid
from contextlib import closing
from io import BytesIO
from textwrap import dedent
from typing import Any, Optional

import PIL.Image
import requests
from requests.exceptions import RequestException

from .default_tools import FinalAnswerTool
from .local_python_executor import CodeOutput, PythonExecutor
from .monitoring import LogLevel
from .tools import Tool, get_tools_definition_code
from .utils import AgentError


__all__ = ["BlaxelExecutor", "E2BExecutor", "ModalExecutor", "DockerExecutor", "WasmExecutor"]


try:
    from dotenv import load_dotenv

    load_dotenv()
except ModuleNotFoundError:
    pass


class RemotePythonExecutor(PythonExecutor):
    FINAL_ANSWER_EXCEPTION = "FinalAnswerException"

    def __init__(self, additional_imports: list[str], logger):
        self.additional_imports = additional_imports
        self.logger = logger
        self.logger.log("Initializing executor, hold on...")
        self.installed_packages = []

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        """
        Execute code, return the result and output, also determining if
        the result is the final answer.
        """
        raise NotImplementedError

    def send_tools(self, tools: dict[str, Tool]):
        if "final_answer" in tools:
            self._patch_final_answer_with_exception(tools["final_answer"])
        # Install tool packages
        packages_to_install = {
            pkg
            for tool in tools.values()
            for pkg in tool.to_dict()["requirements"]
            if pkg not in self.installed_packages + ["smolagents"]
        }
        if packages_to_install:
            self.installed_packages += self.install_packages(list(packages_to_install))
        # Get tool definitions
        code = get_tools_definition_code(tools)
        if code:
            code_output = self.run_code_raise_errors(code)
            self.logger.log(code_output.logs)

    def send_variables(self, variables: dict[str, Any]):
        """
        Send variables to the kernel namespace using pickle.
        """
        if not variables:
            return
        pickled_vars = base64.b64encode(pickle.dumps(variables)).decode()
        code = f"""
import pickle, base64
vars_dict = pickle.loads(base64.b64decode('{pickled_vars}'))
locals().update(vars_dict)
"""
        self.run_code_raise_errors(code)

    def __call__(self, code_action: str) -> CodeOutput:
        """Run the code and determine if it is the final answer."""
        return self.run_code_raise_errors(code_action)

    def install_packages(self, additional_imports: list[str]):
        if additional_imports:
            code_output = self.run_code_raise_errors(f"!pip install {' '.join(additional_imports)}")
            self.logger.log(code_output.logs)
        return additional_imports

    def _patch_final_answer_with_exception(self, final_answer_tool: FinalAnswerTool):
        """Patch the FinalAnswerTool to raise an exception.

        This is necessary because the remote executors
        rely on the FinalAnswerTool to detect the final answer.
        It modifies the `forward` method of the FinalAnswerTool to raise
        a `FinalAnswerException` with the final answer as a pickled value.
        This allows the executor to catch this exception and return the final answer.

        Args:
            final_answer_tool (`FinalAnswerTool`): FinalAnswerTool instance to patch.
        """

        # Create a new class that inherits from the original FinalAnswerTool
        class _FinalAnswerTool(final_answer_tool.__class__):
            pass

        # Add a new forward method that raises the FinalAnswerException
        # - Define the new forward method function
        def forward(self, *args, **kwargs) -> Any:
            import base64
            import pickle

            class FinalAnswerException(BaseException):
                def __init__(self, value):
                    self.value = value

            raise FinalAnswerException(base64.b64encode(pickle.dumps(self._forward(*args, **kwargs))).decode())

        # - Set the new forward method function to the _FinalAnswerTool class
        _FinalAnswerTool.forward = forward

        # Rename the original forward method to _forward
        # - Get the original forward method function from the final_answer_tool instance
        original_forward_function = final_answer_tool.forward.__func__
        # - Set the new _forward method function to the _FinalAnswerTool class
        _FinalAnswerTool._forward = original_forward_function
        # - Update the source code of the new forward method to match the original but with the new name
        _FinalAnswerTool._forward.__source__ = inspect.getsource(original_forward_function).replace(
            "def forward(", "def _forward("
        )

        # Set the new class as the class of the final_answer_tool instance
        final_answer_tool.__class__ = _FinalAnswerTool


class E2BExecutor(RemotePythonExecutor):
    """
    Executes Python code using E2B.

    Args:
        additional_imports (`list[str]`): Additional imports to install.
        logger (`Logger`): Logger to use.
        **kwargs: Additional arguments to pass to the E2B Sandbox.
    """

    def __init__(self, additional_imports: list[str], logger, **kwargs):
        super().__init__(additional_imports, logger)
        try:
            from e2b_code_interpreter import Sandbox
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                """Please install 'e2b' extra to use E2BExecutor: `pip install 'smolagents[e2b]'`"""
            )
        # Support both e2b v1 and v2 constructors
        # v2 exposes Sandbox.create(...), while v1 uses Sandbox(...)
        if hasattr(Sandbox, "create"):
            self.sandbox = Sandbox.create(**kwargs)
        else:
            self.sandbox = Sandbox(**kwargs)
        self.installed_packages = self.install_packages(additional_imports)
        self.logger.log("E2B is running", level=LogLevel.INFO)

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        execution = self.sandbox.run_code(code)
        execution_logs = "\n".join([str(log) for log in execution.logs.stdout])

        # Handle errors
        if execution.error:
            # Check if the error is a FinalAnswerException
            if execution.error.name == RemotePythonExecutor.FINAL_ANSWER_EXCEPTION:
                final_answer = pickle.loads(base64.b64decode(execution.error.value))
                return CodeOutput(output=final_answer, logs=execution_logs, is_final_answer=True)

            # Construct error message
            error_message = (
                f"{execution_logs}\n"
                f"Executing code yielded an error:\n"
                f"{execution.error.name}\n"
                f"{execution.error.value}\n"
                f"{execution.error.traceback}"
            )
            raise AgentError(error_message, self.logger)

        # Handle results
        if not execution.results:
            return CodeOutput(output=None, logs=execution_logs, is_final_answer=False)

        for result in execution.results:
            if not result.is_main_result:
                continue
            # Handle image outputs
            for attribute_name in ["jpeg", "png"]:
                img_data = getattr(result, attribute_name, None)
                if img_data is not None:
                    decoded_bytes = base64.b64decode(img_data.encode("utf-8"))
                    return CodeOutput(
                        output=PIL.Image.open(BytesIO(decoded_bytes)), logs=execution_logs, is_final_answer=False
                    )
            # Handle other data formats
            for attribute_name in [
                "chart",
                "data",
                "html",
                "javascript",
                "json",
                "latex",
                "markdown",
                "pdf",
                "svg",
                "text",
            ]:
                data = getattr(result, attribute_name, None)
                if data is not None:
                    return CodeOutput(output=data, logs=execution_logs, is_final_answer=False)
        # If no main result found, return None
        return CodeOutput(output=None, logs=execution_logs, is_final_answer=False)

    def cleanup(self):
        """Clean up the E2B sandbox and resources."""
        try:
            if hasattr(self, "sandbox"):
                self.logger.log("Shutting down sandbox...", level=LogLevel.INFO)
                self.sandbox.kill()
                self.logger.log("Sandbox cleanup completed", level=LogLevel.INFO)
                del self.sandbox
        except Exception as e:
            self.logger.log_error(f"Error during cleanup: {e}")


def _websocket_send_execute_request(code: str, ws) -> str:
    """Send code execution request to kernel."""
    import uuid

    # Generate a unique message ID
    msg_id = str(uuid.uuid4())

    # Create execute request
    execute_request = {
        "header": {
            "msg_id": msg_id,
            "username": "anonymous",
            "session": str(uuid.uuid4()),
            "msg_type": "execute_request",
            "version": "5.0",
        },
        "parent_header": {},
        "metadata": {},
        "content": {
            "code": code,
            "silent": False,
            "store_history": True,
            "user_expressions": {},
            "allow_stdin": False,
        },
    }

    ws.send(json.dumps(execute_request))
    return msg_id


def _websocket_run_code_raise_errors(code: str, ws, logger) -> CodeOutput:
    """Run code over a websocket."""
    try:
        # Send execute request
        msg_id = _websocket_send_execute_request(code, ws)

        # Collect output and results
        outputs = []
        result = None
        is_final_answer = False

        while True:
            msg = json.loads(ws.recv())
            parent_msg_id = msg.get("parent_header", {}).get("msg_id")
            # Skip unrelated messages
            if parent_msg_id != msg_id:
                continue
            msg_type = msg.get("msg_type", "")
            msg_content = msg.get("content", {})
            if msg_type == "stream":
                outputs.append(msg_content["text"])
            elif msg_type == "execute_result":
                result = msg_content["data"].get("text/plain", None)
            elif msg_type == "error":
                if msg_content.get("ename", "") == RemotePythonExecutor.FINAL_ANSWER_EXCEPTION:
                    result = pickle.loads(base64.b64decode(msg_content.get("evalue", "")))
                    is_final_answer = True
                else:
                    raise AgentError("\n".join(msg_content.get("traceback", [])), logger)
            elif msg_type == "status" and msg_content["execution_state"] == "idle":
                break

        return CodeOutput(output=result, logs="".join(outputs), is_final_answer=is_final_answer)

    except Exception as e:
        logger.log_error(f"Code execution failed: {e}")
        raise


def _create_kernel_http(crate_kernel_endpoint: str, logger, headers: Optional[dict] = None) -> str:
    """Create kernel using http."""

    r = requests.post(crate_kernel_endpoint, headers=headers)
    if r.status_code != 201:
        error_details = {
            "status_code": r.status_code,
            "headers": dict(r.headers),
            "url": r.url,
            "body": r.text,
            "request_method": r.request.method,
            "request_headers": dict(r.request.headers),
            "request_body": r.request.body,
        }
        logger.log_error(f"Failed to create kernel. Details: {json.dumps(error_details, indent=2)}")
        raise RuntimeError(f"Failed to create kernel: Status {r.status_code}\nResponse: {r.text}") from None
    return r.json()["id"]


class DockerExecutor(RemotePythonExecutor):
    """
    Executes Python code using Jupyter Kernel Gateway in a Docker container.
    """

    def __init__(
        self,
        additional_imports: list[str],
        logger,
        host: str = "127.0.0.1",
        port: int = 8888,
        image_name: str = "jupyter-kernel",
        build_new_image: bool = True,
        container_run_kwargs: dict[str, Any] | None = None,
        dockerfile_content: str | None = None,
    ):
        """
        Initialize the Docker-based Jupyter Kernel Gateway executor.

        Args:
            additional_imports: Additional imports to install.
            logger: Logger to use.
            host: Host to bind to.
            port: Port to bind to.
            image_name: Name of the Docker image to use. If the image doesn't exist, it will be built.
            build_new_image: If True, the image will be rebuilt even if it already exists.
            container_run_kwargs: Additional keyword arguments to pass to the Docker container run command.
            dockerfile_content: Custom Dockerfile content. If None, uses default.
        """
        super().__init__(additional_imports, logger)
        try:
            import docker
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "Please install 'docker' extra to use DockerExecutor: `pip install 'smolagents[docker]'`"
            )
        self.host = host
        self.port = port
        self.image_name = image_name

        self.dockerfile_content = dockerfile_content or dedent(
            """\
            FROM python:3.12-bullseye

            RUN pip install jupyter_kernel_gateway jupyter_client ipykernel

            EXPOSE 8888
            CMD ["jupyter", "kernelgateway", "--KernelGatewayApp.ip='0.0.0.0'", "--KernelGatewayApp.port=8888", "--KernelGatewayApp.allow_origin='*'"]
            """
        )

        # Initialize Docker
        try:
            self.client = docker.from_env()
        except docker.errors.DockerException as e:
            raise RuntimeError("Could not connect to Docker daemon: make sure Docker is running.") from e

        # Build and start container
        try:
            # Check if image exists, unless forced to rebuild
            if not build_new_image:
                try:
                    self.client.images.get(self.image_name)
                    self.logger.log(f"Using existing Docker image: {self.image_name}", level=LogLevel.INFO)
                except docker.errors.ImageNotFound:
                    self.logger.log(f"Image {self.image_name} not found, building...", level=LogLevel.INFO)
                    build_new_image = True

            if build_new_image:
                self.logger.log(f"Building Docker image {self.image_name}...", level=LogLevel.INFO)
                dockerfile_obj = BytesIO(self.dockerfile_content.encode("utf-8"))
                _, build_logs = self.client.images.build(fileobj=dockerfile_obj, tag=self.image_name)
                for log_chunk in build_logs:
                    # Only log non-empty messages
                    if log_message := log_chunk.get("stream", "").rstrip():
                        self.logger.log(log_message, level=LogLevel.DEBUG)

            self.logger.log(f"Starting container on {host}:{port}...", level=LogLevel.INFO)
            # Create base container parameters
            container_kwargs = {}
            if container_run_kwargs:
                container_kwargs.update(container_run_kwargs)

            # Ensure required port mapping and background running
            if not isinstance(container_kwargs.get("ports"), dict):
                container_kwargs["ports"] = {}
            container_kwargs["ports"]["8888/tcp"] = (host, port)
            container_kwargs["detach"] = True

            self.container = self.client.containers.run(self.image_name, **container_kwargs)

            retries = 0
            while self.container.status != "running" and retries < 5:
                self.logger.log(f"Container status: {self.container.status}, waiting...", level=LogLevel.INFO)
                time.sleep(1)
                self.container.reload()
                retries += 1

            self.base_url = f"http://{host}:{port}"

            # Wait for Jupyter to start
            self._wait_for_server()

            # Create new kernel via HTTP
            self.kernel_id = _create_kernel_http(f"{self.base_url}/api/kernels", self.logger)
            self.ws_url = f"ws://{host}:{port}/api/kernels/{self.kernel_id}/channels"

            self.installed_packages = self.install_packages(additional_imports)
            self.logger.log(
                f"Container {self.container.short_id} is running with kernel {self.kernel_id}", level=LogLevel.INFO
            )

        except Exception as e:
            self.cleanup()
            raise RuntimeError(f"Failed to initialize Jupyter kernel: {e}") from e

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        from websocket import create_connection

        with closing(create_connection(self.ws_url)) as ws:
            return _websocket_run_code_raise_errors(code, ws, self.logger)

    def cleanup(self):
        """Clean up the Docker container and resources."""
        try:
            if hasattr(self, "container"):
                self.logger.log(f"Stopping and removing container {self.container.short_id}...", level=LogLevel.INFO)
                self.container.stop()
                self.container.remove()
                self.logger.log("Container cleanup completed", level=LogLevel.INFO)
                del self.container
        except Exception as e:
            self.logger.log_error(f"Error during cleanup: {e}")

    def delete(self):
        """Ensure cleanup on deletion."""
        self.cleanup()

    def _wait_for_server(self):
        retries = 0
        jupyter_ready = False
        while not jupyter_ready and retries < 10:
            try:
                if requests.get(f"{self.base_url}/api/kernelspecs", timeout=2).status_code == 200:
                    jupyter_ready = True
                else:
                    self.logger.log("Jupyter not ready, waiting...", level=LogLevel.INFO)
            except requests.RequestException:
                self.logger.log("Jupyter not ready, waiting...", level=LogLevel.INFO)
            if not jupyter_ready:
                time.sleep(1)
                retries += 1


class ModalExecutor(RemotePythonExecutor):
    """
    Executes Python code using Modal.

    Args:
        additional_imports: Additional imports to install.
        logger (`Logger`): Logger to use for output and errors.
        app_name (`str`): App name.
        port (`int`): Port for jupyter to bind to.
        create_kwargs (`dict`, optional): Keyword arguments to pass to creating the sandbox. See
            `modal.Sandbox.create` [docs](https://modal.com/docs/reference/modal.Sandbox#create) for all the
            keyword arguments.
    """

    _ANSI_ESCAPE = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")

    def __init__(
        self,
        additional_imports: list[str],
        logger,
        app_name: str = "smolagent-executor",
        port: int = 8888,
        create_kwargs: Optional[dict] = None,
    ):
        super().__init__(additional_imports, logger)
        self.port = port
        try:
            import modal
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                """Please install 'modal' extra to use ModalExecutor: `pip install 'smolagents[modal]'`"""
            )

        if create_kwargs is None:
            create_kwargs = {}

        create_kwargs = {
            "image": modal.Image.debian_slim().uv_pip_install("jupyter_kernel_gateway", "ipykernel"),
            "timeout": 60 * 5,
            **create_kwargs,
        }

        if "app" not in create_kwargs:
            create_kwargs["app"] = modal.App.lookup(app_name, create_if_missing=True)

        if "encrypted_ports" not in create_kwargs:
            create_kwargs["encrypted_ports"] = [port]
        else:
            create_kwargs["encrypted_ports"] = create_kwargs["encrypted_ports"] + [port]

        token = secrets.token_urlsafe(16)
        default_secrets = [modal.Secret.from_dict({"KG_AUTH_TOKEN": token})]

        if "secrets" not in create_kwargs:
            create_kwargs["secrets"] = default_secrets
        else:
            create_kwargs["secrets"] = create_kwargs["secrets"] + default_secrets

        entrypoint = [
            "jupyter",
            "kernelgateway",
            "--KernelGatewayApp.ip='0.0.0.0'",
            f"--KernelGatewayApp.port={port}",
            "--KernelGatewayApp.allow_origin='*'",
        ]

        self.logger.log("Starting Modal sandbox", level=LogLevel.INFO)
        self.sandbox = modal.Sandbox.create(
            *entrypoint,
            **create_kwargs,
        )

        tunnel = self.sandbox.tunnels()[port]
        self.logger.log(f"Waiting for Modal sandbox on {tunnel.host}:{port}", level=LogLevel.INFO)
        self._wait_for_server(tunnel.host, token)

        self.logger.log("Starting Jupyter kernel", level=LogLevel.INFO)
        kernel_id = _create_kernel_http(f"https://{tunnel.host}/api/kernels?token={token}", logger)
        self.ws_url = f"wss://{tunnel.host}/api/kernels/{kernel_id}/channels?token={token}"
        self.installed_packages = self.install_packages(additional_imports)

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        from websocket import create_connection

        with closing(create_connection(self.ws_url)) as ws:
            return _websocket_run_code_raise_errors(code, ws, self.logger)

    def cleanup(self):
        if hasattr(self, "sandbox"):
            self.sandbox.terminate()

    def delete(self):
        """Ensure cleanup on deletion."""
        self.cleanup()

    def _wait_for_server(self, host: str, token: str):
        """Wait for server to start up."""
        n_retries = 0
        while True:
            try:
                resp = requests.get(f"https://{host}/api/kernelspecs?token={token}")
                if resp.status_code == 200:
                    break
            except RequestException:
                n_retries += 1
                if n_retries % 10 == 0:
                    self.logger.log("Waiting for server to startup, retrying...", level=LogLevel.INFO)
                if n_retries > 60:
                    raise RuntimeError("Unable to connect to sandbox")
                time.sleep(1.0)

    @classmethod
    def _strip_ansi_colors(cls, text: str) -> str:
        """Remove ansi colors from text."""
        return cls._ANSI_ESCAPE.sub("", text)


class BlaxelExecutor(RemotePythonExecutor):
    """
    Executes Python code using Blaxel sandboxes.

    Blaxel provides fast-launching virtual machines that start from hibernation in under 25ms
    and scale back to zero after inactivity while maintaining memory state.

    Args:
        additional_imports (`list[str]`): Additional Python packages to install.
        logger (`Logger`): Logger to use for output and errors.
        sandbox_name (`str`, optional): Name for the sandbox. Defaults to "smolagent-executor".
        image (`str`, optional): Docker image to use. Defaults to "blaxel/jupyter-notebook".
        memory (`int`, optional): Memory allocation in MB. Defaults to 4096.
        region (`str`, optional): Deployment region. If not specified, Blaxel chooses default.
        create_kwargs (`dict`, optional): Additional arguments for sandbox creation.
    """

    def __init__(
        self,
        additional_imports: list[str],
        logger,
        sandbox_name: str | None = None,
        image: str = "blaxel/jupyter-notebook",
        memory: int = 4096,
        ttl: str | None = None,
        region: Optional[str] = None,
    ):
        super().__init__(additional_imports, logger)

        try:
            import blaxel  # noqa: F401
        except ModuleNotFoundError:
            raise ModuleNotFoundError(
                "Please install 'blaxel' extra to use BlaxelExecutor: `pip install 'smolagents[blaxel]'`"
            )

        self.sandbox_name = sandbox_name or f"smolagent-executor-{uuid.uuid4().hex[:8]}"
        self.image = image
        self.memory = memory
        self.region = region
        self.port = 8888
        self._cleaned_up = False  # Flag to prevent double cleanup

        # Prepare sandbox creation parameters
        token = secrets.token_urlsafe(16)
        sandbox_config = {
            "metadata": {
                "name": self.sandbox_name,
            },
            "spec": {
                "runtime": {"image": image, "memory": memory, "ports": [{"target": self.port}]},
            },
        }

        if region:
            sandbox_config["spec"]["region"] = region

        if ttl:
            sandbox_config["spec"]["runtime"]["ttl"] = ttl

        # Create the sandbox
        try:
            # Create sandbox environment on Blaxel
            self.sandbox = BlaxelExecutor._create_sandbox(sandbox_config)

            # Create kernel via HTTP
            from blaxel.core import settings

            kernel_id = _create_kernel_http(
                f"{self.sandbox.metadata.url}/port/{self.port}/api/kernels?token={token}",
                self.logger,
                headers=settings.headers,
            )

            # Set up websocket URL
            # Convert http/https to ws/wss
            ws_scheme = "wss" if self.sandbox.metadata.url.startswith("https") else "ws"
            ws_base = self.sandbox.metadata.url.replace("https://", "").replace("http://", "")
            self.ws_url = f"{ws_scheme}://{ws_base}/port/{self.port}/api/kernels/{kernel_id}/channels?token={token}"

            # Install additional packages
            self.installed_packages = self.install_packages(additional_imports)
            self.logger.log("Blaxel is running", level=LogLevel.INFO)
        except Exception as e:
            self.cleanup()
            raise RuntimeError(f"Failed to initialize Blaxel sandbox: {e}") from e

    @staticmethod
    def _create_sandbox(config):
        """Helper method to create sandbox asynchronously."""
        from blaxel.core import SandboxInstance
        from blaxel.core.client import client
        from blaxel.core.client.api.compute import create_sandbox

        response = create_sandbox.sync(client=client, body=config)
        return SandboxInstance(response)

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        """
        Execute Python code in the Blaxel sandbox and return the result.

        Args:
            code (`str`): Python code to execute.

        Returns:
            `CodeOutput`: Code output containing the result, logs, and whether it is the final answer.
        """
        from blaxel.core import settings
        from websocket import create_connection

        headers = []
        for key, value in settings.headers.items():
            headers.append(f"{key}: {value}")
        with closing(create_connection(self.ws_url, header=headers)) as ws:
            return _websocket_run_code_raise_errors(code, ws, self.logger)

    def install_packages(self, additional_imports: list[str]) -> list[str]:
        """Helper method to install packages asynchronously."""
        if not additional_imports:
            return []

        from blaxel.core import settings
        from blaxel.core.sandbox.client import client
        from blaxel.core.sandbox.client.api.process import get_process_identifier, post_process
        from blaxel.core.sandbox.client.models import ErrorResponse, ProcessResponse

        try:
            client.with_base_url(self.sandbox.metadata.url)
            client.with_headers(settings.headers)

            # Install packages using pip via run_code
            self.logger.log(f"Installing packages: {', '.join(additional_imports)}", level=LogLevel.INFO)
            pip_install_code = f"pip install --root-user-action=ignore {' '.join(additional_imports)}"

            identifier = "install-packages"
            body = {
                "name": identifier,
                "command": pip_install_code,
            }
            post_process.sync(client=client, body=body)

            status = "running"
            interval = 1000
            max_wait = 600000
            start_time = time.time() * 1000
            logs = ""
            exit_code = 0

            while status == "running":
                if (time.time() * 1000) - start_time > max_wait:
                    raise Exception("Process did not finish in time")
                data = get_process_identifier.sync(identifier, client=client)
                if isinstance(data, ProcessResponse):
                    status = data.status or "running"
                    exit_code = data.exit_code
                    logs = data.logs
                elif isinstance(data, ErrorResponse):
                    raise Exception(f"Failed to install packages: {data.message}")
                else:
                    raise Exception(f"Unknown response: {data}")

                if status == "running":
                    time.sleep(interval / 1000)  # Convert to seconds

            if exit_code != 0:
                self.logger.log_error(f"Failed to install packages (exit code {exit_code}): {logs}")
                return []

            self.logger.log(f"Successfully installed packages: {', '.join(additional_imports)}", level=LogLevel.INFO)
            return additional_imports

        except Exception as e:
            self.logger.log_error(f"Error installing packages: {e}")
            return []

    def _delete_sandbox(self):
        """Delete sandbox using Blaxel's sync API and wait for completion."""
        from blaxel.core.client import client
        from blaxel.core.client.api.compute import delete_sandbox

        self.logger.log(f"Requesting sandbox {self.sandbox_name} deletion...", level=LogLevel.INFO)
        delete_sandbox.sync(client=client, sandbox_name=self.sandbox_name)

    def cleanup(self):
        """Sync wrapper to clean up sandbox and resources."""
        # Prevent double cleanup
        if self._cleaned_up:
            return
        self.logger.log("Shutting down sandbox...", level=LogLevel.INFO)
        self._cleaned_up = True
        try:
            self._delete_sandbox()
        except Exception as e:
            # Log cleanup errors but don't raise - cleanup should be best-effort
            self.logger.log(f"Error during cleanup: : {e}", level=LogLevel.INFO)
        finally:
            # Always clean up local references
            if hasattr(self, "sandbox"):
                del self.sandbox
            self.logger.log("Sandbox cleanup completed", level=LogLevel.INFO)

    def delete(self):
        """Ensure cleanup on deletion."""
        self.cleanup()

    def __del__(self):
        """Ensure cleanup on deletion."""
        try:
            self.cleanup()
        except Exception:
            pass  # Silently ignore errors during cleanup


class WasmExecutor(RemotePythonExecutor):
    """
    Remote Python code executor in a sandboxed WebAssembly environment powered by Pyodide and Deno.

    This executor combines Deno's secure runtime with Pyodide's WebAssemblyâ€‘compiled Python interpreter to deliver s
    trong isolation guarantees while enabling full Python execution.

    Args:
        additional_imports (`list[str]`): Additional Python packages to install in the Pyodide environment.
        logger (`Logger`): Logger to use for output and errors.
        deno_path (`str`, optional): Path to the Deno executable. If not provided, will use "deno" from PATH.
        deno_permissions (`list[str]`, optional): List of permissions to grant to the Deno runtime.
            Default is minimal permissions needed for execution.
        timeout (`int`, optional): Timeout in seconds for code execution. Default is 60 seconds.
    """

    def __init__(
        self,
        additional_imports: list[str],
        logger,
        deno_path: str = "deno",
        deno_permissions: list[str] | None = None,
        timeout: int = 60,
    ):
        super().__init__(additional_imports, logger)

        # Check if Deno is installed
        try:
            subprocess.run([deno_path, "--version"], capture_output=True, check=True)
        except (subprocess.SubprocessError, FileNotFoundError):
            raise RuntimeError(
                "Deno is not installed or not found in PATH. Please install Deno from https://deno.land/"
            )

        self.deno_path = deno_path
        self.timeout = timeout

        # Default minimal permissions needed
        if deno_permissions is None:
            # Use minimal permissions for Deno execution
            home_dir = os.getenv("HOME")
            deno_permissions = [
                "allow-net="
                + ",".join(
                    [
                        "0.0.0.0:8000",  # allow requests to the local server
                        "cdn.jsdelivr.net:443",  # allow loading pyodide packages
                        "pypi.org:443,files.pythonhosted.org:443",  # allow pyodide install packages from PyPI
                    ]
                ),
                f"allow-read={home_dir}/.cache/deno",
                f"allow-write={home_dir}/.cache/deno",
            ]
        self.deno_permissions = [f"--{perm}" for perm in deno_permissions]

        # Create the Deno JavaScript runner file
        self._create_deno_runner()

        # Install additional packages
        self.installed_packages = self.install_packages(additional_imports)
        self.logger.log("WasmExecutor is running", level=LogLevel.INFO)

    def _create_deno_runner(self):
        """Create the Deno JavaScript file that will run Pyodide and execute Python code."""
        self.runner_dir = tempfile.mkdtemp(prefix="pyodide_deno_")
        self.runner_path = os.path.join(self.runner_dir, "pyodide_runner.js")

        # Create the JavaScript runner file
        with open(self.runner_path, "w") as f:
            f.write(self.JS_CODE)

        # Start the Deno server
        self._start_deno_server()

    def _start_deno_server(self):
        """Start the Deno server that will run our JavaScript code."""
        cmd = [self.deno_path, "run"] + self.deno_permissions + [self.runner_path]

        # Start the server process
        self.server_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )

        # Wait for the server to start
        time.sleep(2)  # Give the server time to start

        # Check if the server started successfully
        if self.server_process.poll() is not None:
            stderr = self.server_process.stderr.read()
            raise RuntimeError(f"Failed to start Deno server: {stderr}")

        self.server_url = "http://localhost:8000"  # TODO: Another port?

        # Test the connection
        try:
            response = requests.get(self.server_url)
            if response.status_code != 200:
                raise RuntimeError(f"Server responded with status code {response.status_code}: {response.text}")
        except requests.RequestException as e:
            raise RuntimeError(f"Failed to connect to Deno server: {e}")

    def run_code_raise_errors(self, code: str) -> CodeOutput:
        """
        Execute Python code in the Pyodide environment and return the result.

        Args:
            code (`str`): Python code to execute.

        Returns:
            `CodeOutput`: Code output containing the result, logs, and whether it is the final answer.
        """
        try:
            # Prepare the request payload
            payload = {
                "code": code,
                "packages": self.installed_packages,
            }

            # Send the request to the Deno server
            response = requests.post(self.server_url, json=payload, timeout=self.timeout)

            if response.status_code != 200:
                raise AgentError(f"Server error: {response.text}", self.logger)

            result = None
            is_final_answer = False

            # Parse the response
            result_data = response.json()

            # Process the result
            if result_data.get("result"):
                result = result_data.get("result")
            # Check for execution errors
            elif result_data.get("error"):
                error = result_data["error"]
                if (
                    error.get("pythonExceptionType") == RemotePythonExecutor.FINAL_ANSWER_EXCEPTION
                    and "pythonExceptionValue" in error
                ):
                    result = pickle.loads(base64.b64decode(error["pythonExceptionValue"]))
                    is_final_answer = True
                else:
                    error_message = f"{error.get('name', 'Error')}: {error.get('message', 'Unknown error')}"
                    if "stack" in error:
                        error_message += f"\n{error['stack']}"
                    raise AgentError(error_message, self.logger)

            # Get the execution logs
            execution_logs = result_data.get("stdout", "")

            # Handle image results
            if isinstance(result, dict) and result.get("type") == "image":
                image_data = result.get("data", "")
                decoded_bytes = base64.b64decode(image_data.encode("utf-8"))
                return PIL.Image.open(BytesIO(decoded_bytes)), execution_logs

            return CodeOutput(output=result, logs=execution_logs, is_final_answer=is_final_answer)

        except requests.RequestException as e:
            raise AgentError(f"Failed to communicate with Deno server: {e}", self.logger)

    def install_packages(self, additional_imports: list[str]) -> list[str]:
        """
        Install additional Python packages in the Pyodide environment.

        Args:
            additional_imports (`list[str]`): Package names to install.

        Returns:
            list[str]: Installed packages.
        """
        # In Pyodide, we don't actually install packages here, but we keep track of them
        # to load them when executing code
        # TODO: Install  here instead?
        self.logger.log(f"Adding packages to load: {', '.join(additional_imports)}", level=LogLevel.INFO)
        return additional_imports

    def cleanup(self):
        """Clean up resources used by the executor."""
        if hasattr(self, "server_process") and self.server_process:
            self.logger.log("Stopping Deno server...", level=LogLevel.INFO)
            self.server_process.terminate()
            try:
                self.server_process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                self.server_process.kill()

        # Remove the temporary directory
        if hasattr(self, "runner_dir") and os.path.exists(self.runner_dir):
            import shutil

            shutil.rmtree(self.runner_dir)

    def delete(self):
        """Ensure cleanup on deletion."""
        self.cleanup()

    JS_CODE = dedent("""\
        // pyodide_runner.js - Runs Python code in Pyodide within Deno
        import { serve } from "https://deno.land/std/http/server.ts";
        import { loadPyodide } from "npm:pyodide";

        // Initialize Pyodide instance
        const pyodidePromise = loadPyodide();

        // Function to execute Python code and return the result
        async function executePythonCode(code) {
          const pyodide = await pyodidePromise;

          // Create a capture for stdout
          pyodide.runPython(`
            import sys
            import io
            sys.stdout = io.StringIO()
          `);

          // Execute the code and capture any errors
          let result = null;
          let error = null;
          let stdout = "";

          try {
            // Execute the code
            result = await pyodide.runPythonAsync(code);

            // Get captured stdout
            stdout = pyodide.runPython("sys.stdout.getvalue()");
          } catch (e) {
            error = {
              name: e.constructor.name,
              message: e.message,
              stack: e.stack
            };

            // Extract Python exception details
            if (e.constructor.name === "PythonError") {
              // Get the Python exception type from the error message: at the end of the traceback
              const errorMatch = e.message.match(/\\n([^:]+Exception): /);
              if (errorMatch) {
                error.pythonExceptionType = errorMatch[1].split(".").pop();
              }

              // If the error is a FinalAnswerException, extract its the encoded value
              if (error.pythonExceptionType === "FinalAnswerException") {
                // Extract the base64 encoded value from the error message
                const valueMatch = e.message.match(/FinalAnswerException: (.*?)(?:\\n|$)/);
                if (valueMatch) {
                  error.pythonExceptionValue = valueMatch[1];
                }
              }
            }
          }

          return {
            result,
            stdout,
            error
          };
        }

        // Start a simple HTTP server to receive code execution requests
        //const port = 8765;
        //console.log(`Starting Pyodide server on port ${port}`);

        serve(async (req) => {
          if (req.method === "POST") {
            try {
              const body = await req.json();
              const { code, packages = [] } = body;

              // Load any requested packages
              if (packages && packages.length > 0) {
                const pyodide = await pyodidePromise;
                //await pyodide.loadPackagesFromImports(code);
                await pyodide.loadPackage("micropip");
                const micropip = pyodide.pyimport("micropip");
                try {
                  await micropip.install(packages);
                } catch (e) {
                  console.error(`Failed to load package ${pkg}: ${e.message}`);
                }
              }

              const result = await executePythonCode(code);
              return new Response(JSON.stringify(result), {
                headers: { "Content-Type": "application/json" }
              });
            } catch (e) {
              return new Response(JSON.stringify({ error: e.message }), {
                status: 500,
                headers: { "Content-Type": "application/json" }
              });
            }
          }

          return new Response("Pyodide-Deno Executor is running. Send POST requests with code to execute.", {
            headers: { "Content-Type": "text/plain" }
          });
        });
        """)



================================================
FILE: src/smolagents/tool_validation.py
================================================
import ast
import builtins
from itertools import zip_longest

from .utils import BASE_BUILTIN_MODULES, get_source, is_valid_name


_BUILTIN_NAMES = set(vars(builtins))


class MethodChecker(ast.NodeVisitor):
    """
    Checks that a method
    - only uses defined names
    - contains no local imports (e.g. numpy is ok but local_script is not)
    """

    def __init__(self, class_attributes: set[str], check_imports: bool = True):
        self.undefined_names = set()
        self.imports = {}
        self.from_imports = {}
        self.assigned_names = set()
        self.arg_names = set()
        self.class_attributes = class_attributes
        self.errors = []
        self.check_imports = check_imports
        self.typing_names = {"Any"}
        self.defined_classes = set()

    def visit_arguments(self, node):
        """Collect function arguments"""
        self.arg_names = {arg.arg for arg in node.args}
        if node.kwarg:
            self.arg_names.add(node.kwarg.arg)
        if node.vararg:
            self.arg_names.add(node.vararg.arg)

    def visit_Import(self, node):
        for name in node.names:
            actual_name = name.asname or name.name
            self.imports[actual_name] = name.name

    def visit_ImportFrom(self, node):
        module = node.module or ""
        for name in node.names:
            actual_name = name.asname or name.name
            self.from_imports[actual_name] = (module, name.name)

    def visit_Assign(self, node):
        for target in node.targets:
            if isinstance(target, ast.Name):
                self.assigned_names.add(target.id)
            elif isinstance(target, (ast.Tuple, ast.List)):
                for elt in target.elts:
                    if isinstance(elt, ast.Name):
                        self.assigned_names.add(elt.id)
        self.visit(node.value)

    def visit_With(self, node):
        """Track aliases in 'with' statements (the 'y' in 'with X as y')"""
        for item in node.items:
            if item.optional_vars:  # This is the 'y' in 'with X as y'
                if isinstance(item.optional_vars, ast.Name):
                    self.assigned_names.add(item.optional_vars.id)
        self.generic_visit(node)

    def visit_ExceptHandler(self, node):
        """Track exception aliases (the 'e' in 'except Exception as e')"""
        if node.name:  # This is the 'e' in 'except Exception as e'
            self.assigned_names.add(node.name)
        self.generic_visit(node)

    def visit_AnnAssign(self, node):
        """Track annotated assignments."""
        if isinstance(node.target, ast.Name):
            self.assigned_names.add(node.target.id)
        if node.value:
            self.visit(node.value)

    def visit_For(self, node):
        target = node.target
        if isinstance(target, ast.Name):
            self.assigned_names.add(target.id)
        elif isinstance(target, ast.Tuple):
            for elt in target.elts:
                if isinstance(elt, ast.Name):
                    self.assigned_names.add(elt.id)
        self.generic_visit(node)

    def _handle_comprehension_generators(self, generators):
        """Helper method to handle generators in all types of comprehensions"""
        for generator in generators:
            if isinstance(generator.target, ast.Name):
                self.assigned_names.add(generator.target.id)
            elif isinstance(generator.target, ast.Tuple):
                for elt in generator.target.elts:
                    if isinstance(elt, ast.Name):
                        self.assigned_names.add(elt.id)

    def visit_ListComp(self, node):
        """Track variables in list comprehensions"""
        self._handle_comprehension_generators(node.generators)
        self.generic_visit(node)

    def visit_DictComp(self, node):
        """Track variables in dictionary comprehensions"""
        self._handle_comprehension_generators(node.generators)
        self.generic_visit(node)

    def visit_SetComp(self, node):
        """Track variables in set comprehensions"""
        self._handle_comprehension_generators(node.generators)
        self.generic_visit(node)

    def visit_Attribute(self, node):
        if not (isinstance(node.value, ast.Name) and node.value.id == "self"):
            self.generic_visit(node)

    def visit_ClassDef(self, node):
        """Track class definitions"""
        self.defined_classes.add(node.name)
        self.generic_visit(node)

    def visit_Name(self, node):
        if isinstance(node.ctx, ast.Load):
            if not (
                node.id in _BUILTIN_NAMES
                or node.id in BASE_BUILTIN_MODULES
                or node.id in self.arg_names
                or node.id == "self"
                or node.id in self.class_attributes
                or node.id in self.imports
                or node.id in self.from_imports
                or node.id in self.assigned_names
                or node.id in self.typing_names
                or node.id in self.defined_classes
            ):
                self.errors.append(f"Name '{node.id}' is undefined.")

    def visit_Call(self, node):
        if isinstance(node.func, ast.Name):
            if not (
                node.func.id in _BUILTIN_NAMES
                or node.func.id in BASE_BUILTIN_MODULES
                or node.func.id in self.arg_names
                or node.func.id == "self"
                or node.func.id in self.class_attributes
                or node.func.id in self.imports
                or node.func.id in self.from_imports
                or node.func.id in self.assigned_names
                or node.func.id in self.defined_classes
            ):
                self.errors.append(f"Name '{node.func.id}' is undefined.")
        self.generic_visit(node)


def validate_tool_attributes(cls, check_imports: bool = True) -> None:
    """
    Validates that a Tool class follows the proper patterns:
    0. Any argument of __init__ should have a default.
    Args chosen at init are not traceable, so we cannot rebuild the source code for them, thus any important arg should be defined as a class attribute.
    1. About the class:
        - Class attributes should only be strings or dicts
        - Class attributes cannot be complex attributes
    2. About all class methods:
        - Imports must be from packages, not local files
        - All methods must be self-contained

    Raises all errors encountered, if no error returns None.
    """

    class ClassLevelChecker(ast.NodeVisitor):
        def __init__(self):
            self.imported_names = set()
            self.complex_attributes = set()
            self.class_attributes = set()
            self.non_defaults = set()
            self.non_literal_defaults = set()
            self.in_method = False
            self.invalid_attributes = []

        def visit_FunctionDef(self, node):
            if node.name == "__init__":
                self._check_init_function_parameters(node)
            old_context = self.in_method
            self.in_method = True
            self.generic_visit(node)
            self.in_method = old_context

        def visit_Assign(self, node):
            if self.in_method:
                return
            # Track class attributes
            for target in node.targets:
                if isinstance(target, ast.Name):
                    self.class_attributes.add(target.id)

            # Check if the assignment is more complex than simple literals
            if not all(isinstance(val, (ast.Constant, ast.Dict, ast.List, ast.Set)) for val in ast.walk(node.value)):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        self.complex_attributes.add(target.id)

            # Check specific class attributes
            if getattr(node.targets[0], "id", "") == "name":
                if not isinstance(node.value, ast.Constant):
                    self.invalid_attributes.append(f"Class attribute 'name' must be a constant, found '{node.value}'")
                elif not isinstance(node.value.value, str):
                    self.invalid_attributes.append(
                        f"Class attribute 'name' must be a string, found '{node.value.value}'"
                    )
                elif not is_valid_name(node.value.value):
                    self.invalid_attributes.append(
                        f"Class attribute 'name' must be a valid Python identifier and not a reserved keyword, found '{node.value.value}'"
                    )

        def _check_init_function_parameters(self, node):
            # Check defaults in parameters
            for arg, default in reversed(list(zip_longest(reversed(node.args.args), reversed(node.args.defaults)))):
                if default is None:
                    if arg.arg != "self":
                        self.non_defaults.add(arg.arg)
                elif not isinstance(default, (ast.Constant, ast.Dict, ast.List, ast.Set)):
                    self.non_literal_defaults.add(arg.arg)

    class_level_checker = ClassLevelChecker()
    source = get_source(cls)
    tree = ast.parse(source)
    class_node = tree.body[0]
    if not isinstance(class_node, ast.ClassDef):
        raise ValueError("Source code must define a class")
    class_level_checker.visit(class_node)

    errors = []
    # Check invalid class attributes
    if class_level_checker.invalid_attributes:
        errors += class_level_checker.invalid_attributes
    if class_level_checker.complex_attributes:
        errors.append(
            f"Complex attributes should be defined in __init__, not as class attributes: "
            f"{', '.join(class_level_checker.complex_attributes)}"
        )
    if class_level_checker.non_defaults:
        errors.append(
            f"Parameters in __init__ must have default values, found required parameters: "
            f"{', '.join(class_level_checker.non_defaults)}"
        )
    if class_level_checker.non_literal_defaults:
        errors.append(
            f"Parameters in __init__ must have literal default values, found non-literal defaults: "
            f"{', '.join(class_level_checker.non_literal_defaults)}"
        )

    # Run checks on all methods
    for node in class_node.body:
        if isinstance(node, ast.FunctionDef):
            method_checker = MethodChecker(class_level_checker.class_attributes, check_imports=check_imports)
            method_checker.visit(node)
            errors += [f"- {node.name}: {error}" for error in method_checker.errors]

    if errors:
        raise ValueError(f"Tool validation failed for {cls.__name__}:\n" + "\n".join(errors))
    return



================================================
FILE: src/smolagents/utils.py
================================================
#!/usr/bin/env python
# coding=utf-8

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import ast
import base64
import importlib.util
import inspect
import json
import keyword
import os
import random
import re
import time
from functools import lru_cache
from io import BytesIO
from logging import Logger
from pathlib import Path
from textwrap import dedent
from typing import TYPE_CHECKING, Any, Callable

import jinja2


if TYPE_CHECKING:
    from smolagents.memory import AgentLogger


__all__ = ["AgentError"]


@lru_cache
def _is_package_available(package_name: str) -> bool:
    return importlib.util.find_spec(package_name) is not None


BASE_BUILTIN_MODULES = [
    "collections",
    "datetime",
    "itertools",
    "math",
    "queue",
    "random",
    "re",
    "stat",
    "statistics",
    "time",
    "unicodedata",
]


def escape_code_brackets(text: str) -> str:
    """Escapes square brackets in code segments while preserving Rich styling tags."""

    def replace_bracketed_content(match):
        content = match.group(1)
        cleaned = re.sub(
            r"bold|red|green|blue|yellow|magenta|cyan|white|black|italic|dim|\s|#[0-9a-fA-F]{6}", "", content
        )
        return f"\\[{content}\\]" if cleaned.strip() else f"[{content}]"

    return re.sub(r"\[([^\]]*)\]", replace_bracketed_content, text)


class AgentError(Exception):
    """Base class for other agent-related exceptions"""

    def __init__(self, message, logger: "AgentLogger"):
        super().__init__(message)
        self.message = message
        logger.log_error(message)

    def dict(self) -> dict[str, str]:
        return {"type": self.__class__.__name__, "message": str(self.message)}


class AgentParsingError(AgentError):
    """Exception raised for errors in parsing in the agent"""

    pass


class AgentExecutionError(AgentError):
    """Exception raised for errors in execution in the agent"""

    pass


class AgentMaxStepsError(AgentError):
    """Exception raised for errors in execution in the agent"""

    pass


class AgentToolCallError(AgentExecutionError):
    """Exception raised for errors when incorrect arguments are passed to the tool"""

    pass


class AgentToolExecutionError(AgentExecutionError):
    """Exception raised for errors when executing a tool"""

    pass


class AgentGenerationError(AgentError):
    """Exception raised for errors in generation in the agent"""

    pass


def make_json_serializable(obj: Any) -> Any:
    """Recursive function to make objects JSON serializable"""
    if obj is None:
        return None
    elif isinstance(obj, (str, int, float, bool)):
        # Try to parse string as JSON if it looks like a JSON object/array
        if isinstance(obj, str):
            try:
                if (obj.startswith("{") and obj.endswith("}")) or (obj.startswith("[") and obj.endswith("]")):
                    parsed = json.loads(obj)
                    return make_json_serializable(parsed)
            except json.JSONDecodeError:
                pass
        return obj
    elif isinstance(obj, (list, tuple)):
        return [make_json_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {str(k): make_json_serializable(v) for k, v in obj.items()}
    elif hasattr(obj, "__dict__"):
        # For custom objects, convert their __dict__ to a serializable format
        return {"_type": obj.__class__.__name__, **{k: make_json_serializable(v) for k, v in obj.__dict__.items()}}
    else:
        # For any other type, convert to string
        return str(obj)


def parse_json_blob(json_blob: str) -> tuple[dict[str, str], str]:
    "Extracts the JSON blob from the input and returns the JSON data and the rest of the input."
    try:
        first_accolade_index = json_blob.find("{")
        last_accolade_index = [a.start() for a in list(re.finditer("}", json_blob))][-1]
        json_str = json_blob[first_accolade_index : last_accolade_index + 1]
        json_data = json.loads(json_str, strict=False)
        return json_data, json_blob[:first_accolade_index]
    except IndexError:
        raise ValueError("The model output does not contain any JSON blob.")
    except json.JSONDecodeError as e:
        place = e.pos
        if json_blob[place - 1 : place + 2] == "},\n":
            raise ValueError(
                "JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL."
            )
        raise ValueError(
            f"The JSON blob you used is invalid due to the following error: {e}.\n"
            f"JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\n"
            f"'{json_blob[place - 4 : place + 5]}'."
        )


def extract_code_from_text(text: str, code_block_tags: tuple[str, str]) -> str | None:
    """Extract code from the LLM's output."""
    pattern = rf"{code_block_tags[0]}(.*?){code_block_tags[1]}"
    matches = re.findall(pattern, text, re.DOTALL)
    if matches:
        return "\n\n".join(match.strip() for match in matches)
    return None


def parse_code_blobs(text: str, code_block_tags: tuple[str, str]) -> str:
    """Extract code blocs from the LLM's output.

    If a valid code block is passed, it returns it directly.

    Args:
        text (`str`): LLM's output text to parse.

    Returns:
        `str`: Extracted code block.

    Raises:
        ValueError: If no valid code block is found in the text.
    """
    matches = extract_code_from_text(text, code_block_tags)
    if not matches:  # Fallback to markdown pattern
        matches = extract_code_from_text(text, ("```(?:python|py)", "\n```"))
    if matches:
        return matches
    # Maybe the LLM outputted a code blob directly
    try:
        ast.parse(text)
        return text
    except SyntaxError:
        pass

    if "final" in text and "answer" in text:
        raise ValueError(
            dedent(
                f"""
                Your code snippet is invalid, because the regex pattern {code_block_tags[0]}(.*?){code_block_tags[1]} was not found in it.
                Here is your code snippet:
                {text}
                It seems like you're trying to return the final answer, you can do it as follows:
                {code_block_tags[0]}
                final_answer("YOUR FINAL ANSWER HERE")
                {code_block_tags[1]}
                """
            ).strip()
        )
    raise ValueError(
        dedent(
            f"""
            Your code snippet is invalid, because the regex pattern {code_block_tags[0]}(.*?){code_block_tags[1]} was not found in it.
            Here is your code snippet:
            {text}
            Make sure to include code with the correct pattern, for instance:
            Thoughts: Your thoughts
            {code_block_tags[0]}
            # Your python code here
            {code_block_tags[1]}
            """
        ).strip()
    )


MAX_LENGTH_TRUNCATE_CONTENT = 20000


def truncate_content(content: str, max_length: int = MAX_LENGTH_TRUNCATE_CONTENT) -> str:
    if len(content) <= max_length:
        return content
    else:
        return (
            content[: max_length // 2]
            + f"\n..._This content has been truncated to stay below {max_length} characters_...\n"
            + content[-max_length // 2 :]
        )


class ImportFinder(ast.NodeVisitor):
    def __init__(self):
        self.packages = set()

    def visit_Import(self, node):
        for alias in node.names:
            # Get the base package name (before any dots)
            base_package = alias.name.split(".")[0]
            self.packages.add(base_package)

    def visit_ImportFrom(self, node):
        if node.module:  # for "from x import y" statements
            # Get the base package name (before any dots)
            base_package = node.module.split(".")[0]
            self.packages.add(base_package)


def instance_to_source(instance, base_cls=None):
    """Convert an instance to its class source code representation."""
    cls = instance.__class__
    class_name = cls.__name__

    # Start building class lines
    class_lines = []
    if base_cls:
        class_lines.append(f"class {class_name}({base_cls.__name__}):")
    else:
        class_lines.append(f"class {class_name}:")

    # Add docstring if it exists and differs from base
    if cls.__doc__ and (not base_cls or cls.__doc__ != base_cls.__doc__):
        class_lines.append(f'    """{cls.__doc__}"""')

    # Add class-level attributes
    class_attrs = {
        name: value
        for name, value in cls.__dict__.items()
        if not name.startswith("__")
        and not name == "_abc_impl"
        and not callable(value)
        and not (base_cls and hasattr(base_cls, name) and getattr(base_cls, name) == value)
    }

    for name, value in class_attrs.items():
        if isinstance(value, str):
            # multiline value
            if "\n" in value:
                escaped_value = value.replace('"""', r"\"\"\"")  # Escape triple quotes
                class_lines.append(f'    {name} = """{escaped_value}"""')
            else:
                class_lines.append(f"    {name} = {json.dumps(value)}")
        else:
            class_lines.append(f"    {name} = {repr(value)}")

    if class_attrs:
        class_lines.append("")

    # Add methods
    methods = {
        name: func.__wrapped__ if hasattr(func, "__wrapped__") else func
        for name, func in cls.__dict__.items()
        if callable(func)
        and (
            not base_cls
            or not hasattr(base_cls, name)
            or (
                isinstance(func, (staticmethod, classmethod))
                or (getattr(base_cls, name).__code__.co_code != func.__code__.co_code)
            )
        )
    }

    for name, method in methods.items():
        method_source = get_source(method)
        # Clean up the indentation
        method_lines = method_source.split("\n")
        first_line = method_lines[0]
        indent = len(first_line) - len(first_line.lstrip())
        method_lines = [line[indent:] for line in method_lines]
        method_source = "\n".join(["    " + line if line.strip() else line for line in method_lines])
        class_lines.append(method_source)
        class_lines.append("")

    # Find required imports using ImportFinder
    import_finder = ImportFinder()
    import_finder.visit(ast.parse("\n".join(class_lines)))
    required_imports = import_finder.packages

    # Build final code with imports
    final_lines = []

    # Add base class import if needed
    if base_cls:
        final_lines.append(f"from {base_cls.__module__} import {base_cls.__name__}")

    # Add discovered imports
    for package in required_imports:
        final_lines.append(f"import {package}")

    if final_lines:  # Add empty line after imports
        final_lines.append("")

    # Add the class code
    final_lines.extend(class_lines)

    return "\n".join(final_lines)


def get_source(obj) -> str:
    """Get the source code of a class or callable object (e.g.: function, method).
    First attempts to get the source code using `inspect.getsource`.
    In a dynamic environment (e.g.: Jupyter, IPython), if this fails,
    falls back to retrieving the source code from the current interactive shell session.

    Args:
        obj: A class or callable object (e.g.: function, method)

    Returns:
        str: The source code of the object, dedented and stripped

    Raises:
        TypeError: If object is not a class or callable
        OSError: If source code cannot be retrieved from any source
        ValueError: If source cannot be found in IPython history

    Note:
        TODO: handle Python standard REPL
    """
    if not (isinstance(obj, type) or callable(obj)):
        raise TypeError(f"Expected class or callable, got {type(obj)}")

    inspect_error = None
    try:
        # Handle dynamically created classes
        source = getattr(obj, "__source__", None) or inspect.getsource(obj)
        return dedent(source).strip()
    except OSError as e:
        # let's keep track of the exception to raise it if all further methods fail
        inspect_error = e
    try:
        import IPython

        shell = IPython.get_ipython()
        if not shell:
            raise ImportError("No active IPython shell found")
        all_cells = "\n".join(shell.user_ns.get("In", [])).strip()
        if not all_cells:
            raise ValueError("No code cells found in IPython session")

        tree = ast.parse(all_cells)
        for node in ast.walk(tree):
            if isinstance(node, (ast.ClassDef, ast.FunctionDef)) and node.name == obj.__name__:
                return dedent("\n".join(all_cells.split("\n")[node.lineno - 1 : node.end_lineno])).strip()
        raise ValueError(f"Could not find source code for {obj.__name__} in IPython history")
    except ImportError:
        # IPython is not available, let's just raise the original inspect error
        raise inspect_error
    except ValueError as e:
        # IPython is available but we couldn't find the source code, let's raise the error
        raise e from inspect_error


def encode_image_base64(image):
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")


def make_image_url(base64_image):
    return f"data:image/png;base64,{base64_image}"


def make_init_file(folder: str | Path):
    os.makedirs(folder, exist_ok=True)
    # Create __init__
    with open(os.path.join(folder, "__init__.py"), "w"):
        pass


def is_valid_name(name: str) -> bool:
    return name.isidentifier() and not keyword.iskeyword(name) if isinstance(name, str) else False


AGENT_GRADIO_APP_TEMPLATE = """import yaml
import os
from smolagents import GradioUI, {{ class_name }}, {{ agent_dict['model']['class'] }}

# Get current directory path
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))

{% for tool in tools.values() -%}
from {{managed_agent_relative_path}}tools.{{ tool.name }} import {{ tool.__class__.__name__ }} as {{ tool.name | camelcase }}
{% endfor %}
{% for managed_agent in managed_agents.values() -%}
from {{managed_agent_relative_path}}managed_agents.{{ managed_agent.name }}.app import agent_{{ managed_agent.name }}
{% endfor %}

model = {{ agent_dict['model']['class'] }}(
{% for key in agent_dict['model']['data'] if key != 'class' -%}
    {{ key }}={{ agent_dict['model']['data'][key]|repr }},
{% endfor %})

{% for tool in tools.values() -%}
{{ tool.name }} = {{ tool.name | camelcase }}()
{% endfor %}

with open(os.path.join(CURRENT_DIR, "prompts.yaml"), 'r') as stream:
    prompt_templates = yaml.safe_load(stream)

{{ agent_name }} = {{ class_name }}(
    model=model,
    tools=[{% for tool_name in tools.keys() if tool_name != "final_answer" %}{{ tool_name }}{% if not loop.last %}, {% endif %}{% endfor %}],
    managed_agents=[{% for subagent_name in managed_agents.keys() %}agent_{{ subagent_name }}{% if not loop.last %}, {% endif %}{% endfor %}],
    {% for attribute_name, value in agent_dict.items() if attribute_name not in ["class", "model", "tools", "prompt_templates", "authorized_imports", "managed_agents", "requirements"] -%}
    {{ attribute_name }}={{ value|repr }},
    {% endfor %}prompt_templates=prompt_templates
)
if __name__ == "__main__":
    GradioUI({{ agent_name }}).launch()
""".strip()


def create_agent_gradio_app_template():
    env = jinja2.Environment(loader=jinja2.BaseLoader(), undefined=jinja2.StrictUndefined)
    env.filters["repr"] = repr
    env.filters["camelcase"] = lambda value: "".join(word.capitalize() for word in value.split("_"))
    return env.from_string(AGENT_GRADIO_APP_TEMPLATE)


class RateLimiter:
    """Simple rate limiter that enforces a minimum delay between consecutive requests.

    This class is useful for limiting the rate of operations such as API requests,
    by ensuring that calls to `throttle()` are spaced out by at least a given interval
    based on the desired requests per minute.

    If no rate is specified (i.e., `requests_per_minute` is None), rate limiting
    is disabled and `throttle()` becomes a no-op.

    Args:
        requests_per_minute (`float | None`): Maximum number of allowed requests per minute.
            Use `None` to disable rate limiting.
    """

    def __init__(self, requests_per_minute: float | None = None):
        self._enabled = requests_per_minute is not None
        self._interval = 60.0 / requests_per_minute if self._enabled else 0.0
        self._last_call = 0.0

    def throttle(self):
        """Pause execution to respect the rate limit, if enabled."""
        if not self._enabled:
            return
        now = time.time()
        elapsed = now - self._last_call
        if elapsed < self._interval:
            time.sleep(self._interval - elapsed)
        self._last_call = time.time()


class Retrying:
    """Simple retrying controller. Inspired from library [tenacity](https://github.com/jd/tenacity/)."""

    def __init__(
        self,
        max_attempts: int = 1,
        wait_seconds: float = 0.0,
        exponential_base: float = 2.0,
        jitter: bool = True,
        retry_predicate: Callable[[BaseException], bool] | None = None,
        reraise: bool = False,
        before_sleep_logger: tuple[Logger, int] | None = None,
        after_logger: tuple[Logger, int] | None = None,
    ):
        self.max_attempts = max_attempts
        self.wait_seconds = wait_seconds
        self.exponential_base = exponential_base
        self.jitter = jitter
        self.retry_predicate = retry_predicate
        self.reraise = reraise
        self.before_sleep_logger = before_sleep_logger
        self.after_logger = after_logger

    def __call__(self, fn, *args: Any, **kwargs: Any) -> Any:
        start_time = time.time()
        delay = self.wait_seconds

        for attempt_number in range(1, self.max_attempts + 1):
            try:
                result = fn(*args, **kwargs)

                # Log after successful call if we had retries
                if self.after_logger and attempt_number > 1:
                    logger, log_level = self.after_logger
                    seconds = time.time() - start_time
                    fn_name = getattr(fn, "__name__", repr(fn))
                    logger.log(
                        log_level,
                        f"Finished call to '{fn_name}' after {seconds:.3f}(s), this was attempt nÂ°{attempt_number}/{self.max_attempts}.",
                    )

                return result

            except BaseException as e:
                # Check if we should retry
                should_retry = self.retry_predicate(e) if self.retry_predicate else False

                # If this is the last attempt or we shouldn't retry, raise
                if not should_retry or attempt_number >= self.max_attempts:
                    if self.reraise:
                        raise
                    raise

                # Log after failed attempt
                if self.after_logger:
                    logger, log_level = self.after_logger
                    seconds = time.time() - start_time
                    fn_name = getattr(fn, "__name__", repr(fn))
                    logger.log(
                        log_level,
                        f"Finished call to '{fn_name}' after {seconds:.3f}(s), this was attempt nÂ°{attempt_number}/{self.max_attempts}.",
                    )

                # Exponential backoff with jitter
                # https://cookbook.openai.com/examples/how_to_handle_rate_limits#example-3-manual-backoff-implementation
                delay *= self.exponential_base * (1 + self.jitter * random.random())

                # Log before sleeping
                if self.before_sleep_logger:
                    logger, log_level = self.before_sleep_logger
                    fn_name = getattr(fn, "__name__", repr(fn))
                    logger.log(
                        log_level,
                        f"Retrying {fn_name} in {delay} seconds as it raised {e.__class__.__name__}: {e}.",
                    )

                # Sleep before next attempt
                if delay > 0:
                    time.sleep(delay)



================================================
FILE: src/smolagents/vision_web_browser.py
================================================
import argparse
from io import BytesIO
from time import sleep

import helium
import PIL.Image
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from smolagents import CodeAgent, WebSearchTool, tool
from smolagents.agents import ActionStep
from smolagents.cli import load_model


github_request = """
I'm trying to find how hard I have to work to get a repo in github.com/trending.
Can you navigate to the profile for the top author of the top trending repo, and give me their total number of commits over the last year?
"""  # The agent is able to achieve this request only when powered by GPT-4o or Claude-3.5-sonnet.

search_request = """
Please navigate to https://en.wikipedia.org/wiki/Chicago and give me a sentence containing the word "1992" that mentions a construction accident.
"""


def parse_arguments():
    parser = argparse.ArgumentParser(description="Run a web browser automation script with a specified model.")
    parser.add_argument(
        "prompt",
        type=str,
        nargs="?",  # Makes it optional
        default=search_request,
        help="The prompt to run with the agent",
    )
    parser.add_argument(
        "--model-type",
        type=str,
        default="LiteLLMModel",
        help="The model type to use (e.g., OpenAIModel, LiteLLMModel, TransformersModel, InferenceClientModel)",
    )
    parser.add_argument(
        "--model-id",
        type=str,
        default="gpt-4o",
        help="The model ID to use for the specified model type",
    )
    parser.add_argument(
        "--provider",
        type=str,
        help="The inference provider to use for the model",
    )
    parser.add_argument(
        "--api-base",
        type=str,
        help="The API base to use for the model",
    )
    parser.add_argument(
        "--api-key",
        type=str,
        help="The API key to use for the model",
    )
    return parser.parse_args()


def save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    sleep(1.0)  # Let JavaScript animations happen before taking the screenshot
    driver = helium.get_driver()
    current_step = memory_step.step_number
    if driver is not None:
        for previous_memory_step in agent.memory.steps:  # Remove previous screenshots from logs for lean processing
            if isinstance(previous_memory_step, ActionStep) and previous_memory_step.step_number <= current_step - 2:
                previous_memory_step.observations_images = None
        png_bytes = driver.get_screenshot_as_png()
        image = PIL.Image.open(BytesIO(png_bytes))
        print(f"Captured a browser screenshot: {image.size} pixels")
        memory_step.observations_images = [image.copy()]  # Create a copy to ensure it persists, important!

    # Update observations with current URL
    url_info = f"Current url: {driver.current_url}"
    memory_step.observations = (
        url_info if memory_step.observations is None else memory_step.observations + "\n" + url_info
    )
    return


def _escape_xpath_string(s: str) -> str:
    """
    Escapes a string for safe use in an XPath expression.

    Args:
        s (`str`): Arbitrary input string to escape.

    Returns:
        `str`: Valid XPath expression representing the literal value of `s`.
    """
    if "'" not in s:
        return f"'{s}'"
    if '"' not in s:
        return f'"{s}"'
    parts = s.split("'")
    return "concat(" + ', "\'", '.join(f"'{p}'" for p in parts) + ")"


@tool
def search_item_ctrl_f(text: str, nth_result: int = 1) -> str:
    """
    Searches for text on the current page via Ctrl + F and jumps to the nth occurrence.
    Args:
        text: The text to search for
        nth_result: Which occurrence to jump to (default: 1)
    """
    escaped_text = _escape_xpath_string(text)
    elements = driver.find_elements(By.XPATH, f"//*[contains(text(), {escaped_text})]")
    if nth_result > len(elements):
        raise Exception(f"Match nÂ°{nth_result} not found (only {len(elements)} matches found)")
    result = f"Found {len(elements)} matches for '{text}'."
    elem = elements[nth_result - 1]
    driver.execute_script("arguments[0].scrollIntoView(true);", elem)
    result += f"Focused on element {nth_result} of {len(elements)}"
    return result


@tool
def go_back() -> None:
    """Goes back to previous page."""
    driver.back()


@tool
def close_popups() -> str:
    """
    Closes any visible modal or pop-up on the page. Use this to dismiss pop-up windows! This does not work on cookie consent banners.
    """
    webdriver.ActionChains(driver).send_keys(Keys.ESCAPE).perform()


def initialize_driver():
    """Initialize the Selenium WebDriver."""
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--force-device-scale-factor=1")
    chrome_options.add_argument("--window-size=1000,1350")
    chrome_options.add_argument("--disable-pdf-viewer")
    chrome_options.add_argument("--window-position=0,0")
    return helium.start_chrome(headless=False, options=chrome_options)


def initialize_agent(model):
    """Initialize the CodeAgent with the specified model."""
    return CodeAgent(
        tools=[WebSearchTool(), go_back, close_popups, search_item_ctrl_f],
        model=model,
        additional_authorized_imports=["helium"],
        step_callbacks=[save_screenshot],
        max_steps=20,
        verbosity_level=2,
    )


helium_instructions = """
Use your web_search tool when you want to get Google search results.
Then you can use helium to access websites. Don't use helium for Google search, only for navigating websites!
Don't bother about the helium driver, it's already managed.
We've already ran "from helium import *"
Then you can go to pages!
<code>
go_to('github.com/trending')
</code>

You can directly click clickable elements by inputting the text that appears on them.
<code>
click("Top products")
</code>

If it's a link:
<code>
click(Link("Top products"))
</code>

If you try to interact with an element and it's not found, you'll get a LookupError.
In general stop your action after each button click to see what happens on your screenshot.
Never try to login in a page.

To scroll up or down, use scroll_down or scroll_up with as an argument the number of pixels to scroll from.
<code>
scroll_down(num_pixels=1200) # This will scroll one viewport down
</code>

When you have pop-ups with a cross icon to close, don't try to click the close icon by finding its element or targeting an 'X' element (this most often fails).
Just use your built-in tool `close_popups` to close them:
<code>
close_popups()
</code>

You can use .exists() to check for the existence of an element. For example:
<code>
if Text('Accept cookies?').exists():
    click('I accept')
</code>

Proceed in several steps rather than trying to solve the task in one shot.
And at the end, only when you have your answer, return your final answer.
<code>
final_answer("YOUR_ANSWER_HERE")
</code>

If pages seem stuck on loading, you might have to wait, for instance `import time` and run `time.sleep(5.0)`. But don't overuse this!
To list elements on page, DO NOT try code-based element searches like 'contributors = find_all(S("ol > li"))': just look at the latest screenshot you have and read it visually, or use your tool search_item_ctrl_f.
Of course, you can act on buttons like a user would do when navigating.
After each code blob you write, you will be automatically provided with an updated screenshot of the browser and the current browser url.
But beware that the screenshot will only be taken at the end of the whole action, it won't see intermediate states.
Don't kill the browser.
When you have modals or cookie banners on screen, you should get rid of them before you can click anything else.
"""


def run_webagent(
    prompt: str,
    model_type: str,
    model_id: str,
    provider: str | None = None,
    api_base: str | None = None,
    api_key: str | None = None,
) -> None:
    # Load environment variables
    load_dotenv()

    # Initialize the model based on the provided arguments
    model = load_model(model_type, model_id, provider=provider, api_base=api_base, api_key=api_key)

    global driver
    driver = initialize_driver()
    agent = initialize_agent(model)

    # Run the agent with the provided prompt
    agent.python_executor("from helium import *")
    agent.run(prompt + helium_instructions)


def main() -> None:
    # Parse command line arguments
    args = parse_arguments()
    run_webagent(args.prompt, args.model_type, args.model_id, args.provider, args.api_base, args.api_key)


if __name__ == "__main__":
    main()



================================================
FILE: src/smolagents/prompts/code_agent.yaml
================================================
system_prompt: |-
  You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
  To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
  To solve the task, you must plan forward to proceed in a series of steps, in a cycle of Thought, Code, and Observation sequences.

  At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.
  Then in the Code sequence you should write the code in simple Python. The code sequence must be opened with '{{code_block_opening_tag}}', and closed with '{{code_block_closing_tag}}'.
  During each intermediate step, you can use 'print()' to save whatever important information you will then need.
  These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
  In the end you have to return a final answer using the `final_answer` tool.

  Here are a few examples using notional tools:
  ---
  Task: "Generate an image of the oldest person in this document."

  Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.
  {{code_block_opening_tag}}
  answer = document_qa(document=document, question="Who is the oldest person mentioned?")
  print(answer)
  {{code_block_closing_tag}}
  Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

  Thought: I will now generate an image showcasing the oldest person.
  {{code_block_opening_tag}}
  image = image_generator("A portrait of John Doe, a 55-year-old man living in Canada.")
  final_answer(image)
  {{code_block_closing_tag}}

  ---
  Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

  Thought: I will use Python code to compute the result of the operation and then return the final answer using the `final_answer` tool.
  {{code_block_opening_tag}}
  result = 5 + 3 + 1294.678
  final_answer(result)
  {{code_block_closing_tag}}

  ---
  Task:
  "Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.
  You have been provided with these additional arguments, that you can access using the keys as variables in your Python code:
  {'question': 'Quel est l'animal sur l'image?', 'image': 'path/to/image.jpg'}"

  Thought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.
  {{code_block_opening_tag}}
  translated_question = translator(question=question, src_lang="French", tgt_lang="English")
  print(f"The translated question is {translated_question}.")
  answer = image_qa(image=image, question=translated_question)
  final_answer(f"The answer is {answer}")
  {{code_block_closing_tag}}

  ---
  Task:
  In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.
  What does he say was the consequence of Einstein learning too much math on his creativity, in one word?

  Thought: I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.
  {{code_block_opening_tag}}
  pages = web_search(query="1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein")
  print(pages)
  {{code_block_closing_tag}}
  Observation:
  No result found for query "1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein".

  Thought: The query was maybe too restrictive and did not find any results. Let's try again with a broader query.
  {{code_block_opening_tag}}
  pages = web_search(query="1979 interview Stanislaus Ulam")
  print(pages)
  {{code_block_closing_tag}}
  Observation:
  Found 6 pages:
  [Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)

  [Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)

  (truncated)

  Thought: I will read the first 2 pages to know more.
  {{code_block_opening_tag}}
  for url in ["https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/", "https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/"]:
      whole_page = visit_webpage(url)
      print(whole_page)
      print("\n" + "="*80 + "\n")  # Print separator between pages
  {{code_block_closing_tag}}
  Observation:
  Manhattan Project Locations:
  Los Alamos, NM
  Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at
  (truncated)

  Thought: I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: "He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity." Let's answer in one word.
  {{code_block_opening_tag}}
  final_answer("diminished")
  {{code_block_closing_tag}}

  ---
  Task: "Which city has the highest population: Guangzhou or Shanghai?"

  Thought: I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.
  {{code_block_opening_tag}}
  for city in ["Guangzhou", "Shanghai"]:
      print(f"Population {city}:", web_search(f"{city} population"))
  {{code_block_closing_tag}}
  Observation:
  Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
  Population Shanghai: '26 million (2019)'

  Thought: Now I know that Shanghai has the highest population.
  {{code_block_opening_tag}}
  final_answer("Shanghai")
  {{code_block_closing_tag}}

  ---
  Task: "What is the current age of the pope, raised to the power 0.36?"

  Thought: I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.
  {{code_block_opening_tag}}
  pope_age_wiki = wikipedia_search(query="current pope age")
  print("Pope age as per wikipedia:", pope_age_wiki)
  pope_age_search = web_search(query="current pope age")
  print("Pope age as per google search:", pope_age_search)
  {{code_block_closing_tag}}
  Observation:
  Pope age: "The pope Francis is currently 88 years old."

  Thought: I know that the pope is 88 years old. Let's compute the result using Python code.
  {{code_block_opening_tag}}
  pope_current_age = 88 ** 0.36
  final_answer(pope_current_age)
  {{code_block_closing_tag}}

  Above examples were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:
  {{code_block_opening_tag}}
  {%- for tool in tools.values() %}
  {{ tool.to_code_prompt() }}
  {% endfor %}
  {{code_block_closing_tag}}

  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  {{code_block_opening_tag}}
  {%- for agent in managed_agents.values() %}
  def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
      """{{ agent.description }}

      Args:
          task: Long detailed description of the task.
          additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
      """
  {% endfor %}
  {{code_block_closing_tag}}
  {%- endif %}

  Here are the rules you should always follow to solve your task:
  1. Always provide a 'Thought:' sequence, and a '{{code_block_opening_tag}}' sequence ending with '{{code_block_closing_tag}}', else you will fail.
  2. Use only variables that you have defined!
  3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = wikipedia_search(query="What is the place where James Bond lives?")'.
  4. For tools WITHOUT JSON output schema: Take care to not chain too many sequential tool calls in the same code block, as their output format is unpredictable. For instance, a call to wikipedia_search without a JSON output schema has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.
  5. For tools WITH JSON output schema: You can confidently chain multiple tool calls and directly access structured output fields in the same code block! When a tool has a JSON output schema, you know exactly what fields and data types to expect, allowing you to write robust code that directly accesses the structured response (e.g., result['field_name']) without needing intermediate print() statements.
  6. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.
  7. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.
  8. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.
  9. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}
  10. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.
  11. Don't give up! You're in charge of solving the task, not providing directions to solve it.

  {%- if custom_instructions %}
  {{custom_instructions}}
  {%- endif %}

  Now Begin!
planning:
  initial_plan : |-
    You are a world expert at analyzing a situation to derive facts, and plan accordingly towards solving a task.
    Below I will present you a task. You will need to 1. build a survey of facts known or needed to solve the task, then 2. make a plan of action to solve the task.

    ## 1. Facts survey
    You will build a comprehensive preparatory survey of which facts we have at our disposal and which ones we still need.
    These "facts" will typically be specific names, dates, values, etc. Your answer should use the below headings:
    ### 1.1. Facts given in the task
    List here the specific facts given in the task that could help you (there might be nothing here).

    ### 1.2. Facts to look up
    List here any facts that we may need to look up.
    Also list where to find each of these, for instance a website, a file... - maybe the task contains some sources that you should re-use here.

    ### 1.3. Facts to derive
    List here anything that we want to derive from the above by logical reasoning, for instance computation or simulation.

    Don't make any assumptions. For each item, provide a thorough reasoning. Do not add anything else on top of three headings above.

    ## 2. Plan
    Then for the given task, develop a step-by-step high-level plan taking into account the above inputs and list of facts.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools, behaving like regular python functions:
    ```python
    {%- for tool in tools.values() %}
    {{ tool.to_code_prompt() }}
    {% endfor %}
    ```

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    ```python
    {%- for agent in managed_agents.values() %}
    def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
        """{{ agent.description }}

        Args:
            task: Long detailed description of the task.
            additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
        """
    {% endfor %}
    ```
    {%- endif %}

    ---
    Now begin! Here is your task:
    ```
    {{task}}
    ```
    First in part 1, write the facts survey, then in part 2, write your plan.
  update_plan_pre_messages: |-
    You are a world expert at analyzing a situation, and plan accordingly towards solving a task.
    You have been given the following task:
    ```
    {{task}}
    ```

    Below you will find a history of attempts made to solve this task.
    You will first have to produce a survey of known and unknown facts, then propose a step-by-step high-level plan to solve the task.
    If the previous tries so far have met some success, your updated plan can build on these results.
    If you are stalled, you can make a completely new plan starting from scratch.

    Find the task and history below:
  update_plan_post_messages: |-
    Now write your updated facts below, taking into account the above history:
    ## 1. Updated facts survey
    ### 1.1. Facts given in the task
    ### 1.2. Facts that we have learned
    ### 1.3. Facts still to look up
    ### 1.4. Facts still to derive

    Then write a step-by-step high-level plan to solve the task above.
    ## 2. Plan
    ### 2. 1. ...
    Etc.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Beware that you have {remaining_steps} steps remaining.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools, behaving like regular python functions:
    ```python
    {%- for tool in tools.values() %}
    {{ tool.to_code_prompt() }}
    {% endfor %}
    ```

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    ```python
    {%- for agent in managed_agents.values() %}
    def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
        """{{ agent.description }}

        Args:
            task: Long detailed description of the task.
            additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
        """
    {% endfor %}
    ```
    {%- endif %}

    Now write your updated facts survey below, then your new plan.
managed_agent:
  task: |-
      You're a helpful agent named '{{name}}'.
      You have been submitted this task by your manager.
      ---
      Task:
      {{task}}
      ---
      You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.

      Your final_answer WILL HAVE to contain these parts:
      ### 1. Task outcome (short version):
      ### 2. Task outcome (extremely detailed version):
      ### 3. Additional context (if relevant):

      Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.
      And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.
  report: |-
      Here is the final answer from your managed agent '{{name}}':
      {{final_answer}}
final_answer:
  pre_messages: |-
    An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:
  post_messages: |-
    Based on the above, please provide an answer to the following user task:
    {{task}}



================================================
FILE: src/smolagents/prompts/structured_code_agent.yaml
================================================
system_prompt: |-
  You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.
  To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.
  To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.

  At each step, in the 'Thought:' attribute, you should first explain your reasoning towards solving the task and the tools that you want to use.
  Then in the 'Code' attribute, you should write the code in simple Python.
  During each intermediate step, you can use 'print()' to save whatever important information you will then need.
  These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.
  In the end you have to return a final answer using the `final_answer` tool. You will be generating a JSON object with the following structure:
  ```json
  {
    "thought": "...",
    "code": "..."
  }
  ```

  Here are a few examples using notional tools:
  ---
  Task: "Generate an image of the oldest person in this document."

  {"thought": "I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.", "code": "answer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\nprint(answer)\n"}
  Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

  {"thought": "I will now generate an image showcasing the oldest person.", "code": "image = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\nfinal_answer(image)\n"}
  ---
  Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

  {"thought": "I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool", "code": "result = 5 + 3 + 1294.678\nfinal_answer(result)\n"}

  ---
  Task:
  In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.
  What does he say was the consequence of Einstein learning too much math on his creativity, in one word?

  {"thought": "I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.", "code": "pages = web_search(query=\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\")\nprint(pages)\n"}
  Observation:
  No result found for query "1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein".

  {"thought": "The query was maybe too restrictive and did not find any results. Let's try again with a broader query.", "code": "pages = web_search(query=\"1979 interview Stanislaus Ulam\")\nprint(pages)\n"}
  Observation:
  Found 6 pages:
  [Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)

  [Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)

  (truncated)

  {"thought": "I will read the first 2 pages to know more.", "code": "for url in [\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\", \"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\"]:\n      whole_page = visit_webpage(url)\n      print(whole_page)\n      print(\"\n\" + \"=\"*80 + \"\n\")  # Print separator between pages"}

  Observation:
  Manhattan Project Locations:
  Los Alamos, NM
  Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at
  (truncated)

  {"thought": "I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\" Let's answer in one word.", "code": "final_answer(\"diminished\")"}

  ---
  Task: "Which city has the highest population: Guangzhou or Shanghai?"

  {"thought": "I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.", "code": "for city in [\"Guangzhou\", \"Shanghai\"]:\n      print(f\"Population {city}:\", web_search(f\"{city} population\")"}
  Observation:
  Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']
  Population Shanghai: '26 million (2019)'

  {"thought": "Now I know that Shanghai has the highest population.", "code": "final_answer(\"Shanghai\")"}

  ---
  Task: "What is the current age of the pope, raised to the power 0.36?"

  {"thought": "I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.", "code": "pope_age_wiki = wikipedia_search(query=\"current pope age\")\nprint(\"Pope age as per wikipedia:\", pope_age_wiki)\npope_age_search = web_search(query=\"current pope age\")\nprint(\"Pope age as per google search:\", pope_age_search)"}
  Observation:
  Pope age: "The pope Francis is currently 88 years old."

  {"thought": "I know that the pope is 88 years old. Let's compute the result using python code.", "code": "pope_current_age = 88 ** 0.36\nfinal_answer(pope_current_age)"}

  Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:
  ```python
  {%- for tool in tools.values() %}
  {{ tool.to_code_prompt() }}
  {% endfor %}
  ```

  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  ```python
  {%- for agent in managed_agents.values() %}
  def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
      """{{ agent.description }}

      Args:
          task: Long detailed description of the task.
          additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
      """
  {% endfor %}
  ```
  {%- endif %}

  {%- if custom_instructions %}
  {{custom_instructions}}
  {%- endif %}

  Here are the rules you should always follow to solve your task:
  1. Use only variables that you have defined!
  2. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': "What is the place where James Bond lives?"})', but use the arguments directly as in 'answer = wikipedia_search(query="What is the place where James Bond lives?")'.
  3. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to wikipedia_search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.
  4. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.
  5. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.
  6. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.
  7. You can use imports in your code, but only from the following list of modules: {{authorized_imports}}
  8. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.
  9. Don't give up! You're in charge of solving the task, not providing directions to solve it.

  Now Begin!
planning:
  initial_plan: |-
    You are a world expert at analyzing a situation to derive facts, and plan accordingly towards solving a task.
    Below I will present you a task. You will need to 1. build a survey of facts known or needed to solve the task, then 2. make a plan of action to solve the task.

    ## 1. Facts survey
    You will build a comprehensive preparatory survey of which facts we have at our disposal and which ones we still need.
    These "facts" will typically be specific names, dates, values, etc. Your answer should use the below headings:
    ### 1.1. Facts given in the task
    List here the specific facts given in the task that could help you (there might be nothing here).

    ### 1.2. Facts to look up
    List here any facts that we may need to look up.
    Also list where to find each of these, for instance a website, a file... - maybe the task contains some sources that you should re-use here.

    ### 1.3. Facts to derive
    List here anything that we want to derive from the above by logical reasoning, for instance computation or simulation.

    Don't make any assumptions. For each item, provide a thorough reasoning. Do not add anything else on top of three headings above.

    ## 2. Plan
    Then for the given task, develop a step-by-step high-level plan taking into account the above inputs and list of facts.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools, behaving like regular python functions:
    ```python
    {%- for tool in tools.values() %}
    {{ tool.to_code_prompt() }}
    {% endfor %}
    ```

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    ```python
    {%- for agent in managed_agents.values() %}
    def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
        """{{ agent.description }}

        Args:
            task: Long detailed description of the task.
            additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
        """
    {% endfor %}
    ```
    {%- endif %}

    ---
    Now begin! Here is your task:
    ```
    {{task}}
    ```
    First in part 1, write the facts survey, then in part 2, write your plan.
  update_plan_pre_messages: |-
    You are a world expert at analyzing a situation, and plan accordingly towards solving a task.
    You have been given the following task:
    ```
    {{task}}
    ```

    Below you will find a history of attempts made to solve this task.
    You will first have to produce a survey of known and unknown facts, then propose a step-by-step high-level plan to solve the task.
    If the previous tries so far have met some success, your updated plan can build on these results.
    If you are stalled, you can make a completely new plan starting from scratch.

    Find the task and history below:
  update_plan_post_messages: |-
    Now write your updated facts below, taking into account the above history:
    ## 1. Updated facts survey
    ### 1.1. Facts given in the task
    ### 1.2. Facts that we have learned
    ### 1.3. Facts still to look up
    ### 1.4. Facts still to derive

    Then write a step-by-step high-level plan to solve the task above.
    ## 2. Plan
    ### 2. 1. ...
    Etc.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Beware that you have {remaining_steps} steps remaining.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools, behaving like regular python functions:
    ```python
    {%- for tool in tools.values() %}
    {{ tool.to_code_prompt() }}
    {% endfor %}
    ```

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    ```python
    {%- for agent in managed_agents.values() %}
    def {{ agent.name }}(task: str, additional_args: dict[str, Any]) -> str:
        """{{ agent.description }}

        Args:
            task: Long detailed description of the task.
            additional_args: Dictionary of extra inputs to pass to the managed agent, e.g. images, dataframes, or any other contextual data it may need.
        """
    {% endfor %}
    ```
    {%- endif %}

    Now write your updated facts survey below, then your new plan.
managed_agent:
  task: |-
    You're a helpful agent named '{{name}}'.
    You have been submitted this task by your manager.
    ---
    Task:
    {{task}}
    ---
    You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.

    Your final_answer WILL HAVE to contain these parts:
    ### 1. Task outcome (short version):
    ### 2. Task outcome (extremely detailed version):
    ### 3. Additional context (if relevant):

    Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.
    And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.
  report: |-
    Here is the final answer from your managed agent '{{name}}':
    {{final_answer}}
final_answer:
  pre_messages: |-
    An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:
  post_messages: |-
    Based on the above, please provide an answer to the following user task:
    {{task}}



================================================
FILE: src/smolagents/prompts/toolcalling_agent.yaml
================================================
system_prompt: |-
  You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.
  To do so, you have been given access to some tools.

  The tool call you write is an action: after the tool is executed, you will get the result of the tool call as an "observation".
  This Action/Observation can repeat N times, you should take several steps when needed.

  You can use the result of the previous action as input for the next action.
  The observation will always be a string: it can represent a file, like "image_1.jpg".
  Then you can use it as input for the next action. You can do it for instance as follows:

  Observation: "image_1.jpg"

  Action:
  {
    "name": "image_transformer",
    "arguments": {"image": "image_1.jpg"}
  }

  To provide the final answer to the task, use an action blob with "name": "final_answer" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:
  Action:
  {
    "name": "final_answer",
    "arguments": {"answer": "insert your final answer here"}
  }


  Here are a few examples using notional tools:
  ---
  Task: "Generate an image of the oldest person in this document."

  Action:
  {
    "name": "document_qa",
    "arguments": {"document": "document.pdf", "question": "Who is the oldest person mentioned?"}
  }
  Observation: "The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland."

  Action:
  {
    "name": "image_generator",
    "arguments": {"prompt": "A portrait of John Doe, a 55-year-old man living in Canada."}
  }
  Observation: "image.png"

  Action:
  {
    "name": "final_answer",
    "arguments": "image.png"
  }

  ---
  Task: "What is the result of the following operation: 5 + 3 + 1294.678?"

  Action:
  {
      "name": "python_interpreter",
      "arguments": {"code": "5 + 3 + 1294.678"}
  }
  Observation: 1302.678

  Action:
  {
    "name": "final_answer",
    "arguments": "1302.678"
  }

  ---
  Task: "Which city has the highest population , Guangzhou or Shanghai?"

  Action:
  {
      "name": "web_search",
      "arguments": "Population Guangzhou"
  }
  Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']


  Action:
  {
      "name": "web_search",
      "arguments": "Population Shanghai"
  }
  Observation: '26 million (2019)'

  Action:
  {
    "name": "final_answer",
    "arguments": "Shanghai"
  }

  Above example were using notional tools that might not exist for you. You only have access to these tools:
  {%- for tool in tools.values() %}
  - {{ tool.to_tool_calling_prompt() }}
  {%- endfor %}

  {%- if managed_agents and managed_agents.values() | list %}
  You can also give tasks to team members.
  Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
  You can also include any relevant variables or context using the 'additional_args' argument.
  Here is a list of the team members that you can call:
  {%- for agent in managed_agents.values() %}
  - {{ agent.name }}: {{ agent.description }}
    - Takes inputs: {{agent.inputs}}
    - Returns an output of type: {{agent.output_type}}
  {%- endfor %}
  {%- endif %}

  {%- if custom_instructions %}
  {{custom_instructions}}
  {%- endif %}

  Here are the rules you should always follow to solve your task:
  1. ALWAYS provide a tool call, else you will fail.
  2. Always use the right arguments for the tools. Never use variable names as the action arguments, use the value instead.
  3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself. If no tool call is needed, use final_answer tool to return your answer.
  4. Never re-do a tool call that you previously did with the exact same parameters.

  Now Begin!
planning:
  initial_plan : |-
    You are a world expert at analyzing a situation to derive facts, and plan accordingly towards solving a task.
    Below I will present you a task. You will need to 1. build a survey of facts known or needed to solve the task, then 2. make a plan of action to solve the task.

    ## 1. Facts survey
    You will build a comprehensive preparatory survey of which facts we have at our disposal and which ones we still need.
    These "facts" will typically be specific names, dates, values, etc. Your answer should use the below headings:
    ### 1.1. Facts given in the task
    List here the specific facts given in the task that could help you (there might be nothing here).

    ### 1.2. Facts to look up
    List here any facts that we may need to look up.
    Also list where to find each of these, for instance a website, a file... - maybe the task contains some sources that you should re-use here.

    ### 1.3. Facts to derive
    List here anything that we want to derive from the above by logical reasoning, for instance computation or simulation.

    Don't make any assumptions. For each item, provide a thorough reasoning. Do not add anything else on top of three headings above.

    ## 2. Plan
    Then for the given task, develop a step-by-step high-level plan taking into account the above inputs and list of facts.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools:
    {%- for tool in tools.values() %}
    - {{ tool.to_tool_calling_prompt() }}
    {%- endfor %}

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    {%- for agent in managed_agents.values() %}
    - {{ agent.name }}: {{ agent.description }}
      - Takes inputs: {{agent.inputs}}
      - Returns an output of type: {{agent.output_type}}
    {%- endfor %}
    {%- endif %}

    ---
    Now begin! Here is your task:
    ```
    {{task}}
    ```
    First in part 1, write the facts survey, then in part 2, write your plan.
  update_plan_pre_messages: |-
    You are a world expert at analyzing a situation, and plan accordingly towards solving a task.
    You have been given the following task:
    ```
    {{task}}
    ```
  
    Below you will find a history of attempts made to solve this task.
    You will first have to produce a survey of known and unknown facts, then propose a step-by-step high-level plan to solve the task.
    If the previous tries so far have met some success, your updated plan can build on these results.
    If you are stalled, you can make a completely new plan starting from scratch.

    Find the task and history below:
  update_plan_post_messages: |-
    Now write your updated facts below, taking into account the above history:
    ## 1. Updated facts survey
    ### 1.1. Facts given in the task
    ### 1.2. Facts that we have learned
    ### 1.3. Facts still to look up
    ### 1.4. Facts still to derive
  
    Then write a step-by-step high-level plan to solve the task above.
    ## 2. Plan
    ### 2. 1. ...
    Etc.
    This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.
    Beware that you have {remaining_steps} steps remaining.
    Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.
    After writing the final step of the plan, write the '<end_plan>' tag and stop there.

    You can leverage these tools:
    {%- for tool in tools.values() %}
    - {{ tool.to_tool_calling_prompt() }}
    {%- endfor %}

    {%- if managed_agents and managed_agents.values() | list %}
    You can also give tasks to team members.
    Calling a team member works similarly to calling a tool: provide the task description as the 'task' argument. Since this team member is a real human, be as detailed and verbose as necessary in your task description.
    You can also include any relevant variables or context using the 'additional_args' argument.
    Here is a list of the team members that you can call:
    {%- for agent in managed_agents.values() %}
    - {{ agent.name }}: {{ agent.description }}
      - Takes inputs: {{agent.inputs}}
      - Returns an output of type: {{agent.output_type}}
    {%- endfor %}
    {%- endif %}

    Now write your new plan below.
managed_agent:
  task: |-
      You're a helpful agent named '{{name}}'.
      You have been submitted this task by your manager.
      ---
      Task:
      {{task}}
      ---
      You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible to give them a clear understanding of the answer.

      Your final_answer WILL HAVE to contain these parts:
      ### 1. Task outcome (short version):
      ### 2. Task outcome (extremely detailed version):
      ### 3. Additional context (if relevant):

      Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.
      And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.
  report: |-
      Here is the final answer from your managed agent '{{name}}':
      {{final_answer}}
final_answer:
  pre_messages: |-
    An agent tried to answer a user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:
  post_messages: |-
    Based on the above, please provide an answer to the following user task:
    {{task}}



================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/conftest.py
================================================
from unittest.mock import patch

import pytest

from smolagents.agents import MultiStepAgent
from smolagents.monitoring import LogLevel


# Import fixture modules as plugins
pytest_plugins = ["tests.fixtures.agents", "tests.fixtures.tools"]

original_multi_step_agent_init = MultiStepAgent.__init__


@pytest.fixture(autouse=True)
def patch_multi_step_agent_with_suppressed_logging():
    with patch.object(MultiStepAgent, "__init__", autospec=True) as mock_init:

        def init_with_suppressed_logging(self, *args, verbosity_level=LogLevel.OFF, **kwargs):
            original_multi_step_agent_init(self, *args, verbosity_level=verbosity_level, **kwargs)

        mock_init.side_effect = init_with_suppressed_logging
        yield



================================================
FILE: tests/test_all_docs.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import ast
import os
import re
import shutil
import subprocess
import tempfile
import traceback
from pathlib import Path

import pytest
from dotenv import load_dotenv

from .utils.markers import require_run_all


class SubprocessCallException(Exception):
    pass


def run_command(command: list[str], return_stdout=False, env=None):
    """
    Runs command with subprocess.check_output and returns stdout if requested.
    Properly captures and handles errors during command execution.
    """
    for i, c in enumerate(command):
        if isinstance(c, Path):
            command[i] = str(c)

    if env is None:
        env = os.environ.copy()

    try:
        output = subprocess.check_output(command, stderr=subprocess.STDOUT, env=env)
        if return_stdout:
            if hasattr(output, "decode"):
                output = output.decode("utf-8")
            return output
    except subprocess.CalledProcessError as e:
        raise SubprocessCallException(
            f"Command `{' '.join(command)}` failed with the following error:\n\n{e.output.decode()}"
        ) from e


class DocCodeExtractor:
    """Handles extraction and validation of Python code from markdown files."""

    @staticmethod
    def extract_python_code(content: str) -> list[str]:
        """Extract Python code blocks from markdown content."""
        pattern = r"```(?:python|py)\n(.*?)\n```"
        matches = re.finditer(pattern, content, re.DOTALL)
        return [match.group(1).strip() for match in matches]

    @staticmethod
    def create_test_script(code_blocks: list[str], tmp_dir: str) -> Path:
        """Create a temporary Python script from code blocks."""
        combined_code = "\n\n".join(code_blocks)
        assert len(combined_code) > 0, "Code is empty!"
        tmp_file = Path(tmp_dir) / "test_script.py"

        with open(tmp_file, "w", encoding="utf-8") as f:
            f.write(combined_code)

        return tmp_file


# Skip: slow tests + require API keys
@require_run_all
class TestDocs:
    """Test case for documentation code testing."""

    @classmethod
    def setup_class(cls):
        cls._tmpdir = tempfile.mkdtemp()
        cls.launch_args = ["python3"]
        cls.docs_dir = Path(__file__).parent.parent / "docs" / "source" / "en"
        cls.extractor = DocCodeExtractor()

        if not cls.docs_dir.exists():
            raise ValueError(f"Docs directory not found at {cls.docs_dir}")

        load_dotenv()

        cls.md_files = list(cls.docs_dir.rglob("*.md")) + list(cls.docs_dir.rglob("*.mdx"))
        if not cls.md_files:
            raise ValueError(f"No markdown files found in {cls.docs_dir}")

    @classmethod
    def teardown_class(cls):
        shutil.rmtree(cls._tmpdir)

    @pytest.mark.timeout(100)
    def test_single_doc(self, doc_path: Path):
        """Test a single documentation file."""
        with open(doc_path, "r", encoding="utf-8") as f:
            content = f.read()

        code_blocks = self.extractor.extract_python_code(content)
        excluded_snippets = [
            "ToolCollection",
            "image_generation_tool",  # We don't want to run this expensive operation
            "from_langchain",  # Langchain is not a dependency
            "while llm_should_continue(memory):",  # This is pseudo code
            "ollama_chat/llama3.2",  # Exclude ollama building in guided tour
            "model = TransformersModel(model_id=model_id)",  # Exclude testing with transformers model
            "SmolagentsInstrumentor",  # Exclude telemetry since it needs additional installs
        ]
        code_blocks = [
            block
            for block in code_blocks
            if not any(
                [snippet in block for snippet in excluded_snippets]
            )  # Exclude these tools that take longer to run and add dependencies
        ]
        if len(code_blocks) == 0:
            pytest.skip(f"No Python code blocks found in {doc_path.name}")

        # Validate syntax of each block individually by parsing it
        for i, block in enumerate(code_blocks, 1):
            ast.parse(block)

        # Create and execute test script
        print("\n\nCollected code block:==========\n".join(code_blocks))
        try:
            code_blocks = [
                (
                    block.replace("<YOUR_HUGGINGFACEHUB_API_TOKEN>", os.getenv("HF_TOKEN"))
                    .replace("YOUR_ANTHROPIC_API_KEY", os.getenv("ANTHROPIC_API_KEY"))
                    .replace("{your_username}", "m-ric")
                )
                for block in code_blocks
            ]
            test_script = self.extractor.create_test_script(code_blocks, self._tmpdir)
            run_command(self.launch_args + [str(test_script)])

        except SubprocessCallException as e:
            pytest.fail(f"\nError while testing {doc_path.name}:\n{str(e)}")
        except Exception:
            pytest.fail(f"\nUnexpected error while testing {doc_path.name}:\n{traceback.format_exc()}")

    @pytest.fixture(autouse=True)
    def _setup(self):
        """Fixture to ensure temporary directory exists for each test."""
        os.makedirs(self._tmpdir, exist_ok=True)
        yield
        # Clean up test files after each test
        for file in Path(self._tmpdir).glob("*"):
            file.unlink()


def pytest_generate_tests(metafunc):
    """Generate test cases for each markdown file."""
    if "doc_path" in metafunc.fixturenames:
        test_class = metafunc.cls

        # Initialize the class if needed
        if not hasattr(test_class, "md_files"):
            test_class.setup_class()

        # Parameterize with the markdown files
        metafunc.parametrize("doc_path", test_class.md_files, ids=[f.stem for f in test_class.md_files])



================================================
FILE: tests/test_cli.py
================================================
from unittest.mock import patch

import pytest

from smolagents.cli import load_model
from smolagents.local_python_executor import CodeOutput, LocalPythonExecutor
from smolagents.models import InferenceClientModel, LiteLLMModel, OpenAIModel, TransformersModel


@pytest.fixture
def set_env_vars(monkeypatch):
    monkeypatch.setenv("FIREWORKS_API_KEY", "test_fireworks_api_key")
    monkeypatch.setenv("HF_TOKEN", "test_hf_api_key")


def test_load_model_openai_model(set_env_vars):
    with patch("openai.OpenAI") as MockOpenAI:
        model = load_model("OpenAIModel", "test_model_id")
    assert isinstance(model, OpenAIModel)
    assert model.model_id == "test_model_id"
    assert MockOpenAI.call_count == 1
    assert MockOpenAI.call_args.kwargs["base_url"] == "https://api.fireworks.ai/inference/v1"
    assert MockOpenAI.call_args.kwargs["api_key"] == "test_fireworks_api_key"


def test_load_model_litellm_model():
    model = load_model("LiteLLMModel", "test_model_id", api_key="test_api_key", api_base="https://api.test.com")
    assert isinstance(model, LiteLLMModel)
    assert model.api_key == "test_api_key"
    assert model.api_base == "https://api.test.com"
    assert model.model_id == "test_model_id"


def test_load_model_transformers_model():
    with (
        patch(
            "transformers.AutoModelForImageTextToText.from_pretrained",
            side_effect=ValueError("Unrecognized configuration class"),
        ),
        patch("transformers.AutoModelForCausalLM.from_pretrained"),
        patch("transformers.AutoTokenizer.from_pretrained"),
    ):
        model = load_model("TransformersModel", "test_model_id")
    assert isinstance(model, TransformersModel)
    assert model.model_id == "test_model_id"


def test_load_model_hf_api_model(set_env_vars):
    with patch("huggingface_hub.InferenceClient") as huggingface_hub_InferenceClient:
        model = load_model("InferenceClientModel", "test_model_id")
    assert isinstance(model, InferenceClientModel)
    assert model.model_id == "test_model_id"
    assert huggingface_hub_InferenceClient.call_count == 1
    assert huggingface_hub_InferenceClient.call_args.kwargs["token"] == "test_hf_api_key"


def test_load_model_invalid_model_type():
    with pytest.raises(ValueError, match="Unsupported model type: InvalidModel"):
        load_model("InvalidModel", "test_model_id")


def test_cli_main(capsys):
    with patch("smolagents.cli.load_model") as mock_load_model:
        mock_load_model.return_value = "mock_model"
        with patch("smolagents.cli.CodeAgent") as mock_code_agent:
            from smolagents.cli import run_smolagent

            run_smolagent("test_prompt", [], "InferenceClientModel", "test_model_id", provider="hf-inference")
    # load_model
    assert len(mock_load_model.call_args_list) == 1
    assert mock_load_model.call_args.args == ("InferenceClientModel", "test_model_id")
    assert mock_load_model.call_args.kwargs == {"api_base": None, "api_key": None, "provider": "hf-inference"}
    # CodeAgent
    assert len(mock_code_agent.call_args_list) == 1
    assert mock_code_agent.call_args.args == ()
    assert mock_code_agent.call_args.kwargs == {
        "tools": [],
        "model": "mock_model",
        "additional_authorized_imports": None,
        "stream_outputs": True,
    }
    # agent.run
    assert len(mock_code_agent.return_value.run.call_args_list) == 1
    assert mock_code_agent.return_value.run.call_args.args == ("test_prompt",)


def test_vision_web_browser_main():
    with patch("smolagents.vision_web_browser.helium"):
        with patch("smolagents.vision_web_browser.load_model") as mock_load_model:
            mock_load_model.return_value = "mock_model"
            with patch("smolagents.vision_web_browser.CodeAgent") as mock_code_agent:
                from smolagents.vision_web_browser import helium_instructions, run_webagent

                run_webagent("test_prompt", "InferenceClientModel", "test_model_id", provider="hf-inference")
    # load_model
    assert len(mock_load_model.call_args_list) == 1
    assert mock_load_model.call_args.args == ("InferenceClientModel", "test_model_id")
    # CodeAgent
    assert len(mock_code_agent.call_args_list) == 1
    assert mock_code_agent.call_args.args == ()
    assert len(mock_code_agent.call_args.kwargs["tools"]) == 4
    assert mock_code_agent.call_args.kwargs["model"] == "mock_model"
    assert mock_code_agent.call_args.kwargs["additional_authorized_imports"] == ["helium"]
    # agent.python_executor
    assert len(mock_code_agent.return_value.python_executor.call_args_list) == 1
    assert mock_code_agent.return_value.python_executor.call_args.args == ("from helium import *",)
    assert LocalPythonExecutor(["helium"])("from helium import *") == CodeOutput(
        output=None, logs="", is_final_answer=False
    )
    # agent.run
    assert len(mock_code_agent.return_value.run.call_args_list) == 1
    assert mock_code_agent.return_value.run.call_args.args == ("test_prompt" + helium_instructions,)



================================================
FILE: tests/test_default_tools.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import unittest

import pytest

from smolagents.agent_types import _AGENT_TYPE_MAPPING
from smolagents.default_tools import (
    DuckDuckGoSearchTool,
    PythonInterpreterTool,
    SpeechToTextTool,
    VisitWebpageTool,
    WikipediaSearchTool,
)

from .test_tools import ToolTesterMixin
from .utils.markers import require_run_all


class DefaultToolTests(unittest.TestCase):
    def test_visit_webpage(self):
        arguments = {"url": "https://huggingface.co/"}
        result = VisitWebpageTool()(arguments)
        assert isinstance(result, str)
        assert "Hugging Face â€“ The AI community building the future" in result

    @require_run_all
    def test_ddgs_with_kwargs(self):
        result = DuckDuckGoSearchTool(timeout=20)("DeepSeek parent company")
        assert isinstance(result, str)


class TestPythonInterpreterTool(ToolTesterMixin):
    def setup_method(self):
        self.tool = PythonInterpreterTool(authorized_imports=["numpy"])
        self.tool.setup()

    def test_exact_match_arg(self):
        result = self.tool("(2 / 2) * 4")
        assert result == "Stdout:\n\nOutput: 4.0"

    def test_exact_match_kwarg(self):
        result = self.tool(code="(2 / 2) * 4")
        assert result == "Stdout:\n\nOutput: 4.0"

    def test_agent_type_output(self):
        inputs = ["2 * 2"]
        output = self.tool(*inputs, sanitize_inputs_outputs=True)
        output_type = _AGENT_TYPE_MAPPING[self.tool.output_type]
        assert isinstance(output, output_type)

    def test_agent_types_inputs(self):
        inputs = ["2 * 2"]
        _inputs = []

        for _input, expected_input in zip(inputs, self.tool.inputs.values()):
            input_type = expected_input["type"]
            if isinstance(input_type, list):
                _inputs.append([_AGENT_TYPE_MAPPING[_input_type](_input) for _input_type in input_type])
            else:
                _inputs.append(_AGENT_TYPE_MAPPING[input_type](_input))

        # Should not raise an error
        output = self.tool(*inputs, sanitize_inputs_outputs=True)
        output_type = _AGENT_TYPE_MAPPING[self.tool.output_type]
        assert isinstance(output, output_type)

    def test_imports_work(self):
        result = self.tool("import numpy as np")
        assert "import from numpy is not allowed" not in result.lower()

    def test_unauthorized_imports_fail(self):
        with pytest.raises(Exception) as e:
            self.tool("import sympy as sp")
        assert "sympy" in str(e).lower()


class TestSpeechToTextTool:
    def test_new_instance(self):
        from transformers.models.whisper import WhisperForConditionalGeneration, WhisperProcessor

        tool = SpeechToTextTool()
        assert tool is not None
        assert tool.pre_processor_class == WhisperProcessor
        assert tool.model_class == WhisperForConditionalGeneration

    def test_initialization(self):
        from transformers.models.whisper import WhisperForConditionalGeneration, WhisperProcessor

        tool = SpeechToTextTool(model="dummy_model_id")
        assert tool is not None
        assert tool.pre_processor_class == WhisperProcessor
        assert tool.model_class == WhisperForConditionalGeneration


@pytest.mark.parametrize(
    "language, content_type, extract_format, query",
    [
        ("en", "summary", "HTML", "Python_(programming_language)"),  # English, Summary Mode, HTML format
        ("en", "text", "WIKI", "Python_(programming_language)"),  # English, Full Text Mode, WIKI format
        ("es", "summary", "HTML", "Python_(lenguaje_de_programaciÃ³n)"),  # Spanish, Summary Mode, HTML format
        ("es", "text", "WIKI", "Python_(lenguaje_de_programaciÃ³n)"),  # Spanish, Full Text Mode, WIKI format
    ],
)
def test_wikipedia_search(language, content_type, extract_format, query):
    tool = WikipediaSearchTool(
        user_agent="TestAgent (test@example.com)",
        language=language,
        content_type=content_type,
        extract_format=extract_format,
    )

    result = tool.forward(query)

    assert isinstance(result, str), "Output should be a string"
    assert "âœ… **Wikipedia Page:**" in result, "Response should contain Wikipedia page title"
    assert "ðŸ”— **Read more:**" in result, "Response should contain Wikipedia page URL"

    if content_type == "summary":
        assert len(result.split()) < 1000, "Summary mode should return a shorter text"
    if content_type == "text":
        assert len(result.split()) > 1000, "Full text mode should return a longer text"



================================================
FILE: tests/test_final_answer.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import numpy as np
import PIL.Image
import pytest

from smolagents.agent_types import _AGENT_TYPE_MAPPING
from smolagents.default_tools import FinalAnswerTool

from .test_tools import ToolTesterMixin
from .utils.markers import require_torch


class TestFinalAnswerTool(ToolTesterMixin):
    def setup_method(self):
        self.inputs = {"answer": "Final answer"}
        self.tool = FinalAnswerTool()

    def test_exact_match_arg(self):
        result = self.tool("Final answer")
        assert result == "Final answer"

    def test_exact_match_kwarg(self):
        result = self.tool(answer=self.inputs["answer"])
        assert result == "Final answer"

    @require_torch
    def test_agent_type_output(self, inputs):
        for input_type, input in inputs.items():
            output = self.tool(**input, sanitize_inputs_outputs=True)
            agent_type = _AGENT_TYPE_MAPPING[input_type]
            assert isinstance(output, agent_type)

    @pytest.fixture
    def inputs(self, shared_datadir):
        import torch

        return {
            "string": {"answer": "Text input"},
            "image": {"answer": PIL.Image.open(shared_datadir / "000000039769.png").resize((512, 512))},
            "audio": {"answer": torch.Tensor(np.ones(3000))},
        }



================================================
FILE: tests/test_function_type_hints_utils.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Any

import pytest

from smolagents._function_type_hints_utils import DocstringParsingException, get_imports, get_json_schema


@pytest.fixture
def valid_func():
    """A well-formed function with docstring, type hints, and return block."""

    def multiply(x: int, y: float) -> float:
        """
        Multiplies two numbers.

        Args:
            x: The first number.
            y: The second number.
        Returns:
            Product of x and y.
        """
        return x * y

    return multiply


@pytest.fixture
def no_docstring_func():
    """Function with no docstring."""

    def sample(x: int):
        return x

    return sample


@pytest.fixture
def missing_arg_doc_func():
    """Function with docstring but missing an argument description."""

    def add(x: int, y: int):
        """
        Adds two numbers.

        Args:
            x: The first number.
        """
        return x + y

    return add


@pytest.fixture
def bad_return_func():
    """Function docstring with missing return description (allowed)."""

    def do_nothing(x: str | None = None):
        """
        Does nothing.

        Args:
            x: Some optional string.
        """
        pass

    return do_nothing


@pytest.fixture
def complex_types_func():
    def process_data(items: list[str], config: dict[str, float], point: tuple[int, int]) -> dict:
        """
        Process some data.

        Args:
            items: List of items to process.
            config: Configuration parameters.
            point: A position as (x,y).

        Returns:
            Processed data result.
        """
        return {"result": True}

    return process_data


@pytest.fixture
def optional_types_func():
    def process_with_optional(required_arg: str, optional_arg: int | None = None) -> str:
        """
        Process with optional argument.

        Args:
            required_arg: A required string argument.
            optional_arg: An optional integer argument.

        Returns:
            Processing result.
        """
        return "processed"

    return process_with_optional


@pytest.fixture
def enum_choices_func():
    def select_color(color: str) -> str:
        """
        Select a color.

        Args:
            color: The color to select (choices: ["red", "green", "blue"])

        Returns:
            Selected color.
        """
        return color

    return select_color


@pytest.fixture
def union_types_func():
    def process_union(value: int | str) -> bool | str:
        """
        Process a value that can be either int or string.

        Args:
            value: An integer or string value.

        Returns:
            Processing result.
        """
        return True if isinstance(value, int) else "string result"

    return process_union


@pytest.fixture
def nested_types_func():
    def process_nested_data(data: list[dict[str, Any]]) -> list[str]:
        """
        Process nested data structure.

        Args:
            data: List of dictionaries to process.

        Returns:
            List of processed results.
        """
        return ["result"]

    return process_nested_data


@pytest.fixture
def typed_docstring_func():
    def calculate(x: int, y: float) -> float:
        """
        Calculate something.

        Args:
            x (int): An integer parameter with type in docstring.
            y (float): A float parameter with type in docstring.

        Returns:
            float: The calculated result.
        """
        return x * y

    return calculate


@pytest.fixture
def mismatched_types_func():
    def convert(value: int) -> str:
        """
        Convert a value.

        Args:
            value (str): A string value (type mismatch with hint).

        Returns:
            int: Converted value (type mismatch with hint).
        """
        return str(value)

    return convert


@pytest.fixture
def complex_docstring_types_func():
    def process(data: dict[str, list[int]]) -> list[dict[str, Any]]:
        """
        Process complex data.

        Args:
            data (Dict[str, List[int]]): Nested structure with types.

        Returns:
            List[Dict[str, Any]]: Processed results with types.
        """
        return [{"result": sum(v) for k, v in data.items()}]

    return process


@pytest.fixture
def keywords_in_description_func():
    def process(value: str) -> str:
        """
        Function with Args: or Returns: keywords in its description.

        Args:
            value: A string value.

        Returns:
            str: Processed value.
        """
        return value.upper()

    return process


class TestGetJsonSchema:
    def test_get_json_schema_example(self):
        def fn(x: int, y: tuple[str, str, float] | None = None) -> None:
            """
            Test function
            Args:
                x: The first input
                y: The second input
            """
            pass

        schema = get_json_schema(fn)
        expected_schema = {
            "name": "fn",
            "description": "Test function",
            "parameters": {
                "type": "object",
                "properties": {
                    "x": {"type": "integer", "description": "The first input"},
                    "y": {
                        "type": "array",
                        "description": "The second input",
                        "nullable": True,
                        "prefixItems": [{"type": "string"}, {"type": "string"}, {"type": "number"}],
                    },
                },
                "required": ["x"],
            },
            "return": {"type": "null"},
        }
        assert schema["function"]["parameters"]["properties"]["y"] == expected_schema["parameters"]["properties"]["y"]
        assert schema["function"] == expected_schema

    @pytest.mark.parametrize(
        "fixture_name,should_fail",
        [
            ("valid_func", False),
            # ('no_docstring_func', True),
            # ('missing_arg_doc_func', True),
            ("bad_return_func", False),
        ],
    )
    def test_get_json_schema(self, request, fixture_name, should_fail):
        func = request.getfixturevalue(fixture_name)
        schema = get_json_schema(func)
        assert schema["type"] == "function"
        assert "function" in schema
        assert "parameters" in schema["function"]

    @pytest.mark.parametrize(
        "fixture_name,should_fail",
        [
            # ('valid_func', False),
            ("no_docstring_func", True),
            ("missing_arg_doc_func", True),
            # ('bad_return_func', False),
        ],
    )
    def test_get_json_schema_raises(self, request, fixture_name, should_fail):
        func = request.getfixturevalue(fixture_name)
        with pytest.raises(DocstringParsingException):
            get_json_schema(func)

    @pytest.mark.parametrize(
        "fixture_name,expected_properties",
        [
            ("valid_func", {"x": "integer", "y": "number"}),
            ("bad_return_func", {"x": "string"}),
        ],
    )
    def test_property_types(self, request, fixture_name, expected_properties):
        """Test that property types are correctly mapped."""
        func = request.getfixturevalue(fixture_name)
        schema = get_json_schema(func)

        properties = schema["function"]["parameters"]["properties"]
        for prop_name, expected_type in expected_properties.items():
            assert properties[prop_name]["type"] == expected_type

    def test_schema_basic_structure(self, valid_func):
        """Test that basic schema structure is correct."""
        schema = get_json_schema(valid_func)
        # Check schema type
        assert schema["type"] == "function"
        assert "function" in schema
        # Check function schema
        function_schema = schema["function"]
        assert function_schema["name"] == "multiply"
        assert "description" in function_schema
        assert function_schema["description"] == "Multiplies two numbers."
        # Check parameters schema
        assert "parameters" in function_schema
        params = function_schema["parameters"]
        assert params["type"] == "object"
        assert "properties" in params
        assert "required" in params
        assert set(params["required"]) == {"x", "y"}
        properties = params["properties"]
        assert properties["x"]["type"] == "integer"
        assert properties["y"]["type"] == "number"
        # Check return schema
        assert "return" in function_schema
        return_schema = function_schema["return"]
        assert return_schema["type"] == "number"
        assert return_schema["description"] == "Product of x and y."

    def test_complex_types(self, complex_types_func):
        """Test schema generation for complex types."""
        schema = get_json_schema(complex_types_func)
        properties = schema["function"]["parameters"]["properties"]
        # Check list type
        assert properties["items"]["type"] == "array"
        # Check dict type
        assert properties["config"]["type"] == "object"
        # Check tuple type
        assert properties["point"]["type"] == "array"
        assert len(properties["point"]["prefixItems"]) == 2
        assert properties["point"]["prefixItems"][0]["type"] == "integer"
        assert properties["point"]["prefixItems"][1]["type"] == "integer"

    def test_optional_types(self, optional_types_func):
        """Test schema generation for optional arguments."""
        schema = get_json_schema(optional_types_func)
        params = schema["function"]["parameters"]
        # Required argument should be in required list
        assert "required_arg" in params["required"]
        # Optional argument should not be in required list
        assert "optional_arg" not in params["required"]
        # Optional argument should be nullable
        assert params["properties"]["optional_arg"]["nullable"] is True
        assert params["properties"]["optional_arg"]["type"] == "integer"

    def test_enum_choices(self, enum_choices_func):
        """Test schema generation for enum choices in docstring."""
        schema = get_json_schema(enum_choices_func)
        color_prop = schema["function"]["parameters"]["properties"]["color"]
        assert "enum" in color_prop
        assert color_prop["enum"] == ["red", "green", "blue"]

    def test_union_types(self, union_types_func):
        """Test schema generation for union types."""
        schema = get_json_schema(union_types_func)
        value_prop = schema["function"]["parameters"]["properties"]["value"]
        return_prop = schema["function"]["return"]
        # Check union in parameter
        assert len(value_prop["type"]) == 2
        # Check union in return type: should be converted to "any"
        assert return_prop["type"] == "any"

    def test_nested_types(self, nested_types_func):
        """Test schema generation for nested complex types."""
        schema = get_json_schema(nested_types_func)
        data_prop = schema["function"]["parameters"]["properties"]["data"]
        assert data_prop["type"] == "array"

    def test_typed_docstring_parsing(self, typed_docstring_func):
        """Test parsing of docstrings with type annotations."""
        schema = get_json_schema(typed_docstring_func)
        # Type hints should take precedence over docstring types
        assert schema["function"]["parameters"]["properties"]["x"]["type"] == "integer"
        assert schema["function"]["parameters"]["properties"]["y"]["type"] == "number"
        # Description should be extracted correctly
        assert (
            schema["function"]["parameters"]["properties"]["x"]["description"]
            == "An integer parameter with type in docstring."
        )
        assert (
            schema["function"]["parameters"]["properties"]["y"]["description"]
            == "A float parameter with type in docstring."
        )
        # Return type and description should be correct
        assert schema["function"]["return"]["type"] == "number"
        assert schema["function"]["return"]["description"] == "The calculated result."

    def test_mismatched_docstring_types(self, mismatched_types_func):
        """Test that type hints take precedence over docstring types when they conflict."""
        schema = get_json_schema(mismatched_types_func)
        # Type hints should take precedence over docstring types
        assert schema["function"]["parameters"]["properties"]["value"]["type"] == "integer"
        # Return type from type hint should be used, not docstring
        assert schema["function"]["return"]["type"] == "string"

    def test_complex_docstring_types(self, complex_docstring_types_func):
        """Test parsing of complex type annotations in docstrings."""
        schema = get_json_schema(complex_docstring_types_func)
        # Check that complex nested type is parsed correctly from type hints
        data_prop = schema["function"]["parameters"]["properties"]["data"]
        assert data_prop["type"] == "object"
        # Check return type
        return_prop = schema["function"]["return"]
        assert return_prop["type"] == "array"
        # Description should include the type information from docstring
        assert data_prop["description"] == "Nested structure with types."
        assert return_prop["description"] == "Processed results with types."

    @pytest.mark.parametrize(
        "fixture_name,expected_description",
        [
            ("typed_docstring_func", "An integer parameter with type in docstring."),
            ("complex_docstring_types_func", "Nested structure with types."),
        ],
    )
    def test_type_in_description_handling(self, request, fixture_name, expected_description):
        """Test that type information in docstrings is preserved in description."""
        func = request.getfixturevalue(fixture_name)
        schema = get_json_schema(func)
        # First parameter description should contain the expected text
        first_param_name = list(schema["function"]["parameters"]["properties"].keys())[0]
        assert schema["function"]["parameters"]["properties"][first_param_name]["description"] == expected_description

    def test_with_special_words_in_description_func(self, keywords_in_description_func):
        schema = get_json_schema(keywords_in_description_func)
        assert schema["function"]["description"] == "Function with Args: or Returns: keywords in its description."


class TestGetCode:
    @pytest.mark.parametrize(
        "code, expected",
        [
            (
                """
        import numpy
        import pandas
        """,
                ["numpy", "pandas"],
            ),
            # From imports
            (
                """
        from torch import nn
        from transformers import AutoModel
        """,
                ["torch", "transformers"],
            ),
            # Mixed case with nested imports
            (
                """
        import numpy as np
        from torch.nn import Linear
        import os.path
        """,
                ["numpy", "torch", "os"],
            ),
            # Try/except block (should be filtered)
            (
                """
        try:
            import torch
        except ImportError:
            pass
        import numpy
        """,
                ["numpy"],
            ),
            # Flash attention block (should be filtered)
            (
                """
        if is_flash_attn_2_available():
            from flash_attn import flash_attn_func
        import transformers
        """,
                ["transformers"],
            ),
            # Relative imports (should be excluded)
            (
                """
        from .utils import helper
        from ..models import transformer
        """,
                [],
            ),
        ],
    )
    def test_get_imports(self, code: str, expected: list[str]):
        assert sorted(get_imports(code)) == sorted(expected)



================================================
FILE: tests/test_gradio_ui.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import shutil
import tempfile
import unittest
from unittest.mock import Mock, patch

import pytest

from smolagents.agent_types import AgentAudio, AgentImage, AgentText
from smolagents.gradio_ui import GradioUI, pull_messages_from_step, stream_to_gradio
from smolagents.memory import ActionStep, FinalAnswerStep, PlanningStep, ToolCall
from smolagents.models import ChatMessageStreamDelta
from smolagents.monitoring import Timing, TokenUsage


class GradioUITester(unittest.TestCase):
    def setUp(self):
        """Initialize test environment"""
        self.temp_dir = tempfile.mkdtemp()
        self.mock_agent = Mock()
        self.ui = GradioUI(agent=self.mock_agent, file_upload_folder=self.temp_dir)
        self.allowed_types = [".pdf", ".docx", ".txt"]

    def tearDown(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir)

    def test_upload_file_default_types(self):
        """Test default allowed file types"""
        default_types = [".pdf", ".docx", ".txt"]
        for file_type in default_types:
            with tempfile.NamedTemporaryFile(suffix=file_type) as temp_file:
                mock_file = Mock()
                mock_file.name = temp_file.name

                textbox, uploads_log = self.ui.upload_file(mock_file, [])

                self.assertIn("File uploaded:", textbox.value)
                self.assertEqual(len(uploads_log), 1)
                self.assertTrue(os.path.exists(os.path.join(self.temp_dir, os.path.basename(temp_file.name))))

    def test_upload_file_default_types_disallowed(self):
        """Test default disallowed file types"""
        disallowed_types = [".exe", ".sh", ".py", ".jpg"]
        for file_type in disallowed_types:
            with tempfile.NamedTemporaryFile(suffix=file_type) as temp_file:
                mock_file = Mock()
                mock_file.name = temp_file.name

                textbox, uploads_log = self.ui.upload_file(mock_file, [])

                self.assertEqual(textbox.value, "File type disallowed")
                self.assertEqual(len(uploads_log), 0)

    def test_upload_file_success(self):
        """Test successful file upload scenario"""
        with tempfile.NamedTemporaryFile(suffix=".txt") as temp_file:
            mock_file = Mock()
            mock_file.name = temp_file.name

            textbox, uploads_log = self.ui.upload_file(mock_file, [])

            self.assertIn("File uploaded:", textbox.value)
            self.assertEqual(len(uploads_log), 1)
            self.assertTrue(os.path.exists(os.path.join(self.temp_dir, os.path.basename(temp_file.name))))
            self.assertEqual(uploads_log[0], os.path.join(self.temp_dir, os.path.basename(temp_file.name)))

    def test_upload_file_none(self):
        """Test scenario when no file is selected"""
        textbox, uploads_log = self.ui.upload_file(None, [])

        self.assertEqual(textbox.value, "No file uploaded")
        self.assertEqual(len(uploads_log), 0)

    def test_upload_file_invalid_type(self):
        """Test disallowed file type"""
        with tempfile.NamedTemporaryFile(suffix=".exe") as temp_file:
            mock_file = Mock()
            mock_file.name = temp_file.name

            textbox, uploads_log = self.ui.upload_file(mock_file, [])

            self.assertEqual(textbox.value, "File type disallowed")
            self.assertEqual(len(uploads_log), 0)

    def test_upload_file_special_chars(self):
        """Test scenario with special characters in filename"""
        with tempfile.NamedTemporaryFile(suffix=".txt") as temp_file:
            # Create a new temporary file with special characters
            special_char_name = os.path.join(os.path.dirname(temp_file.name), "test@#$%^&*.txt")
            shutil.copy(temp_file.name, special_char_name)
            try:
                mock_file = Mock()
                mock_file.name = special_char_name

                with patch("shutil.copy"):
                    textbox, uploads_log = self.ui.upload_file(mock_file, [])

                    self.assertIn("File uploaded:", textbox.value)
                    self.assertEqual(len(uploads_log), 1)
                    self.assertIn("test_____", uploads_log[0])
            finally:
                # Clean up the special character file
                if os.path.exists(special_char_name):
                    os.remove(special_char_name)

    def test_upload_file_custom_types(self):
        """Test custom allowed file types"""
        with tempfile.NamedTemporaryFile(suffix=".csv") as temp_file:
            mock_file = Mock()
            mock_file.name = temp_file.name

            textbox, uploads_log = self.ui.upload_file(mock_file, [], allowed_file_types=[".csv"])

            self.assertIn("File uploaded:", textbox.value)
            self.assertEqual(len(uploads_log), 1)


class TestStreamToGradio:
    """Tests for the stream_to_gradio function."""

    @patch("smolagents.gradio_ui.pull_messages_from_step")
    def test_stream_to_gradio_memory_step(self, mock_pull_messages):
        """Test streaming a memory step"""
        # Create mock agent and memory step
        mock_agent = Mock()
        mock_agent.run = Mock(return_value=[Mock(spec=ActionStep)])
        mock_agent.model = Mock()
        # Mock the pull_messages_from_step function to return some messages
        mock_message = Mock()
        mock_pull_messages.return_value = [mock_message]
        # Call stream_to_gradio
        result = list(stream_to_gradio(mock_agent, "test task"))
        # Verify that pull_messages_from_step was called and the message was yielded
        mock_pull_messages.assert_called_once()
        assert result == [mock_message]

    def test_stream_to_gradio_stream_delta(self):
        """Test streaming a ChatMessageStreamDelta"""
        # Create mock agent and stream delta
        mock_agent = Mock()
        mock_delta = ChatMessageStreamDelta(content="Hello")
        mock_agent.run = Mock(return_value=[mock_delta])
        mock_agent.model = Mock()
        # Call stream_to_gradio
        result = list(stream_to_gradio(mock_agent, "test task"))
        # Verify that the content was yielded
        assert result == ["Hello"]

    def test_stream_to_gradio_multiple_deltas(self):
        """Test streaming multiple ChatMessageStreamDeltas"""
        # Create mock agent and stream deltas
        mock_agent = Mock()
        mock_delta1 = ChatMessageStreamDelta(content="Hello")
        mock_delta2 = ChatMessageStreamDelta(content=" world")
        mock_agent.run = Mock(return_value=[mock_delta1, mock_delta2])
        mock_agent.model = Mock()
        # Call stream_to_gradio
        result = list(stream_to_gradio(mock_agent, "test task"))
        # Verify that the content was accumulated and yielded
        assert result == ["Hello", "Hello world"]

    @pytest.mark.parametrize(
        "task,task_images,reset_memory,additional_args",
        [
            ("simple task", None, False, None),
            ("task with images", ["image1.png", "image2.png"], False, None),
            ("task with reset", None, True, None),
            ("task with args", None, False, {"arg1": "value1"}),
            ("complex task", ["image.png"], True, {"arg1": "value1", "arg2": "value2"}),
        ],
    )
    def test_stream_to_gradio_parameters(self, task, task_images, reset_memory, additional_args):
        """Test that stream_to_gradio passes parameters correctly to agent.run"""
        # Create mock agent
        mock_agent = Mock()
        mock_agent.run = Mock(return_value=[])
        # Call stream_to_gradio
        list(
            stream_to_gradio(
                mock_agent,
                task=task,
                task_images=task_images,
                reset_agent_memory=reset_memory,
                additional_args=additional_args,
            )
        )
        # Verify that agent.run was called with the right parameters
        mock_agent.run.assert_called_once_with(
            task, images=task_images, stream=True, reset=reset_memory, additional_args=additional_args
        )


class TestPullMessagesFromStep:
    def test_action_step_basic(
        self,
    ):
        """Test basic ActionStep processing."""
        step = ActionStep(
            step_number=1,
            model_output="This is the model output",
            observations="Some execution logs",
            error=None,
            timing=Timing(start_time=1.0, end_time=3.5),
            token_usage=TokenUsage(input_tokens=100, output_tokens=50),
        )
        messages = list(pull_messages_from_step(step))
        assert len(messages) == 5  # step number, model_output, logs, footnote, divider
        for message, expected_content in zip(
            messages,
            [
                "**Step 1**",
                "This is the model output",
                "execution logs",
                "Input tokens: 100 | Output tokens: 50 | Duration: 2.5",
                "-----",
            ],
        ):
            assert expected_content in message.content

    def test_action_step_with_tool_calls(self):
        """Test ActionStep with tool calls."""
        step = ActionStep(
            step_number=2,
            tool_calls=[ToolCall(name="test_tool", arguments={"answer": "Test answer"}, id="tool_call_1")],
            observations="Tool execution logs",
            timing=Timing(start_time=1.0, end_time=2.5),
            token_usage=TokenUsage(input_tokens=100, output_tokens=50),
        )
        messages = list(pull_messages_from_step(step))
        assert len(messages) == 5  # step, tool call, logs, footnote, divider
        assert messages[1].content == "Test answer"
        assert "Used tool test_tool" in messages[1].metadata["title"]

    @pytest.mark.parametrize(
        "tool_name, args, expected",
        [
            ("python_interpreter", "print('Hello')", "```python\nprint('Hello')\n```"),
            ("regular_tool", {"key": "value"}, "{'key': 'value'}"),
            ("string_args_tool", "simple string", "simple string"),
        ],
    )
    def test_action_step_tool_call_formats(self, tool_name, args, expected):
        """Test different formats of tool calls."""
        tool_call = Mock()
        tool_call.name = tool_name
        tool_call.arguments = args
        step = ActionStep(
            step_number=1,
            tool_calls=[tool_call],
            timing=Timing(start_time=1.0, end_time=2.5),
            token_usage=TokenUsage(input_tokens=100, output_tokens=50),
        )
        messages = list(pull_messages_from_step(step))
        tool_message = next(
            msg
            for msg in messages
            if msg.role == "assistant" and msg.metadata and msg.metadata.get("title", "").startswith("ðŸ› ï¸")
        )
        assert expected in tool_message.content

    def test_action_step_with_error(self):
        """Test ActionStep with error."""
        step = ActionStep(
            step_number=3,
            error="This is an error message",
            timing=Timing(start_time=1.0, end_time=2.0),
            token_usage=TokenUsage(input_tokens=100, output_tokens=200),
        )
        messages = list(pull_messages_from_step(step))
        error_message = next((m for m in messages if "error" in str(m.content).lower()), None)
        assert error_message is not None
        assert "This is an error message" in error_message.content

    def test_action_step_with_images(self):
        """Test ActionStep with observation images."""
        step = ActionStep(
            step_number=4,
            observations_images=["image1.png", "image2.jpg"],
            token_usage=TokenUsage(input_tokens=100, output_tokens=200),
            timing=Timing(start_time=1.0, end_time=2.0),
        )
        with patch("smolagents.gradio_ui.AgentImage") as mock_agent_image:
            mock_agent_image.return_value.to_string.side_effect = lambda: "path/to/image.png"
            messages = list(pull_messages_from_step(step))
            image_messages = [m for m in messages if "image" in str(m).lower()]
            assert len(image_messages) == 2
            assert "path/to/image.png" in str(image_messages[0])

    @pytest.mark.parametrize(
        "skip_model_outputs, expected_messages_length, token_usage",
        [(False, 4, TokenUsage(input_tokens=80, output_tokens=30)), (True, 2, None)],
    )
    def test_planning_step(self, skip_model_outputs, expected_messages_length, token_usage):
        """Test PlanningStep processing."""
        step = PlanningStep(
            plan="1. First step\n2. Second step",
            model_input_messages=Mock(),
            model_output_message=Mock(),
            token_usage=token_usage,
            timing=Timing(start_time=1.0, end_time=2.0),
        )
        messages = list(pull_messages_from_step(step, skip_model_outputs=skip_model_outputs))
        assert len(messages) == expected_messages_length  # [header, plan,] footnote, divider
        expected_contents = [
            "**Planning step**",
            "1. First step\n2. Second step",
            "Input tokens: 80 | Output tokens: 30" if token_usage else "",
            "-----",
        ]
        for message, expected_content in zip(messages, expected_contents[-expected_messages_length:]):
            assert expected_content in message.content

        if not token_usage:
            assert "Input tokens: 80 | Output tokens: 30" not in message.content

    @pytest.mark.parametrize(
        "answer_type, answer_value, expected_content",
        [
            (AgentText, "This is a text answer", "**Final answer:**\nThis is a text answer\n"),
            (lambda: "Plain string", "Plain string", "**Final answer:** Plain string"),
        ],
    )
    def test_final_answer_step(self, answer_type, answer_value, expected_content):
        """Test FinalAnswerStep with different answer types."""
        try:
            final_answer = answer_type()
        except TypeError:
            with patch.object(answer_type, "to_string", return_value=answer_value):
                final_answer = answer_type(answer_value)
        step = FinalAnswerStep(
            output=final_answer,
        )
        messages = list(pull_messages_from_step(step))
        assert len(messages) == 1
        assert messages[0].content == expected_content

    def test_final_answer_step_image(self):
        """Test FinalAnswerStep with image answer."""
        with patch.object(AgentImage, "to_string", return_value="path/to/image.png"):
            step = FinalAnswerStep(output=AgentImage("path/to/image.png"))
            messages = list(pull_messages_from_step(step))
            assert len(messages) == 1
            assert messages[0].content["path"] == "path/to/image.png"
            assert messages[0].content["mime_type"] == "image/png"

    def test_final_answer_step_audio(self):
        """Test FinalAnswerStep with audio answer."""
        with patch.object(AgentAudio, "to_string", return_value="path/to/audio.wav"):
            step = FinalAnswerStep(output=AgentAudio("path/to/audio.wav"))
            messages = list(pull_messages_from_step(step))
            assert len(messages) == 1
            assert messages[0].content["path"] == "path/to/audio.wav"
            assert messages[0].content["mime_type"] == "audio/wav"

    def test_unsupported_step_type(self):
        """Test handling of unsupported step types."""

        class UnsupportedStep(Mock):
            pass

        step = UnsupportedStep()
        with pytest.raises(ValueError, match="Unsupported step type"):
            list(pull_messages_from_step(step))



================================================
FILE: tests/test_import.py
================================================
import os
import subprocess
import tempfile


def test_import_smolagents_without_extras(monkeypatch):
    monkeypatch.delenv("VIRTUAL_ENV", raising=False)
    with tempfile.TemporaryDirectory() as temp_dir:
        # Create a virtual environment
        venv_dir = os.path.join(temp_dir, "venv")
        subprocess.run(["uv", "venv", venv_dir], check=True)

        # Install smolagents in the virtual environment
        subprocess.run(
            ["uv", "pip", "install", "--python", os.path.join(venv_dir, "bin", "python"), "smolagents @ ."], check=True
        )

        # Run the import test in the virtual environment
        result = subprocess.run(
            [os.path.join(venv_dir, "bin", "python"), "-c", "import smolagents"],
            capture_output=True,
            text=True,
        )

    # Check if the import was successful
    assert result.returncode == 0, (
        "Import failed with error: "
        + (result.stderr.splitlines()[-1] if result.stderr else "No error message")
        + "\n"
        + result.stderr
    )



================================================
FILE: tests/test_mcp_client.py
================================================
import json
from textwrap import dedent

import pytest
from mcp import StdioServerParameters

from smolagents.mcp_client import MCPClient


@pytest.fixture
def echo_server_script():
    return dedent(
        '''
        from mcp.server.fastmcp import FastMCP

        mcp = FastMCP("Echo Server")

        @mcp.tool()
        def echo_tool(text: str) -> str:
            """Echo the input text"""
            return f"Echo: {text}"

        mcp.run()
        '''
    )


@pytest.fixture
def structured_output_server_script():
    return dedent(
        '''
        from mcp.server.fastmcp import FastMCP
        from typing import Any

        mcp = FastMCP("Structured Output Server")

        @mcp.tool()
        def user_info_tool(name: str) -> dict[str, Any]:
            """Get user information as structured data"""
            user_data = {
                "name": name,
                "age": 25,
                "email": f"{name.lower()}@example.com",
                "active": True
            }
            return user_data

        mcp.run()
        '''
    )


# Ignore FutureWarning about structured_output default value change: this test intentionally uses default behavior
@pytest.mark.filterwarnings("ignore:.*structured_output:FutureWarning")
def test_mcp_client_with_syntax(echo_server_script: str):
    """Test the MCPClient with the context manager syntax."""
    server_parameters = StdioServerParameters(command="python", args=["-c", echo_server_script])
    with MCPClient(server_parameters) as tools:
        assert len(tools) == 1
        assert tools[0].name == "echo_tool"
        assert tools[0].forward(**{"text": "Hello, world!"}) == "Echo: Hello, world!"


def test_mcp_client_with_structured_output(structured_output_server_script: str):
    """Test the MCPClient with structured_output=True parameter."""
    server_parameters = StdioServerParameters(command="python", args=["-c", structured_output_server_script])
    with MCPClient(server_parameters, structured_output=True) as tools:
        assert len(tools) == 1
        assert tools[0].name == "user_info_tool"
        assert tools[0].output_type == "object"  # Should be object due to outputSchema

        # Check the output schema {'additionalProperties': True, 'title': 'user_info_toolDictOutput', 'type': 'object'}
        assert tools[0].output_schema is not None
        schema = tools[0].output_schema
        assert isinstance(schema, dict)
        assert schema.get("type") == "object"

        # Test that structured output is properly parsed
        result = tools[0].forward(**{"name": "Alice"})
        assert isinstance(result, dict)
        assert result["name"] == "Alice"
        assert result["age"] == 25
        assert result["email"] == "alice@example.com"
        assert result["active"] is True


def test_mcp_client_without_structured_output(structured_output_server_script: str):
    """Test the MCPClient with structured_output=False (default) for comparison."""
    server_parameters = StdioServerParameters(command="python", args=["-c", structured_output_server_script])
    with MCPClient(server_parameters, structured_output=False) as tools:
        assert len(tools) == 1
        assert tools[0].name == "user_info_tool"
        assert tools[0].output_type == "object"

        # Test that output is returned as raw text
        result = tools[0].forward(**{"name": "Alice"})
        assert isinstance(result, str)
        # Should be JSON string, not parsed object
        parsed_result = json.loads(result)
        assert parsed_result["name"] == "Alice"


# Ignore FutureWarning about structured_output default value change: this test intentionally uses default behavior
@pytest.mark.filterwarnings("ignore:.*structured_output:FutureWarning")
def test_mcp_client_try_finally_syntax(echo_server_script: str):
    """Test the MCPClient with the try ... finally syntax."""
    server_parameters = StdioServerParameters(command="python", args=["-c", echo_server_script])
    mcp_client = MCPClient(server_parameters)
    try:
        tools = mcp_client.get_tools()
        assert len(tools) == 1
        assert tools[0].name == "echo_tool"
        assert tools[0].forward(**{"text": "Hello, world!"}) == "Echo: Hello, world!"
    finally:
        mcp_client.disconnect()


# Ignore FutureWarning about structured_output default value change: this test intentionally uses default behavior
@pytest.mark.filterwarnings("ignore:.*structured_output:FutureWarning")
def test_multiple_servers(echo_server_script: str):
    """Test the MCPClient with multiple servers."""
    server_parameters = [
        StdioServerParameters(command="python", args=["-c", echo_server_script]),
        StdioServerParameters(command="python", args=["-c", echo_server_script]),
    ]
    with MCPClient(server_parameters) as tools:
        assert len(tools) == 2
        assert tools[0].name == "echo_tool"
        assert tools[1].name == "echo_tool"
        assert tools[0].forward(**{"text": "Hello, world!"}) == "Echo: Hello, world!"
        assert tools[1].forward(**{"text": "Hello, world!"}) == "Echo: Hello, world!"



================================================
FILE: tests/test_memory.py
================================================
import json

import pytest
from PIL import Image

from smolagents.agents import ToolCall
from smolagents.memory import (
    ActionStep,
    AgentMemory,
    ChatMessage,
    MemoryStep,
    MessageRole,
    PlanningStep,
    SystemPromptStep,
    TaskStep,
)
from smolagents.monitoring import Timing, TokenUsage


class TestAgentMemory:
    def test_initialization(self):
        system_prompt = "This is a system prompt."
        memory = AgentMemory(system_prompt=system_prompt)
        assert memory.system_prompt.system_prompt == system_prompt
        assert memory.steps == []

    def test_return_all_code_actions(self):
        memory = AgentMemory(system_prompt="This is a system prompt.")
        memory.steps = [
            ActionStep(step_number=1, timing=Timing(start_time=0.0, end_time=1.0), code_action="print('Hello')"),
            ActionStep(step_number=2, timing=Timing(start_time=0.0, end_time=1.0), code_action=None),
            ActionStep(step_number=3, timing=Timing(start_time=0.0, end_time=1.0), code_action="print('World')"),
        ]  # type: ignore
        assert memory.return_full_code() == "print('Hello')\n\nprint('World')"


class TestMemoryStep:
    def test_initialization(self):
        step = MemoryStep()
        assert isinstance(step, MemoryStep)

    def test_dict(self):
        step = MemoryStep()
        assert step.dict() == {}

    def test_to_messages(self):
        step = MemoryStep()
        with pytest.raises(NotImplementedError):
            step.to_messages()


def test_action_step_dict():
    action_step = ActionStep(
        model_input_messages=[ChatMessage(role=MessageRole.USER, content="Hello")],
        tool_calls=[
            ToolCall(id="id", name="get_weather", arguments={"location": "Paris"}),
        ],
        timing=Timing(start_time=0.0, end_time=1.0),
        step_number=1,
        error=None,
        model_output_message=ChatMessage(role=MessageRole.ASSISTANT, content="Hi"),
        model_output="Hi",
        observations="This is a nice observation",
        observations_images=[Image.new("RGB", (100, 100))],
        action_output="Output",
        token_usage=TokenUsage(input_tokens=10, output_tokens=20),
    )
    action_step_dict = action_step.dict()
    # Check each key individually for better test failure messages
    assert "model_input_messages" in action_step_dict
    assert action_step_dict["model_input_messages"] == [
        {"role": MessageRole.USER, "content": "Hello", "tool_calls": None, "raw": None, "token_usage": None}
    ]

    assert "tool_calls" in action_step_dict
    assert len(action_step_dict["tool_calls"]) == 1
    assert action_step_dict["tool_calls"][0] == {
        "id": "id",
        "type": "function",
        "function": {
            "name": "get_weather",
            "arguments": {"location": "Paris"},
        },
    }

    assert "timing" in action_step_dict
    assert action_step_dict["timing"] == {"start_time": 0.0, "end_time": 1.0, "duration": 1.0}

    assert "token_usage" in action_step_dict
    assert action_step_dict["token_usage"] == {"input_tokens": 10, "output_tokens": 20, "total_tokens": 30}

    assert "step_number" in action_step_dict
    assert action_step_dict["step_number"] == 1

    assert "error" in action_step_dict
    assert action_step_dict["error"] is None

    assert "model_output_message" in action_step_dict
    assert action_step_dict["model_output_message"] == {
        "role": "assistant",
        "content": "Hi",
        "tool_calls": None,
        "raw": None,
        "token_usage": None,
    }

    assert "model_output" in action_step_dict
    assert action_step_dict["model_output"] == "Hi"

    assert "observations" in action_step_dict
    assert action_step_dict["observations"] == "This is a nice observation"

    assert "observations_images" in action_step_dict

    assert "action_output" in action_step_dict
    assert action_step_dict["action_output"] == "Output"


def test_action_step_to_messages():
    action_step = ActionStep(
        model_input_messages=[ChatMessage(role=MessageRole.USER, content="Hello")],
        tool_calls=[
            ToolCall(id="id", name="get_weather", arguments={"location": "Paris"}),
        ],
        timing=Timing(start_time=0.0, end_time=1.0),
        step_number=1,
        error=None,
        model_output_message=ChatMessage(role=MessageRole.ASSISTANT, content="Hi"),
        model_output="Hi",
        observations="This is a nice observation",
        observations_images=[Image.new("RGB", (100, 100))],
        action_output="Output",
        token_usage=TokenUsage(input_tokens=10, output_tokens=20),
    )
    messages = action_step.to_messages()
    assert len(messages) == 4
    for message in messages:
        assert isinstance(message, ChatMessage)
    assistant_message = messages[0]
    assert assistant_message.role == MessageRole.ASSISTANT
    assert len(assistant_message.content) == 1
    assert assistant_message.content[0]["type"] == "text"
    assert assistant_message.content[0]["text"] == "Hi"
    message = messages[1]
    assert message.role == MessageRole.TOOL_CALL

    assert len(message.content) == 1
    assert message.content[0]["type"] == "text"
    assert "Calling tools:" in message.content[0]["text"]

    image_message = messages[2]
    assert image_message.content[0]["type"] == "image"  # type: ignore

    observation_message = messages[3]
    assert observation_message.role == MessageRole.TOOL_RESPONSE
    assert "Observation:\nThis is a nice observation" in observation_message.content[0]["text"]


def test_action_step_to_messages_no_tool_calls_with_observations():
    action_step = ActionStep(
        model_input_messages=None,
        tool_calls=None,
        timing=Timing(start_time=0.0, end_time=1.0),
        step_number=1,
        error=None,
        model_output_message=None,
        model_output=None,
        observations="This is an observation.",
        observations_images=None,
        action_output=None,
        token_usage=TokenUsage(input_tokens=10, output_tokens=20),
    )
    messages = action_step.to_messages()
    assert len(messages) == 1
    observation_message = messages[0]
    assert observation_message.role == MessageRole.TOOL_RESPONSE
    assert "Observation:\nThis is an observation." in observation_message.content[0]["text"]


def test_planning_step_to_messages():
    planning_step = PlanningStep(
        model_input_messages=[ChatMessage(role=MessageRole.USER, content="Hello")],
        model_output_message=ChatMessage(role=MessageRole.ASSISTANT, content="Plan"),
        plan="This is a plan.",
        timing=Timing(start_time=0.0, end_time=1.0),
    )
    messages = planning_step.to_messages(summary_mode=False)
    assert len(messages) == 2
    for message in messages:
        assert isinstance(message, ChatMessage)
        assert isinstance(message.content, list)
        assert len(message.content) == 1
        for content in message.content:
            assert isinstance(content, dict)
            assert "type" in content
            assert "text" in content
    assert messages[0].role == MessageRole.ASSISTANT
    assert messages[1].role == MessageRole.USER


def test_task_step_to_messages():
    task_step = TaskStep(task="This is a task.", task_images=[Image.new("RGB", (100, 100))])
    messages = task_step.to_messages(summary_mode=False)
    assert len(messages) == 1
    for message in messages:
        assert isinstance(message, ChatMessage)
        assert message.role == MessageRole.USER
        assert isinstance(message.content, list)
        assert len(message.content) == 2
        text_content = message.content[0]
        assert isinstance(text_content, dict)
        assert "type" in text_content
        assert "text" in text_content
        for image_content in message.content[1:]:
            assert isinstance(image_content, dict)
            assert "type" in image_content
            assert "image" in image_content


def test_system_prompt_step_to_messages():
    system_prompt_step = SystemPromptStep(system_prompt="This is a system prompt.")
    messages = system_prompt_step.to_messages(summary_mode=False)
    assert len(messages) == 1
    for message in messages:
        assert isinstance(message, ChatMessage)
        assert message.role == MessageRole.SYSTEM
        assert isinstance(message.content, list)
        assert len(message.content) == 1
        for content in message.content:
            assert isinstance(content, dict)
            assert "type" in content
            assert "text" in content


def test_memory_step_json_serialization():
    """Test that memory steps can be JSON serialized without raw fields."""

    # Create a mock ChatCompletion-like object (this is what was causing the error)
    class MockChatCompletion:
        def __init__(self):
            self.id = "chatcmpl-test"
            self.choices = []

    # Create a ChatMessage with raw field containing the non-serializable object
    chat_message = ChatMessage(role=MessageRole.ASSISTANT, content="Test response", raw=MockChatCompletion())

    # Test ActionStep serialization
    action_step = ActionStep(
        step_number=1,
        timing=Timing(start_time=123456, end_time=123457),
        model_output_message=chat_message,
        model_input_messages=[chat_message],
    )

    step_dict = action_step.dict()
    json_str = json.dumps(step_dict)
    # Raw field should be present but serializable
    assert "raw" in json_str
    assert "MockChatCompletion" in json_str

    # Test PlanningStep serialization
    planning_step = PlanningStep(
        model_input_messages=[chat_message],
        model_output_message=chat_message,
        plan="Test plan",
        timing=Timing(start_time=123456, end_time=123457),
    )

    planning_dict = planning_step.dict()
    json_str = json.dumps(planning_dict)
    # Raw field should be present but serializable
    assert "raw" in json_str
    assert "MockChatCompletion" in json_str



================================================
FILE: tests/test_models.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import json
import sys
from contextlib import ExitStack
from unittest.mock import MagicMock, patch

import pytest
from huggingface_hub import ChatCompletionOutputMessage

from smolagents.default_tools import FinalAnswerTool
from smolagents.models import (
    AmazonBedrockModel,
    AzureOpenAIModel,
    ChatMessage,
    ChatMessageToolCall,
    InferenceClientModel,
    LiteLLMModel,
    LiteLLMRouterModel,
    MessageRole,
    MLXModel,
    Model,
    OpenAIModel,
    TransformersModel,
    get_clean_message_list,
    get_tool_call_from_text,
    get_tool_json_schema,
    parse_json_if_needed,
    remove_content_after_stop_sequences,
    supports_stop_parameter,
)
from smolagents.tools import tool

from .utils.markers import require_run_all


class TestModel:
    def test_prepare_completion_kwargs_parameter_precedence(self):
        """Test that self.kwargs have highest precedence and REMOVE_PARAMETER works correctly"""
        from smolagents.models import REMOVE_PARAMETER

        # Test with self.kwargs having highest precedence
        model = Model(max_tokens=100, temperature=0.5)
        completion_kwargs = model._prepare_completion_kwargs(
            messages=[ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello"}])],
            max_tokens=50,  # This should be overridden by self.kwargs
            top_p=0.9,  # This should remain from kwargs
        )

        # self.kwargs should have highest precedence
        assert completion_kwargs["max_tokens"] == 100
        assert completion_kwargs["temperature"] == 0.5
        assert completion_kwargs["top_p"] == 0.9

        # Test REMOVE_PARAMETER functionality
        model_with_removal = Model(max_tokens=REMOVE_PARAMETER, temperature=0.7)
        completion_kwargs = model_with_removal._prepare_completion_kwargs(
            messages=[ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello"}])],
            max_tokens=200,  # This should be removed by REMOVE_PARAMETER
            top_p=0.8,
        )

        # max_tokens should be removed, temperature should be set
        assert "max_tokens" not in completion_kwargs
        assert completion_kwargs["temperature"] == 0.7
        assert completion_kwargs["top_p"] == 0.8

    def test_agglomerate_stream_deltas(self):
        from smolagents.models import (
            ChatMessageStreamDelta,
            ChatMessageToolCallFunction,
            ChatMessageToolCallStreamDelta,
            TokenUsage,
            agglomerate_stream_deltas,
        )

        stream_deltas = [
            ChatMessageStreamDelta(
                content="Hi",
                tool_calls=[
                    ChatMessageToolCallStreamDelta(
                        index=0,
                        type="function",
                        function=ChatMessageToolCallFunction(arguments="", name="web_search", description=None),
                    )
                ],
                token_usage=None,
            ),
            ChatMessageStreamDelta(
                content=" everyone",
                tool_calls=[
                    ChatMessageToolCallStreamDelta(
                        index=0,
                        type="function",
                        function=ChatMessageToolCallFunction(arguments=' {"', name="web_search", description=None),
                    )
                ],
                token_usage=None,
            ),
            ChatMessageStreamDelta(
                content=", it's",
                tool_calls=[
                    ChatMessageToolCallStreamDelta(
                        index=0,
                        type="function",
                        function=ChatMessageToolCallFunction(
                            arguments='query": "current pope name and date of birth"}',
                            name="web_search",
                            description=None,
                        ),
                    )
                ],
                token_usage=None,
            ),
            ChatMessageStreamDelta(
                content="",
                tool_calls=None,
                token_usage=TokenUsage(input_tokens=1348, output_tokens=24),
            ),
        ]
        agglomerated_stream_delta = agglomerate_stream_deltas(stream_deltas)
        assert agglomerated_stream_delta.content == "Hi everyone, it's"
        assert (
            agglomerated_stream_delta.tool_calls[0].function.arguments
            == ' {"query": "current pope name and date of birth"}'
        )
        assert agglomerated_stream_delta.token_usage.total_tokens == 1372

    @pytest.mark.parametrize(
        "model_id, stop_sequences, should_contain_stop",
        [
            ("regular-model", ["stop1", "stop2"], True),  # Regular model should include stop
            ("openai/o3", ["stop1", "stop2"], False),  # o3 model should not include stop
            ("openai/o4-mini", ["stop1", "stop2"], False),  # o4-mini model should not include stop
            ("something/else/o3", ["stop1", "stop2"], False),  # Path ending with o3 should not include stop
            ("something/else/o4-mini", ["stop1", "stop2"], False),  # Path ending with o4-mini should not include stop
            ("o3", ["stop1", "stop2"], False),  # Exact o3 model should not include stop
            ("o4-mini", ["stop1", "stop2"], False),  # Exact o4-mini model should not include stop
            ("regular-model", None, False),  # None stop_sequences should not add stop parameter
        ],
    )
    def test_prepare_completion_kwargs_stop_sequences(self, model_id, stop_sequences, should_contain_stop):
        model = Model()
        model.model_id = model_id
        completion_kwargs = model._prepare_completion_kwargs(
            messages=[
                ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello"}]),
            ],
            stop_sequences=stop_sequences,
        )
        # Verify that the stop parameter is only included when appropriate
        if should_contain_stop:
            assert "stop" in completion_kwargs
            assert completion_kwargs["stop"] == stop_sequences
        else:
            assert "stop" not in completion_kwargs

    @pytest.mark.parametrize(
        "with_tools, tool_choice, expected_result",
        [
            # Default behavior: With tools but no explicit tool_choice, should default to "required"
            (True, ..., {"has_tool_choice": True, "value": "required"}),
            # Custom value: With tools and explicit tool_choice="auto"
            (True, "auto", {"has_tool_choice": True, "value": "auto"}),
            # Tool name as string
            (True, "valid_tool_function", {"has_tool_choice": True, "value": "valid_tool_function"}),
            # Tool choice as dictionary
            (
                True,
                {"type": "function", "function": {"name": "valid_tool_function"}},
                {"has_tool_choice": True, "value": {"type": "function", "function": {"name": "valid_tool_function"}}},
            ),
            # With tools but explicit None tool_choice: should exclude tool_choice
            (True, None, {"has_tool_choice": False, "value": None}),
            # Without tools: tool_choice should never be included
            (False, "required", {"has_tool_choice": False, "value": None}),
            (False, "auto", {"has_tool_choice": False, "value": None}),
            (False, None, {"has_tool_choice": False, "value": None}),
            (False, ..., {"has_tool_choice": False, "value": None}),
        ],
    )
    def test_prepare_completion_kwargs_tool_choice(self, with_tools, tool_choice, expected_result, example_tool):
        model = Model()
        kwargs = {"messages": [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello"}])]}
        if with_tools:
            kwargs["tools_to_call_from"] = [example_tool]
        if tool_choice is not ...:
            kwargs["tool_choice"] = tool_choice

        completion_kwargs = model._prepare_completion_kwargs(**kwargs)

        if expected_result["has_tool_choice"]:
            assert "tool_choice" in completion_kwargs
            assert completion_kwargs["tool_choice"] == expected_result["value"]
        else:
            assert "tool_choice" not in completion_kwargs

    def test_get_json_schema_has_nullable_args(self):
        @tool
        def get_weather(location: str, celsius: bool | None = False) -> str:
            """
            Get weather in the next days at given location.
            Secretly this tool does not care about the location, it hates the weather everywhere.

            Args:
                location: the location
                celsius: the temperature type
            """
            return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

        assert "nullable" in get_tool_json_schema(get_weather)["function"]["parameters"]["properties"]["celsius"]

    def test_chatmessage_has_model_dumps_json(self):
        message = ChatMessage("user", [{"type": "text", "text": "Hello!"}])
        data = json.loads(message.model_dump_json())
        assert data["content"] == [{"type": "text", "text": "Hello!"}]

    def test_chatmessage_from_dict_role_conversion(self):
        message_data = {
            "role": "user",
            "content": [{"type": "text", "text": "Hello!"}],
        }
        message = ChatMessage.from_dict(message_data)
        assert isinstance(message.role, MessageRole)
        assert message.role == MessageRole.USER
        assert message.role.value == "user"
        assert message.content == [{"type": "text", "text": "Hello!"}]

        message_data["role"] = MessageRole.ASSISTANT
        message2 = ChatMessage.from_dict(message_data)
        assert isinstance(message2.role, MessageRole)
        assert message2.role == MessageRole.ASSISTANT

    @pytest.mark.skipif(not sys.platform.startswith("darwin"), reason="requires macOS")
    def test_get_mlx_message_no_tool(self):
        model = MLXModel(model_id="HuggingFaceTB/SmolLM2-135M-Instruct", max_tokens=10)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        output = model(messages, stop_sequences=["great"]).content
        assert output.startswith("Hello")

    @pytest.mark.skipif(not sys.platform.startswith("darwin"), reason="requires macOS")
    def test_get_mlx_message_tricky_stop_sequence(self):
        # In this test HuggingFaceTB/SmolLM2-135M-Instruct generates the token ">'"
        # which is required to test capturing stop_sequences that have extra chars at the end.
        model = MLXModel(model_id="HuggingFaceTB/SmolLM2-135M-Instruct", max_tokens=100)
        stop_sequence = " print '>"
        messages = [
            ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": f"Please{stop_sequence}'"}]),
        ]
        # check our assumption that that ">" is followed by "'"
        assert model.tokenizer.vocab[">'"]
        assert model(messages, stop_sequences=[]).content == f"I'm ready to help you{stop_sequence}'"
        # check stop_sequence capture when output has trailing chars
        assert model(messages, stop_sequences=[stop_sequence]).content == "I'm ready to help you"

    def test_transformers_message_no_tool(self, monkeypatch):
        monkeypatch.setattr("huggingface_hub.constants.HF_HUB_DOWNLOAD_TIMEOUT", 30)  # instead of 10
        model = TransformersModel(
            model_id="HuggingFaceTB/SmolLM2-135M-Instruct",
            max_new_tokens=5,
            device_map="cpu",
            do_sample=False,
        )
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        output = model.generate(messages).content
        assert output == "Hello! I'm here"

        output = model.generate_stream(messages, stop_sequences=["great"])
        output_str = ""
        for el in output:
            output_str += el.content
        assert output_str == "Hello! I'm here"

    def test_transformers_message_vl_no_tool(self, shared_datadir, monkeypatch):
        monkeypatch.setattr("huggingface_hub.constants.HF_HUB_DOWNLOAD_TIMEOUT", 30)  # instead of 10
        import PIL.Image

        img = PIL.Image.open(shared_datadir / "000000039769.png")
        model = TransformersModel(
            model_id="llava-hf/llava-interleave-qwen-0.5b-hf",
            max_new_tokens=4,
            device_map="cpu",
            do_sample=False,
        )
        messages = [
            ChatMessage(
                role=MessageRole.USER,
                content=[{"type": "text", "text": "What is this?"}, {"type": "image", "image": img}],
            )
        ]
        output = model.generate(messages).content
        assert output == "This is a very"

        output = model.generate_stream(messages, stop_sequences=["great"])
        output_str = ""
        for el in output:
            output_str += el.content
        assert output_str == "This is a very"

    def test_parse_json_if_needed(self):
        args = "abc"
        parsed_args = parse_json_if_needed(args)
        assert parsed_args == "abc"

        args = '{"a": 3}'
        parsed_args = parse_json_if_needed(args)
        assert parsed_args == {"a": 3}

        args = "3"
        parsed_args = parse_json_if_needed(args)
        assert parsed_args == 3

        args = 3
        parsed_args = parse_json_if_needed(args)
        assert parsed_args == 3


class TestInferenceClientModel:
    def test_call_with_custom_role_conversions(self):
        custom_role_conversions = {MessageRole.USER: MessageRole.SYSTEM}
        model = InferenceClientModel(model_id="test-model", custom_role_conversions=custom_role_conversions)
        model.client = MagicMock()
        mock_response = model.client.chat_completion.return_value
        mock_response.choices[0].message = ChatCompletionOutputMessage(role=MessageRole.ASSISTANT)
        messages = [ChatMessage(role=MessageRole.USER, content="Test message")]
        _ = model(messages)
        # Verify that the role conversion was applied
        assert model.client.chat_completion.call_args.kwargs["messages"][0]["role"] == "system", (
            "role conversion should be applied"
        )

    def test_init_model_with_tokens(self):
        model = InferenceClientModel(model_id="test-model", token="abc")
        assert model.client.token == "abc"

        model = InferenceClientModel(model_id="test-model", api_key="abc")
        assert model.client.token == "abc"

        with pytest.raises(ValueError, match="Received both `token` and `api_key` arguments."):
            InferenceClientModel(model_id="test-model", token="abc", api_key="def")

    def test_structured_outputs_with_unsupported_provider(self):
        with pytest.raises(
            ValueError, match="InferenceClientModel only supports structured outputs with these providers:"
        ):
            model = InferenceClientModel(model_id="test-model", token="abc", provider="some_provider")
            model.generate(
                messages=[ChatMessage(role=MessageRole.USER, content="Hello!")],
                response_format={"type": "json_object"},
            )

    @require_run_all
    def test_get_hfapi_message_no_tool(self):
        model = InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct", max_tokens=10)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        model(messages, stop_sequences=["great"])

    @require_run_all
    def test_get_hfapi_message_no_tool_external_provider(self):
        model = InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct", provider="together", max_tokens=10)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        model(messages, stop_sequences=["great"])

    @require_run_all
    def test_get_hfapi_message_stream_no_tool(self):
        model = InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct", max_tokens=10)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        for el in model.generate_stream(messages, stop_sequences=["great"]):
            assert el.content is not None

    @require_run_all
    def test_get_hfapi_message_stream_no_tool_external_provider(self):
        model = InferenceClientModel(model_id="Qwen/Qwen2.5-Coder-32B-Instruct", provider="together", max_tokens=10)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}])]
        for el in model.generate_stream(messages, stop_sequences=["great"]):
            assert el.content is not None


class TestLiteLLMModel:
    @pytest.mark.parametrize(
        "model_id",
        [
            "groq/llama-3.3-70b",
            "cerebras/llama-3.3-70b",
            "mistral/mistral-tiny",
        ],
    )
    def test_call_different_providers_without_key(self, model_id):
        # Different litellm versions produce different error messages for missing API keys
        # This test checks for the presence of any common authentication-related error phrases
        possible_error_messages = [
            "Missing API Key",
            "Wrong API Key",
            "Invalid API Key",
            "The api_key client option must be set",
            "AuthenticationError",
            "Unauthorized",
        ]
        model = LiteLLMModel(model_id=model_id)
        messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Test message"}])]
        # Test generate method
        with pytest.raises(Exception) as e:
            model.generate(messages)
        error_message = str(e)
        assert any(possible_error_message in error_message for possible_error_message in possible_error_messages), (
            f"Error message '{error_message}' does not contain any expected phrases"
        )
        # Test generate_stream method
        with pytest.raises(Exception) as e:
            for el in model.generate_stream(messages):
                assert el.content is not None
        error_message = str(e)
        assert any(possible_error_message in error_message for possible_error_message in possible_error_messages), (
            f"Error message '{error_message}' does not contain any expected phrases"
        )

    def test_retry_on_rate_limit_error(self):
        """Test that the retry mechanism does trigger on 429 rate limit errors"""
        import time

        # Patch RETRY_WAIT to 1 second for faster testing
        mock_litellm = MagicMock()

        with (
            patch("smolagents.models.RETRY_WAIT", 0.1),
            patch("smolagents.utils.random.random", side_effect=[0.1, 0.1]),
            patch("smolagents.models.LiteLLMModel.create_client", return_value=mock_litellm),
        ):
            model = LiteLLMModel(model_id="test-model")
            messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Test message"}])]

            # Create a mock response for successful call
            mock_success_response = MagicMock()
            mock_success_response.choices = [MagicMock()]
            # Set content directly (not through model_dump)
            mock_success_response.choices[0].message.content = "Success response"
            mock_success_response.choices[0].message.role = "assistant"
            mock_success_response.choices[0].message.tool_calls = None
            mock_success_response.usage.prompt_tokens = 10
            mock_success_response.usage.completion_tokens = 20

            # Create a 429 rate limit error
            rate_limit_error = Exception("Error code: 429 - Rate limit exceeded")

            # Mock the litellm client to raise an error twice, and then succeed
            model.client.completion.side_effect = [rate_limit_error, rate_limit_error, mock_success_response]

            # Measure time to verify retry wait time
            start_time = time.time()
            result = model.generate(messages)
            elapsed_time = time.time() - start_time

            # Verify that completion was called thrice (twice failed, once succeeded)
            assert model.client.completion.call_count == 3
            assert result.content == "Success response"
            assert result.token_usage.input_tokens == 10
            assert result.token_usage.output_tokens == 20

            # Verify that the wait time was around
            # 0.22s (1st retry) [0.1 * 2.0 * (1 + 1 * 0.1)]
            # + 0.48s (2nd retry) [0.22 * 2.0 * (1 + 1 * 0.1)]
            # = 0.704s (allow some tolerance)
            assert 0.67 <= elapsed_time <= 0.73

    def test_passing_flatten_messages(self):
        model = LiteLLMModel(model_id="groq/llama-3.3-70b", flatten_messages_as_text=False)
        assert not model.flatten_messages_as_text

        model = LiteLLMModel(model_id="fal/llama-3.3-70b", flatten_messages_as_text=True)
        assert model.flatten_messages_as_text


class TestLiteLLMRouterModel:
    @pytest.mark.parametrize(
        "model_id, expected",
        [
            ("llama-3.3-70b", False),
            ("llama-3.3-70b", True),
            ("mistral-tiny", True),
        ],
    )
    def test_flatten_messages_as_text(self, model_id, expected):
        model_list = [
            {"model_name": "llama-3.3-70b", "litellm_params": {"model": "groq/llama-3.3-70b"}},
            {"model_name": "llama-3.3-70b", "litellm_params": {"model": "cerebras/llama-3.3-70b"}},
            {"model_name": "mistral-tiny", "litellm_params": {"model": "mistral/mistral-tiny"}},
        ]
        model = LiteLLMRouterModel(model_id=model_id, model_list=model_list, flatten_messages_as_text=expected)
        assert model.flatten_messages_as_text is expected

    def test_create_client(self):
        model_list = [
            {"model_name": "llama-3.3-70b", "litellm_params": {"model": "groq/llama-3.3-70b"}},
            {"model_name": "llama-3.3-70b", "litellm_params": {"model": "cerebras/llama-3.3-70b"}},
        ]
        with patch("litellm.router.Router") as mock_router:
            router_model = LiteLLMRouterModel(
                model_id="model-group-1", model_list=model_list, client_kwargs={"routing_strategy": "simple-shuffle"}
            )
            # Ensure that the Router constructor was called with the expected keyword arguments
            mock_router.assert_called_once()
            assert mock_router.call_count == 1
            assert mock_router.call_args.kwargs["model_list"] == model_list
            assert mock_router.call_args.kwargs["routing_strategy"] == "simple-shuffle"
            assert router_model.client == mock_router.return_value


class TestOpenAIModel:
    def test_client_kwargs_passed_correctly(self):
        model_id = "gpt-3.5-turbo"
        api_base = "https://api.openai.com/v1"
        api_key = "test_api_key"
        organization = "test_org"
        project = "test_project"
        client_kwargs = {"max_retries": 5}

        with patch("openai.OpenAI") as MockOpenAI:
            model = OpenAIModel(
                model_id=model_id,
                api_base=api_base,
                api_key=api_key,
                organization=organization,
                project=project,
                client_kwargs=client_kwargs,
            )
        MockOpenAI.assert_called_once_with(
            base_url=api_base, api_key=api_key, organization=organization, project=project, max_retries=5
        )
        assert model.client == MockOpenAI.return_value

    @require_run_all
    def test_streaming_tool_calls(self):
        model = OpenAIModel(model_id="gpt-4o-mini")
        messages = [
            ChatMessage(
                role=MessageRole.USER,
                content=[
                    {
                        "type": "text",
                        "text": "Hello! Please return the final answer 'blob' and the final answer 'blob2' in two parallel tool calls",
                    }
                ],
            ),
        ]
        for el in model.generate_stream(messages, tools_to_call_from=[FinalAnswerTool()]):
            if el.tool_calls:
                assert el.tool_calls[0].function.name == "final_answer"
                args = el.tool_calls[0].function.arguments
                if len(el.tool_calls) > 1:
                    assert el.tool_calls[1].function.name == "final_answer"
                    args2 = el.tool_calls[1].function.arguments
        assert args == '{"answer": "blob"}'
        assert args2 == '{"answer": "blob2"}'

    def test_stop_sequence_cutting_for_o4_mini(self):
        """Test that stop sequences are cut a posteriori for models that don't support stop parameter"""
        # Create a mock response that contains a stop sequence in the middle
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.role = "assistant"
        mock_response.choices[0].message.content = "This is some text<STOP>and this should be removed"
        mock_response.choices[0].message.tool_calls = None
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 20

        with patch("openai.OpenAI") as MockOpenAI:
            mock_client = MagicMock()
            MockOpenAI.return_value = mock_client
            mock_client.chat.completions.create.return_value = mock_response

            model = OpenAIModel(model_id="o4-mini")
            messages = [ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello"}])]
            result = model.generate(messages, stop_sequences=["<STOP>"])

            # Verify the stop sequence was removed
            assert result.content == "This is some text"
            assert "<STOP>" not in result.content
            assert "and this should be removed" not in result.content


class TestAmazonBedrockModel:
    def test_client_for_bedrock(self):
        model_id = "us.amazon.nova-pro-v1:0"

        with patch("boto3.client") as MockBoto3:
            model = AmazonBedrockModel(
                model_id=model_id,
            )

        assert model.client == MockBoto3.return_value


class TestAzureOpenAIModel:
    def test_client_kwargs_passed_correctly(self):
        model_id = "gpt-3.5-turbo"
        api_key = "test_api_key"
        api_version = "2023-12-01-preview"
        azure_endpoint = "https://example-resource.azure.openai.com/"
        organization = "test_org"
        project = "test_project"
        client_kwargs = {"max_retries": 5}

        with patch("openai.OpenAI") as MockOpenAI, patch("openai.AzureOpenAI") as MockAzureOpenAI:
            model = AzureOpenAIModel(
                model_id=model_id,
                api_key=api_key,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                organization=organization,
                project=project,
                client_kwargs=client_kwargs,
            )
        assert MockOpenAI.call_count == 0
        MockAzureOpenAI.assert_called_once_with(
            base_url=None,
            api_key=api_key,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            organization=organization,
            project=project,
            max_retries=5,
        )
        assert model.client == MockAzureOpenAI.return_value


class TestTransformersModel:
    @pytest.mark.parametrize(
        "patching",
        [
            [
                (
                    "transformers.AutoModelForImageTextToText.from_pretrained",
                    {"side_effect": ValueError("Unrecognized configuration class")},
                ),
                ("transformers.AutoModelForCausalLM.from_pretrained", {}),
                ("transformers.AutoTokenizer.from_pretrained", {}),
            ],
            [
                ("transformers.AutoModelForImageTextToText.from_pretrained", {}),
                ("transformers.AutoProcessor.from_pretrained", {}),
            ],
        ],
    )
    def test_init(self, patching):
        with ExitStack() as stack:
            mocks = {target: stack.enter_context(patch(target, **kwargs)) for target, kwargs in patching}
            model = TransformersModel(
                model_id="test-model", device_map="cpu", torch_dtype="float16", trust_remote_code=True
            )
        assert model.model_id == "test-model"
        if "transformers.AutoTokenizer.from_pretrained" in mocks:
            assert model.model == mocks["transformers.AutoModelForCausalLM.from_pretrained"].return_value
            assert mocks["transformers.AutoModelForCausalLM.from_pretrained"].call_args.kwargs == {
                "device_map": "cpu",
                "torch_dtype": "float16",
                "trust_remote_code": True,
            }
            assert model.tokenizer == mocks["transformers.AutoTokenizer.from_pretrained"].return_value
            assert mocks["transformers.AutoTokenizer.from_pretrained"].call_args.args == ("test-model",)
            assert mocks["transformers.AutoTokenizer.from_pretrained"].call_args.kwargs == {"trust_remote_code": True}
        elif "transformers.AutoProcessor.from_pretrained" in mocks:
            assert model.model == mocks["transformers.AutoModelForImageTextToText.from_pretrained"].return_value
            assert mocks["transformers.AutoModelForImageTextToText.from_pretrained"].call_args.kwargs == {
                "device_map": "cpu",
                "torch_dtype": "float16",
                "trust_remote_code": True,
            }
            assert model.processor == mocks["transformers.AutoProcessor.from_pretrained"].return_value
            assert mocks["transformers.AutoProcessor.from_pretrained"].call_args.args == ("test-model",)
            assert mocks["transformers.AutoProcessor.from_pretrained"].call_args.kwargs == {"trust_remote_code": True}


def test_get_clean_message_list_basic():
    messages = [
        ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}]),
        ChatMessage(role=MessageRole.ASSISTANT, content=[{"type": "text", "text": "Hi there!"}]),
    ]
    result = get_clean_message_list(messages)
    assert len(result) == 2
    assert result[0]["role"] == "user"
    assert result[0]["content"][0]["text"] == "Hello!"
    assert result[1]["role"] == "assistant"
    assert result[1]["content"][0]["text"] == "Hi there!"


@pytest.mark.parametrize(
    "messages,expected_roles,expected_texts",
    [
        (
            [
                {"role": "user", "content": [{"type": "text", "text": "Hello!"}]},
                {"role": "assistant", "content": [{"type": "text", "text": "Hi there!"}]},
            ],
            ["user", "assistant"],
            ["Hello!", "Hi there!"],
        ),
        (
            [
                {"role": "user", "content": [{"type": "text", "text": "How are you?"}]},
            ],
            ["user"],
            ["How are you?"],
        ),
    ],
)
def test_get_clean_message_list_with_dicts(messages, expected_roles, expected_texts):
    result = get_clean_message_list(messages)
    assert len(result) == len(messages)
    for i, msg in enumerate(result):
        assert msg["role"] == expected_roles[i]
        assert msg["content"][0]["text"] == expected_texts[i]


def test_get_clean_message_list_role_conversions():
    messages = [
        ChatMessage(role=MessageRole.TOOL_CALL, content=[{"type": "text", "text": "Calling tool..."}]),
        ChatMessage(role=MessageRole.TOOL_RESPONSE, content=[{"type": "text", "text": "Tool response"}]),
    ]
    result = get_clean_message_list(messages, role_conversions={"tool-call": "assistant", "tool-response": "user"})
    assert len(result) == 2
    assert result[0]["role"] == "assistant"
    assert result[0]["content"][0]["text"] == "Calling tool..."
    assert result[1]["role"] == "user"
    assert result[1]["content"][0]["text"] == "Tool response"


def test_remove_content_after_stop_sequences():
    content = "Hello<code>world!"
    stop_sequences = ["<code>"]
    removed_content = remove_content_after_stop_sequences(content, stop_sequences)
    assert removed_content == "Hello"


def test_remove_content_after_stop_sequences_handles_none():
    # Test with None stop sequence
    content = "Hello world!"
    removed_content = remove_content_after_stop_sequences(content, None)
    assert removed_content == content

    # Test with None content
    removed_content = remove_content_after_stop_sequences(None, ["<code>"])
    assert removed_content is None


@pytest.mark.parametrize(
    "convert_images_to_image_urls, expected_clean_message",
    [
        (
            False,
            dict(
                role=MessageRole.USER,
                content=[
                    {"type": "image", "image": "encoded_image"},
                    {"type": "image", "image": "second_encoded_image"},
                ],
            ),
        ),
        (
            True,
            dict(
                role=MessageRole.USER,
                content=[
                    {"type": "image_url", "image_url": {"url": "data:image/png;base64,encoded_image"}},
                    {"type": "image_url", "image_url": {"url": "data:image/png;base64,second_encoded_image"}},
                ],
            ),
        ),
    ],
)
def test_get_clean_message_list_image_encoding(convert_images_to_image_urls, expected_clean_message):
    message = ChatMessage(
        role=MessageRole.USER,
        content=[{"type": "image", "image": b"image_data"}, {"type": "image", "image": b"second_image_data"}],
    )
    with patch("smolagents.models.encode_image_base64") as mock_encode:
        mock_encode.side_effect = ["encoded_image", "second_encoded_image"]
        result = get_clean_message_list([message], convert_images_to_image_urls=convert_images_to_image_urls)
        mock_encode.assert_any_call(b"image_data")
        mock_encode.assert_any_call(b"second_image_data")
        assert len(result) == 1
        assert result[0] == expected_clean_message


def test_get_clean_message_list_flatten_messages_as_text():
    messages = [
        ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "Hello!"}]),
        ChatMessage(role=MessageRole.USER, content=[{"type": "text", "text": "How are you?"}]),
    ]
    result = get_clean_message_list(messages, flatten_messages_as_text=True)
    assert len(result) == 1
    assert result[0]["role"] == "user"
    assert result[0]["content"] == "Hello!\nHow are you?"


@pytest.mark.parametrize(
    "model_class, model_kwargs, patching, expected_flatten_messages_as_text",
    [
        (AzureOpenAIModel, {}, ("openai.AzureOpenAI", {}), False),
        (InferenceClientModel, {}, ("huggingface_hub.InferenceClient", {}), False),
        (LiteLLMModel, {}, None, False),
        (LiteLLMModel, {"model_id": "ollama"}, None, True),
        (LiteLLMModel, {"model_id": "groq"}, None, True),
        (LiteLLMModel, {"model_id": "cerebras"}, None, True),
        (MLXModel, {}, ("mlx_lm.load", {"return_value": (MagicMock(), MagicMock())}), True),
        (OpenAIModel, {}, ("openai.OpenAI", {}), False),
        (OpenAIModel, {"flatten_messages_as_text": True}, ("openai.OpenAI", {}), True),
        (
            TransformersModel,
            {},
            [
                (
                    "transformers.AutoModelForImageTextToText.from_pretrained",
                    {"side_effect": ValueError("Unrecognized configuration class")},
                ),
                ("transformers.AutoModelForCausalLM.from_pretrained", {}),
                ("transformers.AutoTokenizer.from_pretrained", {}),
            ],
            True,
        ),
        (
            TransformersModel,
            {},
            [
                ("transformers.AutoModelForImageTextToText.from_pretrained", {}),
                ("transformers.AutoProcessor.from_pretrained", {}),
            ],
            False,
        ),
    ],
)
def test_flatten_messages_as_text_for_all_models(
    model_class, model_kwargs, patching, expected_flatten_messages_as_text
):
    with ExitStack() as stack:
        if isinstance(patching, list):
            for target, kwargs in patching:
                stack.enter_context(patch(target, **kwargs))
        elif patching:
            target, kwargs = patching
            stack.enter_context(patch(target, **kwargs))

        model = model_class(**{"model_id": "test-model", **model_kwargs})
    assert model.flatten_messages_as_text is expected_flatten_messages_as_text, f"{model_class.__name__} failed"


@pytest.mark.parametrize(
    "model_id,expected",
    [
        # Unsupported base models
        ("o3", False),
        ("o4-mini", False),
        ("gpt-5.1", False),
        ("gpt-5.2", False),
        ("gpt-5", False),
        ("gpt-5-mini", False),
        ("gpt-5-nano", False),
        ("gpt-5-turbo", False),
        ("gpt-5.2-mini", False),
        ("grok-4", False),
        ("grok-4-latest", False),
        ("grok-4.1", False),
        ("grok-3", False),
        ("grok-3-mini", False),
        ("grok-code-fast-1", False),
        # Unsupported versioned models
        ("o4-mini-2025-04-16", False),
        ("gpt-5-2025-01-01", False),
        # Unsupported models with path prefixes
        ("openai/o3", False),
        ("openai/o4-mini", False),
        ("openai/o3-2025-04-16", False),
        ("openai/o4-mini-2025-04-16", False),
        ("openai/gpt-5.2", False),
        ("openai/gpt-5.2-mini", False),
        ("openai/gpt-5.2-2025-01-01", False),
        ("oci/xai.grok-4", False),
        ("oci/xai.grok-3-mini", False),
        # Supported models
        ("o3-mini", True),
        ("gpt-4", True),
        ("claude-3-5-sonnet", True),
        ("mistral-large", True),
        # Supported models with path prefixes
        ("openai/gpt-4", True),
        ("anthropic/claude-3-5-sonnet", True),
        ("anthropic/claude-opus-4-5", True),
        ("mistralai/mistral-large", True),
        # Edge cases
        ("", True),  # Empty string doesn't match pattern
        ("o3x", True),  # Not exactly o3
        ("o4x", True),  # Not exactly o4
        ("gpt-5x", False),
        ("gpt-50", False),
        ("o3_mini", True),  # Not o3-mini format
        ("prefix-o3", True),  # o3 not at start
    ],
)
def test_supports_stop_parameter(model_id, expected):
    """Test the supports_stop_parameter function with various model IDs"""
    assert supports_stop_parameter(model_id) == expected, f"Failed for model_id: {model_id}"


class TestGetToolCallFromText:
    @pytest.fixture(autouse=True)
    def mock_uuid4(self):
        with patch("uuid.uuid4", return_value="test-uuid"):
            yield

    def test_get_tool_call_from_text_basic(self):
        text = '{"name": "weather_tool", "arguments": "New York"}'
        result = get_tool_call_from_text(text, "name", "arguments")
        assert isinstance(result, ChatMessageToolCall)
        assert result.id == "test-uuid"
        assert result.type == "function"
        assert result.function.name == "weather_tool"
        assert result.function.arguments == "New York"

    def test_get_tool_call_from_text_name_key_missing(self):
        text = '{"action": "weather_tool", "arguments": "New York"}'
        with pytest.raises(ValueError) as exc_info:
            get_tool_call_from_text(text, "name", "arguments")
        error_msg = str(exc_info.value)
        assert "Tool call needs to have a key 'name'" in error_msg
        assert "'action', 'arguments'" in error_msg

    def test_get_tool_call_from_text_json_object_args(self):
        text = '{"name": "weather_tool", "arguments": {"city": "New York"}}'
        result = get_tool_call_from_text(text, "name", "arguments")
        assert result.function.arguments == {"city": "New York"}

    def test_get_tool_call_from_text_json_string_args(self):
        text = '{"name": "weather_tool", "arguments": "{\\"city\\": \\"New York\\"}"}'
        result = get_tool_call_from_text(text, "name", "arguments")
        assert result.function.arguments == {"city": "New York"}

    def test_get_tool_call_from_text_missing_args(self):
        text = '{"name": "weather_tool"}'
        result = get_tool_call_from_text(text, "name", "arguments")
        assert result.function.arguments is None

    def test_get_tool_call_from_text_custom_keys(self):
        text = '{"tool": "weather_tool", "params": "New York"}'
        result = get_tool_call_from_text(text, "tool", "params")
        assert result.function.name == "weather_tool"
        assert result.function.arguments == "New York"

    def test_get_tool_call_from_text_numeric_args(self):
        text = '{"name": "calculator", "arguments": 42}'
        result = get_tool_call_from_text(text, "name", "arguments")
        assert result.function.name == "calculator"
        assert result.function.arguments == 42


@pytest.mark.parametrize(
    "model_class,model_id",
    [
        (LiteLLMModel, "gpt-4o-mini"),
        (OpenAIModel, "gpt-4o-mini"),
    ],
)
def test_tool_calls_json_serialization(model_class, model_id):
    """Test that tool_calls from various API models (Pydantic, dataclass, dict) are properly converted to dataclasses and can be JSON serialized.
    This tests the horizontal fix that ensures all models (LiteLLM, OpenAI, InferenceClient, AmazonBedrock)
    properly convert tool_calls to dataclasses regardless of the source format (Pydantic models, dataclasses, or dicts).
    """
    tool_arguments = "test_result"
    messages = [
        ChatMessage(
            role=MessageRole.USER,
            content=[
                {
                    "type": "text",
                    "text": "Hello! Please return the final answer 'hi there' in a tool call",
                }
            ],
        ),
    ]

    if model_class == OpenAIModel:
        from openai.types.chat.chat_completion import ChatCompletion, Choice
        from openai.types.chat.chat_completion_message import ChatCompletionMessage
        from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall, Function
        from openai.types.completion_usage import CompletionUsage

        response = ChatCompletion(
            id="chatcmpl-test",
            created=0,
            model="gpt-4o-mini-2024-07-18",
            object="chat.completion",
            choices=[
                Choice(
                    finish_reason="tool_calls",
                    index=0,
                    logprobs=None,
                    message=ChatCompletionMessage(
                        role="assistant",
                        content=None,
                        tool_calls=[
                            ChatCompletionMessageToolCall(
                                id="call_test",
                                type="function",
                                function=Function(name="final_answer", arguments=tool_arguments),
                            )
                        ],
                    ),
                )
            ],
            usage=CompletionUsage(prompt_tokens=69, completion_tokens=15, total_tokens=84),
        )
        client = MagicMock()
        client.chat.completions.create.return_value = response
        create_call = client.chat.completions.create
        patch_target = "smolagents.models.OpenAIModel.create_client"
    elif model_class == LiteLLMModel:
        from litellm.types.utils import ChatCompletionMessageToolCall, Choices, Function, Message, ModelResponse, Usage

        response = ModelResponse(
            id="chatcmpl-test",
            created=0,
            object="chat.completion",
            choices=[
                Choices(
                    finish_reason="tool_calls",
                    index=0,
                    message=Message(
                        role="assistant",
                        content=None,
                        tool_calls=[
                            ChatCompletionMessageToolCall(
                                id="call_test",
                                type="function",
                                function=Function(name="final_answer", arguments=tool_arguments),
                            )
                        ],
                        function_call=None,
                        provider_specific_fields={"refusal": None, "annotations": []},
                    ),
                )
            ],
            usage=Usage(prompt_tokens=69, completion_tokens=15, total_tokens=84),
            model="gpt-4o-mini-2024-07-18",
        )
        client = MagicMock()
        client.completion.return_value = response
        create_call = client.completion
        patch_target = "smolagents.models.LiteLLMModel.create_client"
    else:
        raise ValueError(f"Unexpected model class: {model_class}")

    with patch(patch_target, return_value=client):
        model = model_class(model_id=model_id)
        result = model.generate(messages, tools_to_call_from=[FinalAnswerTool()])

    assert create_call.call_count == 1

    # Verify tool_calls are converted to dataclasses
    assert result.tool_calls is not None
    assert len(result.tool_calls) > 0
    assert isinstance(result.tool_calls[0], ChatMessageToolCall)

    # The critical test: verify JSON serialization works
    json_str = result.model_dump_json()
    data = json.loads(json_str)
    assert "tool_calls" in data
    assert len(data["tool_calls"]) > 0
    assert data["tool_calls"][0]["function"]["name"] == "final_answer"
    assert data["tool_calls"][0]["function"]["arguments"] == "test_result"



================================================
FILE: tests/test_monitoring.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest

import PIL.Image
import pytest

from smolagents import (
    CodeAgent,
    ToolCallingAgent,
    stream_to_gradio,
)
from smolagents.memory import ActionStep, AgentMemory
from smolagents.models import (
    ChatMessage,
    ChatMessageToolCall,
    ChatMessageToolCallFunction,
    MessageRole,
    Model,
    TokenUsage,
)
from smolagents.monitoring import AgentLogger


class FakeLLMModel(Model):
    def generate(self, prompt, tools_to_call_from=None, **kwargs):
        if tools_to_call_from is not None:
            return ChatMessage(
                role=MessageRole.ASSISTANT,
                content="I will call the final_answer tool.",
                tool_calls=[
                    ChatMessageToolCall(
                        id="fake_id",
                        type="function",
                        function=ChatMessageToolCallFunction(
                            name="final_answer", arguments={"answer": "This is the final answer."}
                        ),
                    )
                ],
                token_usage=TokenUsage(input_tokens=10, output_tokens=20),
            )
        else:
            return ChatMessage(
                role=MessageRole.ASSISTANT,
                content="""<code>
final_answer('This is the final answer.')
</code>""",
                token_usage=TokenUsage(input_tokens=10, output_tokens=20),
            )


class MonitoringTester(unittest.TestCase):
    def test_code_agent_metrics_max_steps(self):
        class FakeLLMModelMalformedAnswer(Model):
            def generate(self, prompt, **kwargs):
                return ChatMessage(
                    role=MessageRole.ASSISTANT,
                    content="Malformed answer",
                    token_usage=TokenUsage(input_tokens=10, output_tokens=20),
                )

        agent = CodeAgent(
            tools=[],
            model=FakeLLMModelMalformedAnswer(),
            max_steps=1,
        )

        agent.run("Fake task")

        self.assertEqual(agent.monitor.total_input_token_count, 20)
        self.assertEqual(agent.monitor.total_output_token_count, 40)

    def test_code_agent_metrics_generation_error(self):
        class FakeLLMModelGenerationException(Model):
            def generate(self, prompt, **kwargs):
                raise Exception("Cannot generate")

        agent = CodeAgent(
            tools=[],
            model=FakeLLMModelGenerationException(),
            max_steps=1,
        )
        with pytest.raises(Exception) as e:
            agent.run("Fake task")
        assert "Cannot generate" in str(e.value)

    def test_streaming_agent_text_output(self):
        agent = CodeAgent(
            tools=[],
            model=FakeLLMModel(),
            max_steps=1,
            planning_interval=2,
        )

        # Use stream_to_gradio to capture the output
        outputs = list(stream_to_gradio(agent, task="Test task"))

        self.assertEqual(len(outputs), 11)
        plan_message = outputs[1]
        self.assertEqual(plan_message.role, "assistant")
        self.assertIn("<code>", plan_message.content)
        final_message = outputs[-1]
        self.assertEqual(final_message.role, "assistant")
        self.assertIn("This is the final answer.", final_message.content)

    def test_streaming_agent_image_output(self):
        class FakeLLMModelImage(Model):
            def generate(self, prompt, **kwargs):
                return ChatMessage(
                    role=MessageRole.ASSISTANT,
                    content="I will call the final_answer tool.",
                    tool_calls=[
                        ChatMessageToolCall(
                            id="fake_id",
                            type="function",
                            function=ChatMessageToolCallFunction(name="final_answer", arguments={"answer": "image"}),
                        )
                    ],
                )

        agent = ToolCallingAgent(
            tools=[],
            model=FakeLLMModelImage(),
            max_steps=1,
            verbosity_level=100,
        )

        # Use stream_to_gradio to capture the output
        outputs = list(
            stream_to_gradio(
                agent,
                task="Test task",
                additional_args=dict(image=PIL.Image.new("RGB", (100, 100))),
            )
        )

        self.assertEqual(len(outputs), 7)
        final_message = outputs[-1]
        self.assertEqual(final_message.role, "assistant")
        self.assertIsInstance(final_message.content, dict)
        self.assertEqual(final_message.content["mime_type"], "image/png")

    def test_streaming_with_agent_error(self):
        class DummyModel(Model):
            def generate(self, prompt, **kwargs):
                return ChatMessage(role=MessageRole.ASSISTANT, content="Malformed call")

        agent = CodeAgent(
            tools=[],
            model=DummyModel(),
            max_steps=1,
        )

        # Use stream_to_gradio to capture the output
        outputs = list(stream_to_gradio(agent, task="Test task"))

        self.assertEqual(len(outputs), 11)
        final_message = outputs[-1]
        self.assertEqual(final_message.role, "assistant")
        self.assertIn("Malformed call", final_message.content)


@pytest.mark.parametrize("agent_class", [CodeAgent, ToolCallingAgent])
def test_code_agent_metrics(agent_class):
    agent = agent_class(
        tools=[],
        model=FakeLLMModel(),
        max_steps=1,
    )
    agent.run("Fake task")

    assert agent.monitor.total_input_token_count == 10
    assert agent.monitor.total_output_token_count == 20


class ReplayTester(unittest.TestCase):
    def test_replay_with_chatmessage(self):
        """Regression test for dict(message) to message.dict() fix"""
        logger = AgentLogger()
        memory = AgentMemory(system_prompt="test")
        step = ActionStep(step_number=1, timing=0)
        step.model_input_messages = [ChatMessage(role=MessageRole.USER, content="Hello")]
        memory.steps.append(step)

        try:
            memory.replay(logger, detailed=True)
        except TypeError as e:
            self.fail(f"Replay raised an error: {e}")



================================================
FILE: tests/test_remote_executors.py
================================================
import importlib
import io
from textwrap import dedent
from unittest.mock import MagicMock, patch

import docker
import PIL.Image
import pytest
from rich.console import Console

from smolagents.default_tools import FinalAnswerTool, WikipediaSearchTool
from smolagents.local_python_executor import CodeOutput
from smolagents.monitoring import AgentLogger, LogLevel
from smolagents.remote_executors import (
    BlaxelExecutor,
    DockerExecutor,
    E2BExecutor,
    ModalExecutor,
    RemotePythonExecutor,
    WasmExecutor,
)
from smolagents.utils import AgentError

from .utils.markers import require_run_all


class TestRemotePythonExecutor:
    def test_send_tools_empty_tools(self):
        executor = RemotePythonExecutor(additional_imports=[], logger=MagicMock())
        executor.run_code_raise_errors = MagicMock()
        executor.send_tools({})
        assert executor.run_code_raise_errors.call_count == 1
        # No new packages should be installed
        assert "!pip install" not in executor.run_code_raise_errors.call_args.args[0]

    def test_send_variables_with_empty_dict_is_noop(self):
        executor = RemotePythonExecutor(additional_imports=[], logger=MagicMock())
        executor.run_code_raise_errors = MagicMock()
        executor.send_variables({})
        assert executor.run_code_raise_errors.call_count == 0

    @require_run_all
    def test_send_tools_with_default_wikipedia_search_tool(self):
        tool = WikipediaSearchTool()
        executor = RemotePythonExecutor(additional_imports=[], logger=MagicMock())
        executor.run_code_raise_errors = MagicMock()
        executor.send_tools({"wikipedia_search": tool})
        assert executor.run_code_raise_errors.call_count == 2
        assert "!pip install wikipedia-api" == executor.run_code_raise_errors.call_args_list[0].args[0]
        assert "class WikipediaSearchTool(Tool)" in executor.run_code_raise_errors.call_args_list[1].args[0]


class TestE2BExecutorUnit:
    def test_e2b_executor_instantiation(self):
        logger = MagicMock()
        with patch("e2b_code_interpreter.Sandbox") as mock_sandbox:
            mock_sandbox.return_value.commands.run.return_value.error = None
            mock_sandbox.return_value.run_code.return_value.error = None
            # Also set up v2 path in case Sandbox.create is used
            mock_sandbox.create.return_value.commands.run.return_value.error = None
            mock_sandbox.create.return_value.run_code.return_value.error = None
            executor = E2BExecutor(
                additional_imports=[], logger=logger, api_key="dummy-api-key", template="dummy-template-id", timeout=60
            )
        assert isinstance(executor, E2BExecutor)
        assert executor.logger == logger
        # Support both e2b v1 (Sandbox(...)) and v2 (Sandbox.create(...))
        if mock_sandbox.create.called:
            sandbox_obj = mock_sandbox.create.return_value
            called_ctor = mock_sandbox.create
        else:
            sandbox_obj = mock_sandbox.return_value
            called_ctor = mock_sandbox
        assert executor.sandbox == sandbox_obj
        assert called_ctor.call_count == 1
        assert called_ctor.call_args.kwargs == {
            "api_key": "dummy-api-key",
            "template": "dummy-template-id",
            "timeout": 60,
        }

    def test_cleanup(self):
        """Test that the cleanup method properly shuts down the sandbox"""
        logger = MagicMock()
        with patch("e2b_code_interpreter.Sandbox") as mock_sandbox:
            # Setup mock
            mock_sandbox.return_value.kill = MagicMock()
            # Also set up v2 path in case Sandbox.create is used
            mock_sandbox.create.return_value.kill = MagicMock()

            # Create executor
            executor = E2BExecutor(additional_imports=[], logger=logger, api_key="dummy-api-key")

            # Call cleanup
            executor.cleanup()

            # Verify sandbox was killed
            if mock_sandbox.create.called:
                mock_sandbox.create.return_value.kill.assert_called_once()
            else:
                mock_sandbox.return_value.kill.assert_called_once()
            assert logger.log.call_count >= 2  # Should log start and completion messages


@pytest.fixture
def e2b_executor():
    executor = E2BExecutor(
        additional_imports=["pillow", "numpy"],
        logger=AgentLogger(LogLevel.INFO, Console(force_terminal=False, file=io.StringIO())),
    )
    yield executor
    executor.cleanup()


@require_run_all
class TestE2BExecutorIntegration:
    @pytest.fixture(autouse=True)
    def set_executor(self, e2b_executor):
        self.executor = e2b_executor

    @pytest.mark.parametrize(
        "code_action, expected_result",
        [
            (
                dedent('''
                    final_answer("""This is
                    a multiline
                    final answer""")
                '''),
                "This is\na multiline\nfinal answer",
            ),
            (
                dedent("""
                    text = '''Text containing
                    final_answer(5)
                    '''
                    final_answer(text)
                """),
                "Text containing\nfinal_answer(5)\n",
            ),
            (
                dedent("""
                    num = 2
                    if num == 1:
                        final_answer("One")
                    elif num == 2:
                        final_answer("Two")
                """),
                "Two",
            ),
        ],
    )
    def test_final_answer_patterns(self, code_action, expected_result):
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == expected_result

    def test_custom_final_answer(self):
        class CustomFinalAnswerTool(FinalAnswerTool):
            def forward(self, answer: str) -> str:
                return "CUSTOM" + answer

        self.executor.send_tools({"final_answer": CustomFinalAnswerTool()})
        code_action = dedent("""
            final_answer(answer="_answer")
        """)
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == "CUSTOM_answer"

    def test_custom_final_answer_with_custom_inputs(self):
        class CustomFinalAnswerToolWithCustomInputs(FinalAnswerTool):
            inputs = {
                "answer1": {"type": "string", "description": "First part of the answer."},
                "answer2": {"type": "string", "description": "Second part of the answer."},
            }

            def forward(self, answer1: str, answer2: str) -> str:
                return answer1 + "CUSTOM" + answer2

        self.executor.send_tools({"final_answer": CustomFinalAnswerToolWithCustomInputs()})
        code_action = dedent("""
            final_answer(
                answer1="answer1_",
                answer2="_answer2"
            )
        """)
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == "answer1_CUSTOM_answer2"


class TestDockerExecutorUnit:
    def test_cleanup(self):
        """Test that cleanup properly stops and removes the container"""
        logger = MagicMock()
        with (
            patch("docker.from_env") as mock_docker_client,
            patch("requests.post") as mock_post,
            patch("websocket.create_connection"),
        ):
            # Setup mocks
            mock_container = MagicMock()
            mock_container.status = "running"
            mock_container.short_id = "test123"

            mock_docker_client.return_value.containers.run.return_value = mock_container
            mock_docker_client.return_value.images.get.return_value = MagicMock()

            mock_post.return_value.status_code = 201
            mock_post.return_value.json.return_value = {"id": "test-kernel-id"}

            # Create executor
            executor = DockerExecutor(additional_imports=[], logger=logger, build_new_image=False)

            # Call cleanup
            executor.cleanup()

            # Verify container was stopped and removed
            mock_container.stop.assert_called_once()
            mock_container.remove.assert_called_once()


class CommonDockerExecutorIntegration:
    @pytest.fixture(autouse=True)
    def set_executor(self, custom_executor):
        self.executor = custom_executor

    def test_state_persistence(self):
        """Test that variables and imports form one snippet persist in the next"""
        code_action = "import numpy as np; a = 2"
        self.executor(code_action)

        code_action = "print(np.sqrt(a))"
        code_output = self.executor(code_action)
        assert "1.41421" in code_output.logs

    def test_execute_output(self):
        """Test execution that returns a string"""
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code_action = 'final_answer("This is the final answer")'
        code_output = self.executor(code_action)
        assert code_output.output == "This is the final answer", "Result should be 'This is the final answer'"

    def test_execute_multiline_output(self):
        """Test execution that returns a string"""
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code_action = 'result = "This is the final answer"\nfinal_answer(result)'
        code_output = self.executor(code_action)
        assert code_output.output == "This is the final answer", "Result should be 'This is the final answer'"

    def test_execute_image_output(self):
        """Test execution that returns a base64 image"""
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code_action = dedent("""
            import base64
            from PIL import Image
            from io import BytesIO
            image = Image.new("RGB", (10, 10), (255, 0, 0))
            final_answer(image)
        """)
        code_output = self.executor(code_action)
        assert isinstance(code_output.output, PIL.Image.Image), "Result should be a PIL Image"

    def test_syntax_error_handling(self):
        """Test handling of syntax errors"""
        code_action = 'print("Missing Parenthesis'  # Syntax error
        with pytest.raises(AgentError) as exception_info:
            self.executor(code_action)
        assert "SyntaxError" in str(exception_info.value), "Should raise a syntax error"

    @pytest.mark.parametrize(
        "code_action, expected_result",
        [
            (
                dedent('''
                    final_answer("""This is
                    a multiline
                    final answer""")
                '''),
                "This is\na multiline\nfinal answer",
            ),
            (
                dedent("""
                    text = '''Text containing
                    final_answer(5)
                    '''
                    final_answer(text)
                """),
                "Text containing\nfinal_answer(5)\n",
            ),
            (
                dedent("""
                    num = 2
                    if num == 1:
                        final_answer("One")
                    elif num == 2:
                        final_answer("Two")
                """),
                "Two",
            ),
        ],
    )
    def test_final_answer_patterns(self, code_action, expected_result):
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == expected_result

    def test_custom_final_answer(self):
        class CustomFinalAnswerTool(FinalAnswerTool):
            def forward(self, answer: str) -> str:
                return "CUSTOM" + answer

        self.executor.send_tools({"final_answer": CustomFinalAnswerTool()})
        code_action = dedent("""
            final_answer(answer="_answer")
        """)
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == "CUSTOM_answer"

    def test_custom_final_answer_with_custom_inputs(self):
        class CustomFinalAnswerToolWithCustomInputs(FinalAnswerTool):
            inputs = {
                "answer1": {"type": "string", "description": "First part of the answer."},
                "answer2": {"type": "string", "description": "Second part of the answer."},
            }

            def forward(self, answer1: str, answer2: str) -> str:
                return answer1 + "CUSTOM" + answer2

        self.executor.send_tools({"final_answer": CustomFinalAnswerToolWithCustomInputs()})
        code_action = dedent("""
            final_answer(
                answer1="answer1_",
                answer2="_answer2"
            )
        """)
        code_output = self.executor(code_action)
        assert code_output.is_final_answer is True
        assert code_output.output == "answer1_CUSTOM_answer2"


@require_run_all
class TestDockerExecutorIntegration(CommonDockerExecutorIntegration):
    @pytest.fixture
    def custom_executor(self):
        executor = DockerExecutor(
            additional_imports=["pillow", "numpy"],
            logger=AgentLogger(LogLevel.INFO, Console(force_terminal=False, file=io.StringIO())),
        )
        yield executor
        executor.delete()

    def test_initialization(self):
        """Check if DockerExecutor initializes without errors"""
        assert self.executor.container is not None, "Container should be initialized"

    def test_cleanup_on_deletion(self):
        """Test if Docker container stops and removes on deletion"""
        container_id = self.executor.container.id
        self.executor.delete()  # Trigger cleanup

        client = docker.from_env()
        containers = [c.id for c in client.containers.list(all=True)]
        assert container_id not in containers, "Container should be removed"


@require_run_all
class TestModalExecutorIntegration(CommonDockerExecutorIntegration):
    @pytest.fixture
    def custom_executor(self):
        executor = ModalExecutor(
            additional_imports=["pillow", "numpy"],
            logger=AgentLogger(LogLevel.INFO, Console(force_terminal=False, file=io.StringIO())),
        )
        yield executor
        executor.delete()


class TestModalExecutorUnit:
    @patch("smolagents.remote_executors._websocket_run_code_raise_errors")
    @patch("requests.post")
    @patch("requests.get")
    @patch("websocket.create_connection")
    @patch("modal.App.lookup")
    @patch("modal.Sandbox.create")
    def test_sandbox_lifecycle(
        self, mock_sandbox_create, mock_app_lookup, mock_create_connection, mock_get, mock_post, mock_run_code_raises
    ):
        """Test that sandbox is created with the correct kwargs and cleaned up correctly."""
        modal = pytest.importorskip("modal")
        port = 8889

        logger = MagicMock()
        mock_sandbox = MagicMock()
        tunnel_mock = MagicMock()
        tunnel_mock.host = "r4234.modal.host"
        mock_sandbox.tunnels.return_value = {port: tunnel_mock}

        mock_get.return_value.status_code = 200
        mock_post.return_value.status_code = 201
        mock_post.return_value.json.return_value = {"id": "test-kernel-id"}
        mock_run_code_raises.return_value = CodeOutput(output="3", logs="", is_final_answer=False)
        mock_sandbox_create.return_value = mock_sandbox

        executor = ModalExecutor(
            additional_imports=[],
            logger=logger,
            app_name="my-custom-app-name",
            port=port,
            create_kwargs={
                "secrets": [modal.Secret.from_dict({"MY_SECRET": "ABC"})],
                "timeout": 100,
                "cpu": 2,
            },
        )

        create_call = mock_sandbox_create.mock_calls[0]
        assert create_call.args == (
            "jupyter",
            "kernelgateway",
            "--KernelGatewayApp.ip='0.0.0.0'",
            f"--KernelGatewayApp.port={port}",
            "--KernelGatewayApp.allow_origin='*'",
        )
        assert create_call.kwargs["timeout"] == 100
        assert create_call.kwargs["cpu"] == 2
        assert len(create_call.kwargs["secrets"]) == 2
        mock_app_lookup.assert_called_with("my-custom-app-name", create_if_missing=True)

        executor.run_code_raise_errors("1 + 2")
        executor.cleanup()
        mock_sandbox.terminate.assert_called()


class TestWasmExecutorUnit:
    def test_wasm_executor_instantiation(self):
        logger = MagicMock()

        # Mock subprocess.run to simulate Deno being installed
        with (
            patch("subprocess.run") as mock_run,
            patch("subprocess.Popen") as mock_popen,
            patch("requests.get") as mock_get,
            patch("time.sleep"),
        ):
            # Configure mocks
            mock_run.return_value.returncode = 0
            mock_process = MagicMock()
            mock_process.poll.return_value = None
            mock_popen.return_value = mock_process
            mock_get.return_value.status_code = 200

            # Create the executor
            executor = WasmExecutor(additional_imports=["numpy", "pandas"], logger=logger, timeout=30)

            # Verify the executor was created correctly
            assert isinstance(executor, WasmExecutor)
            assert executor.logger == logger
            assert executor.timeout == 30
            assert "numpy" in executor.installed_packages
            assert "pandas" in executor.installed_packages

            # Verify Deno was checked
            assert mock_run.call_count == 1
            assert mock_run.call_args.args[0][0] == "deno"
            assert mock_run.call_args.args[0][1] == "--version"

            # Verify server was started
            assert mock_popen.call_count == 1
            assert mock_popen.call_args.args[0][0] == "deno"
            assert mock_popen.call_args.args[0][1] == "run"

            # Clean up
            with patch("shutil.rmtree"):
                executor.cleanup()


@require_run_all
class TestWasmExecutorIntegration:
    """
    Integration tests for WasmExecutor.

    These tests require Deno to be installed on the system.
    Skip these tests if you don't have Deno installed.
    """

    @pytest.fixture(autouse=True)
    def setup_and_teardown(self):
        """Setup and teardown for each test."""
        try:
            # Check if Deno is installed
            import subprocess

            subprocess.run(["deno", "--version"], capture_output=True, check=True)

            # Create the executor
            self.executor = WasmExecutor(
                additional_imports=["numpy", "pandas"],
                logger=AgentLogger(LogLevel.INFO, Console(force_terminal=False, file=io.StringIO())),
                timeout=60,
            )
            yield
            # Clean up
            self.executor.cleanup()
        except (subprocess.SubprocessError, FileNotFoundError):
            pytest.skip("Deno is not installed, skipping integration tests")

    def test_basic_execution(self):
        """Test basic code execution."""
        code = "a = 2 + 2; print(f'Result: {a}')"
        code_output = self.executor(code)
        assert "Result: 4" in code_output.logs

    def test_state_persistence(self):
        """Test that variables persist between executions."""
        # Define a variable
        self.executor("x = 42")

        # Use the variable in a subsequent execution
        code_output = self.executor("print(x)")
        assert "42" in code_output.logs

    def test_final_answer(self):
        """Test returning a final answer."""
        self.executor.send_tools({"final_answer": FinalAnswerTool()})
        code = 'final_answer("This is the final answer")'
        code_output = self.executor(code)
        assert code_output.output == "This is the final answer"
        assert code_output.is_final_answer is True

    def test_numpy_execution(self):
        """Test execution with NumPy."""
        code = """
        import numpy as np
        arr = np.array([1, 2, 3, 4, 5])
        print(f"Mean: {np.mean(arr)}")
        """
        code_output = self.executor(code)
        assert "Mean: 3.0" in code_output.logs

    def test_error_handling(self):
        """Test handling of Python errors."""
        code = "1/0"  # Division by zero
        with pytest.raises(AgentError) as excinfo:
            self.executor(code)
        assert "ZeroDivisionError" in str(excinfo.value)

    def test_syntax_error_handling(self):
        """Test handling of syntax errors."""
        code = "print('Missing parenthesis"  # Missing closing parenthesis
        with pytest.raises(AgentError) as excinfo:
            self.executor(code)
        assert "SyntaxError" in str(excinfo.value)


class TestBlaxelExecutorUnit:
    def test_blaxel_executor_instantiation_without_blaxel_sdk(self):
        """Test that BlaxelExecutor raises appropriate error when blaxel SDK is not installed."""
        logger = MagicMock()
        with patch.dict("sys.modules", {"blaxel.core": None}):
            with pytest.raises(ModuleNotFoundError) as excinfo:
                BlaxelExecutor(additional_imports=[], logger=logger)
            assert "Please install 'blaxel' extra" in str(excinfo.value)

    @patch("smolagents.remote_executors._create_kernel_http")
    @patch("blaxel.core.SandboxInstance")
    @patch("blaxel.core.settings")
    def test_blaxel_executor_instantiation_with_blaxel_sdk(
        self, mock_settings, mock_sandbox_instance, mock_create_kernel
    ):
        """Test BlaxelExecutor instantiation with mocked Blaxel SDK."""

        # patch manually for Python 3.10 compatibility
        from unittest.mock import patch

        mod = importlib.import_module("blaxel.core.client.api.compute")
        patcher = patch.object(mod, "create_sandbox")
        mock_create_sandbox = patcher.start()

        logger = MagicMock()
        mock_settings.headers = {}

        # Mock sandbox response
        mock_response = MagicMock()
        mock_create_sandbox.sync.return_value = mock_response

        # Mock SandboxInstance
        mock_sandbox = MagicMock()
        mock_metadata = MagicMock()
        mock_metadata.url = "https://test-sandbox.bl.run"
        mock_sandbox.metadata = mock_metadata
        mock_sandbox_instance.return_value = mock_sandbox

        # Mock kernel creation
        mock_create_kernel.return_value = "kernel-123"

        executor = BlaxelExecutor(additional_imports=[], logger=logger)

        patcher.stop()

        assert executor.sandbox_name.startswith("smolagent-executor-")
        assert executor.image == "blaxel/jupyter-notebook"
        assert executor.memory == 4096
        assert executor.region is None

    @patch("smolagents.remote_executors.BlaxelExecutor.install_packages")
    @patch("smolagents.remote_executors._create_kernel_http")
    @patch("blaxel.core.SandboxInstance")
    @patch("blaxel.core.settings")
    def test_blaxel_executor_custom_parameters(
        self, mock_settings, mock_sandbox_instance, mock_create_kernel, mock_install_packages
    ):
        """Test BlaxelExecutor with custom parameters."""
        logger = MagicMock()
        mock_settings.headers = {}
        mock_install_packages.return_value = ["numpy"]

        # Mock sandbox response
        mock_response = MagicMock()

        # patch manually for Python 3.10 compatibility
        mod = importlib.import_module("blaxel.core.client.api.compute")
        create_sandbox_patcher = patch.object(mod, "create_sandbox")
        mock_create_sandbox = create_sandbox_patcher.start()
        mock_create_sandbox.sync.return_value = mock_response

        # Mock SandboxInstance
        mock_sandbox = MagicMock()
        mock_metadata = MagicMock()
        mock_metadata.url = "https://test-sandbox.us-was-1.bl.run"
        mock_sandbox.metadata = mock_metadata
        mock_sandbox_instance.return_value = mock_sandbox

        # Mock kernel creation
        mock_create_kernel.return_value = "kernel-123"

        executor = BlaxelExecutor(
            additional_imports=["numpy"],
            logger=logger,
            sandbox_name="test-sandbox",
            image="custom-image:latest",
            memory=8192,
            region="us-was-1",
        )

        create_sandbox_patcher.stop()

        assert executor.sandbox_name == "test-sandbox"
        assert executor.image == "custom-image:latest"
        assert executor.memory == 8192
        assert executor.region == "us-was-1"
        assert mock_install_packages.called

    @patch("smolagents.remote_executors._create_kernel_http")
    @patch("blaxel.core.SandboxInstance")
    @patch("blaxel.core.settings")
    def test_blaxel_executor_cleanup(self, mock_settings, mock_sandbox_instance, mock_create_kernel):
        """Test BlaxelExecutor cleanup method."""

        # patch manually for Python 3.10 compatibility
        from unittest.mock import patch

        mod = importlib.import_module("blaxel.core.client.api.compute")
        create_sandbox_patcher = patch.object(mod, "create_sandbox")
        mock_create_sandbox = create_sandbox_patcher.start()
        delete_sandbox_patcher = patch.object(mod, "delete_sandbox")
        mock_delete_sandbox = delete_sandbox_patcher.start()

        logger = MagicMock()
        mock_settings.headers = {}

        # Mock sandbox response
        mock_response = MagicMock()
        mock_create_sandbox.sync.return_value = mock_response

        # Mock SandboxInstance
        mock_sandbox = MagicMock()
        mock_metadata = MagicMock()
        mock_metadata.url = "https://test-sandbox.bl.run"
        mock_sandbox.metadata = mock_metadata
        mock_sandbox_instance.return_value = mock_sandbox

        # Mock kernel creation
        mock_create_kernel.return_value = "kernel-123"

        executor = BlaxelExecutor(additional_imports=[], logger=logger)

        # Test cleanup
        executor.cleanup()
        create_sandbox_patcher.stop()
        delete_sandbox_patcher.stop()

        # Verify that delete_sandbox.sync was called
        assert mock_delete_sandbox.sync.called
        # Verify sandbox reference was cleaned up
        assert not hasattr(executor, "sandbox")



================================================
FILE: tests/test_search.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from smolagents import DuckDuckGoSearchTool

from .test_tools import ToolTesterMixin
from .utils.markers import require_run_all


class TestDuckDuckGoSearchTool(ToolTesterMixin):
    def setup_method(self):
        self.tool = DuckDuckGoSearchTool()
        self.tool.setup()

    @require_run_all
    def test_exact_match_arg(self):
        result = self.tool("Agents")
        assert isinstance(result, str)

    @require_run_all
    def test_agent_type_output(self):
        super().test_agent_type_output()



================================================
FILE: tests/test_telemetry.py
================================================
# coding=utf-8
# Copyright 2025 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Source: https://github.com/Arize-ai/openinference/blob/main/python/instrumentation/openinference-instrumentation-smolagents/tests/openinference/instrumentation/smolagents/test_instrumentor.py

from typing import Generator

import pytest

from .utils.markers import require_run_all


# Add this at the module level to skip all tests if OpenTelemetry is not available
pytest.importorskip("opentelemetry", reason="requires opentelemetry")
pytest.importorskip(
    "openinference.instrumentation.smolagents", reason="requires openinference.instrumentation.smolagents"
)

from openinference.instrumentation.smolagents import SmolagentsInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.sdk import trace as trace_sdk
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter

from smolagents.models import InferenceClientModel


@pytest.fixture
def in_memory_span_exporter() -> InMemorySpanExporter:
    return InMemorySpanExporter()


@pytest.fixture
def tracer_provider(in_memory_span_exporter: InMemorySpanExporter) -> trace_api.TracerProvider:
    resource = Resource(attributes={})
    tracer_provider = trace_sdk.TracerProvider(resource=resource)
    span_processor = SimpleSpanProcessor(span_exporter=in_memory_span_exporter)
    tracer_provider.add_span_processor(span_processor=span_processor)
    return tracer_provider


@pytest.fixture(autouse=True)
def instrument(
    tracer_provider: trace_api.TracerProvider,
    in_memory_span_exporter: InMemorySpanExporter,
) -> Generator[None, None, None]:
    SmolagentsInstrumentor().instrument(tracer_provider=tracer_provider, skip_dep_check=True)
    yield
    SmolagentsInstrumentor().uninstrument()
    in_memory_span_exporter.clear()


@require_run_all
class TestOpenTelemetry:
    def test_model(self, in_memory_span_exporter: InMemorySpanExporter):
        model = InferenceClientModel()
        _ = model(
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "Who won the World Cup in 2018? Answer in one word with no punctuation.",
                        }
                    ],
                }
            ]
        )
        spans = in_memory_span_exporter.get_finished_spans()
        assert len(spans) == 1
        span = spans[0]
        assert span.name == "InferenceClientModel.generate"
        assert span.status.is_ok
        assert span.attributes



================================================
FILE: tests/test_tool_validation.py
================================================
import ast
from textwrap import dedent

import pytest

from smolagents.default_tools import (
    DuckDuckGoSearchTool,
    GoogleSearchTool,
    SpeechToTextTool,
    VisitWebpageTool,
    WebSearchTool,
)
from smolagents.tool_validation import MethodChecker, validate_tool_attributes
from smolagents.tools import Tool, tool


UNDEFINED_VARIABLE = "undefined_variable"


@pytest.mark.parametrize(
    "tool_class", [DuckDuckGoSearchTool, GoogleSearchTool, SpeechToTextTool, VisitWebpageTool, WebSearchTool]
)
def test_validate_tool_attributes_with_default_tools(tool_class):
    assert validate_tool_attributes(tool_class) is None, f"failed for {tool_class.name} tool"


class ValidTool(Tool):
    name = "valid_tool"
    description = "A valid tool"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"
    simple_attr = "string"
    dict_attr = {"key": "value"}

    def __init__(self, optional_param="default"):
        super().__init__()
        self.param = optional_param

    def forward(self, input: str) -> str:
        return input.upper()


@tool
def valid_tool_function(input: str) -> str:
    """A valid tool function.

    Args:
        input (str): Input string.
    """
    return input.upper()


@pytest.mark.parametrize("tool_class", [ValidTool, valid_tool_function.__class__])
def test_validate_tool_attributes_valid(tool_class):
    assert validate_tool_attributes(tool_class) is None


class InvalidToolName(Tool):
    name = "invalid tool name"
    description = "Tool with invalid name"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"

    def __init__(self):
        super().__init__()

    def forward(self, input: str) -> str:
        return input


class InvalidToolComplexAttrs(Tool):
    name = "invalid_tool"
    description = "Tool with complex class attributes"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"
    complex_attr = [x for x in range(3)]  # Complex class attribute

    def __init__(self):
        super().__init__()

    def forward(self, input: str) -> str:
        return input


class InvalidToolRequiredParams(Tool):
    name = "invalid_tool"
    description = "Tool with required params"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"

    def __init__(self, required_param, kwarg1=1):  # No default value
        super().__init__()
        self.param = required_param

    def forward(self, input: str) -> str:
        return input


class InvalidToolNonLiteralDefaultParam(Tool):
    name = "invalid_tool"
    description = "Tool with non-literal default parameter value"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"

    def __init__(self, default_param=UNDEFINED_VARIABLE):  # UNDEFINED_VARIABLE as default is non-literal
        super().__init__()
        self.default_param = default_param

    def forward(self, input: str) -> str:
        return input


class InvalidToolUndefinedNames(Tool):
    name = "invalid_tool"
    description = "Tool with undefined names"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"

    def forward(self, input: str) -> str:
        return UNDEFINED_VARIABLE  # Undefined name


@pytest.mark.parametrize(
    "tool_class, expected_error",
    [
        (
            InvalidToolName,
            "Class attribute 'name' must be a valid Python identifier and not a reserved keyword, found 'invalid tool name'",
        ),
        (InvalidToolComplexAttrs, "Complex attributes should be defined in __init__, not as class attributes"),
        (InvalidToolRequiredParams, "Parameters in __init__ must have default values, found required parameters"),
        (
            InvalidToolNonLiteralDefaultParam,
            "Parameters in __init__ must have literal default values, found non-literal defaults",
        ),
        (InvalidToolUndefinedNames, "Name 'UNDEFINED_VARIABLE' is undefined"),
    ],
)
def test_validate_tool_attributes_exceptions(tool_class, expected_error):
    with pytest.raises(ValueError, match=expected_error):
        validate_tool_attributes(tool_class)


class MultipleAssignmentsTool(Tool):
    name = "multiple_assignments_tool"
    description = "Tool with multiple assignments"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"

    def __init__(self):
        super().__init__()

    def forward(self, input: str) -> str:
        a, b = "1", "2"
        return a + b


def test_validate_tool_attributes_multiple_assignments():
    validate_tool_attributes(MultipleAssignmentsTool)


@tool
def tool_function_with_multiple_assignments(input: str) -> str:
    """A valid tool function.

    Args:
        input (str): Input string.
    """
    a, b = "1", "2"
    return input.upper() + a + b


@pytest.mark.parametrize("tool_instance", [MultipleAssignmentsTool(), tool_function_with_multiple_assignments])
def test_tool_to_dict_validation_with_multiple_assignments(tool_instance):
    tool_instance.to_dict()


class TestMethodChecker:
    def test_multiple_assignments(self):
        source_code = dedent(
            """
            def forward(self) -> str:
                a, b = "1", "2"
                return a + b
            """
        )
        method_checker = MethodChecker(set())
        method_checker.visit(ast.parse(source_code))
        assert method_checker.errors == []



================================================
FILE: tests/test_tools.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import os
import warnings
from textwrap import dedent
from typing import Any, Literal
from unittest.mock import MagicMock, patch

import mcp
import numpy as np
import PIL.Image
import pytest

from smolagents.agent_types import _AGENT_TYPE_MAPPING
from smolagents.tools import AUTHORIZED_TYPES, Tool, ToolCollection, launch_gradio_demo, tool, validate_tool_arguments

from .utils.markers import require_run_all


class ToolTesterMixin:
    def test_inputs_output(self):
        assert hasattr(self.tool, "inputs")
        assert hasattr(self.tool, "output_type")

        inputs = self.tool.inputs
        assert isinstance(inputs, dict)

        for _, input_spec in inputs.items():
            assert "type" in input_spec
            assert "description" in input_spec
            assert input_spec["type"] in AUTHORIZED_TYPES
            assert isinstance(input_spec["description"], str)

        output_type = self.tool.output_type
        assert output_type in AUTHORIZED_TYPES

    def test_common_attributes(self):
        assert hasattr(self.tool, "description")
        assert hasattr(self.tool, "name")
        assert hasattr(self.tool, "inputs")
        assert hasattr(self.tool, "output_type")

    def test_agent_type_output(self, create_inputs):
        inputs = create_inputs(self.tool.inputs)
        output = self.tool(**inputs, sanitize_inputs_outputs=True)
        if self.tool.output_type != "any":
            agent_type = _AGENT_TYPE_MAPPING[self.tool.output_type]
            assert isinstance(output, agent_type)

    @pytest.fixture
    def create_inputs(self, shared_datadir):
        def _create_inputs(tool_inputs: dict[str, dict[str | type, str]]) -> dict[str, Any]:
            inputs = {}

            for input_name, input_desc in tool_inputs.items():
                input_type = input_desc["type"]

                if input_type == "string":
                    inputs[input_name] = "Text input"
                elif input_type == "image":
                    inputs[input_name] = PIL.Image.open(shared_datadir / "000000039769.png").resize((512, 512))
                elif input_type == "audio":
                    inputs[input_name] = np.ones(3000)
                else:
                    raise ValueError(f"Invalid type requested: {input_type}")

            return inputs

        return _create_inputs


class TestTool:
    @pytest.mark.parametrize(
        "type_value, should_raise_error, error_contains",
        [
            # Valid cases
            ("string", False, None),
            (["string", "number"], False, None),
            # Invalid cases
            ("invalid_type", ValueError, "must be one of"),
            (["string", "invalid_type"], ValueError, "must be one of"),
            ([123, "string"], TypeError, "when type is a list, all elements must be strings"),
            (123, TypeError, "must be a string or list of strings"),
        ],
    )
    def test_tool_input_type_validation(self, type_value, should_raise_error, error_contains):
        """Test the validation of the type property in tool inputs."""

        # Define a tool class with the test type value
        def create_tool():
            class TestTool(Tool):
                name = "test_tool"
                description = "A tool for testing type validation"
                inputs = {"text": {"type": type_value, "description": "Some input"}}
                output_type = "string"

                def forward(self, text) -> str:
                    return text

            return TestTool()

        # Check if we expect this to raise an exception
        if should_raise_error:
            with pytest.raises(should_raise_error) as exc_info:
                create_tool()
            # Verify the error message contains expected text
            assert error_contains in str(exc_info.value)
        else:
            # Should not raise an exception
            tool = create_tool()
            assert isinstance(tool, Tool)

    @pytest.mark.parametrize(
        "tool_fixture, expected_output",
        [
            ("no_input_tool", 'def no_input_tool() -> string:\n    """Tool with no inputs\n    """'),
            (
                "single_input_tool",
                'def single_input_tool(text: string) -> string:\n    """Tool with one input\n\n    Args:\n        text: Input text\n    """',
            ),
            (
                "multi_input_tool",
                'def multi_input_tool(text: string, count: integer) -> object:\n    """Tool with multiple inputs\n\n    Args:\n        text: Text input\n        count: Number count\n    """',
            ),
            (
                "multiline_description_tool",
                'def multiline_description_tool(input: string) -> string:\n    """This is a tool with\n    multiple lines\n    in the description\n\n    Args:\n        input: Some input\n    """',
            ),
        ],
    )
    def test_tool_to_code_prompt_output_format(self, tool_fixture, expected_output, request):
        """Test that to_code_prompt generates properly formatted and indented output."""
        tool = request.getfixturevalue(tool_fixture)
        code_prompt = tool.to_code_prompt()
        assert code_prompt == expected_output

    @pytest.mark.parametrize(
        "tool_fixture, expected_output",
        [
            (
                "no_input_tool",
                "no_input_tool: Tool with no inputs\n    Takes inputs: {}\n    Returns an output of type: string",
            ),
            (
                "single_input_tool",
                "single_input_tool: Tool with one input\n    Takes inputs: {'text': {'type': 'string', 'description': 'Input text'}}\n    Returns an output of type: string",
            ),
            (
                "multi_input_tool",
                "multi_input_tool: Tool with multiple inputs\n    Takes inputs: {'text': {'type': 'string', 'description': 'Text input'}, 'count': {'type': 'integer', 'description': 'Number count'}}\n    Returns an output of type: object",
            ),
            (
                "multiline_description_tool",
                "multiline_description_tool: This is a tool with\nmultiple lines\nin the description\n    Takes inputs: {'input': {'type': 'string', 'description': 'Some input'}}\n    Returns an output of type: string",
            ),
        ],
    )
    def test_tool_to_tool_calling_prompt_output_format(self, tool_fixture, expected_output, request):
        """Test that to_tool_calling_prompt generates properly formatted output."""
        tool = request.getfixturevalue(tool_fixture)
        tool_calling_prompt = tool.to_tool_calling_prompt()
        assert tool_calling_prompt == expected_output

    def test_tool_init_with_decorator(self):
        @tool
        def coolfunc(a: str, b: int) -> float:
            """Cool function

            Args:
                a: The first argument
                b: The second one
            """
            return b + 2, a

        assert coolfunc.output_type == "number"

    def test_tool_init_vanilla(self):
        class HFModelDownloadsTool(Tool):
            name = "model_download_counter"
            description = """
            This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.
            It returns the name of the checkpoint."""

            inputs = {
                "task": {
                    "type": "string",
                    "description": "the task category (such as text-classification, depth-estimation, etc)",
                }
            }
            output_type = "string"

            def forward(self, task: str) -> str:
                return "best model"

        tool = HFModelDownloadsTool()
        assert list(tool.inputs.keys())[0] == "task"

    def test_tool_init_decorator_raises_issues(self):
        with pytest.raises(Exception) as e:

            @tool
            def coolfunc(a: str, b: int):
                """Cool function

                Args:
                    a: The first argument
                    b: The second one
                """
                return a + b

            assert coolfunc.output_type == "number"
        assert "Tool return type not found" in str(e)

        with pytest.raises(Exception) as e:

            @tool
            def coolfunc(a: str, b: int) -> int:
                """Cool function

                Args:
                    a: The first argument
                """
                return b + a

            assert coolfunc.output_type == "number"
        assert "docstring has no description for the argument" in str(e)

    def test_saving_tool_raises_error_imports_outside_function(self, tmp_path):
        with pytest.raises(Exception) as e:
            import numpy as np

            @tool
            def get_current_time() -> str:
                """
                Gets the current time.
                """
                return str(np.random.random())

            get_current_time.save(tmp_path)

        assert "np" in str(e)

        # Also test with classic definition
        with pytest.raises(Exception) as e:

            class GetCurrentTimeTool(Tool):
                name = "get_current_time_tool"
                description = "Gets the current time"
                inputs = {}
                output_type = "string"

                def forward(self):
                    return str(np.random.random())

            get_current_time = GetCurrentTimeTool()
            get_current_time.save(tmp_path)

        assert "np" in str(e)

    def test_tool_definition_raises_no_error_imports_in_function(self):
        @tool
        def get_current_time() -> str:
            """
            Gets the current time.
            """
            from datetime import datetime

            return str(datetime.now())

        class GetCurrentTimeTool(Tool):
            name = "get_current_time_tool"
            description = "Gets the current time"
            inputs = {}
            output_type = "string"

            def forward(self):
                from datetime import datetime

                return str(datetime.now())

    def test_tool_to_dict_allows_no_arg_in_init(self):
        """Test that a tool cannot be saved with required args in init"""

        class FailTool(Tool):
            name = "specific"
            description = "test description"
            inputs = {"string_input": {"type": "string", "description": "input description"}}
            output_type = "string"

            def __init__(self, url):
                super().__init__(self)
                self.url = url

            def forward(self, string_input: str) -> str:
                return self.url + string_input

        fail_tool = FailTool("dummy_url")
        with pytest.raises(Exception) as e:
            fail_tool.to_dict()
        assert "Parameters in __init__ must have default values, found required parameters" in str(e)

        class PassTool(Tool):
            name = "specific"
            description = "test description"
            inputs = {"string_input": {"type": "string", "description": "input description"}}
            output_type = "string"

            def __init__(self, url: str | None = "none"):
                super().__init__(self)
                self.url = url

            def forward(self, string_input: str) -> str:
                return self.url + string_input

        fail_tool = PassTool()
        fail_tool.to_dict()

    def test_saving_tool_allows_no_imports_from_outside_methods(self, tmp_path):
        # Test that using imports from outside functions fails
        import numpy as np

        class FailTool(Tool):
            name = "specific"
            description = "test description"
            inputs = {"string_input": {"type": "string", "description": "input description"}}
            output_type = "string"

            def useless_method(self):
                self.client = np.random.random()
                return ""

            def forward(self, string_input):
                return self.useless_method() + string_input

        fail_tool = FailTool()
        with pytest.raises(Exception) as e:
            fail_tool.save(tmp_path)
        assert "'np' is undefined" in str(e)

        # Test that putting these imports inside functions works
        class SuccessTool(Tool):
            name = "specific"
            description = "test description"
            inputs = {"string_input": {"type": "string", "description": "input description"}}
            output_type = "string"

            def useless_method(self):
                import numpy as np

                self.client = np.random.random()
                return ""

            def forward(self, string_input):
                return self.useless_method() + string_input

        success_tool = SuccessTool()
        success_tool.save(tmp_path)

    def test_tool_missing_class_attributes_raises_error(self):
        with pytest.raises(Exception) as e:

            class GetWeatherTool(Tool):
                name = "get_weather"
                description = "Get weather in the next days at given location."
                inputs = {
                    "location": {"type": "string", "description": "the location"},
                    "celsius": {
                        "type": "string",
                        "description": "the temperature type",
                    },
                }

                def forward(self, location: str, celsius: bool | None = False) -> str:
                    return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

            GetWeatherTool()
        assert "You must set an attribute output_type" in str(e)

    def test_tool_from_decorator_optional_args(self):
        @tool
        def get_weather(location: str, celsius: bool | None = False) -> str:
            """
            Get weather in the next days at given location.
            Secretly this tool does not care about the location, it hates the weather everywhere.

            Args:
                location: the location
                celsius: the temperature type
            """
            return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

        assert "nullable" in get_weather.inputs["celsius"]
        assert get_weather.inputs["celsius"]["nullable"]
        assert "nullable" not in get_weather.inputs["location"]

    def test_tool_mismatching_nullable_args_raises_error(self):
        with pytest.raises(Exception) as e:

            class GetWeatherTool(Tool):
                name = "get_weather"
                description = "Get weather in the next days at given location."
                inputs = {
                    "location": {"type": "string", "description": "the location"},
                    "celsius": {
                        "type": "string",
                        "description": "the temperature type",
                    },
                }
                output_type = "string"

                def forward(self, location: str, celsius: bool | None = False) -> str:
                    return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

            GetWeatherTool()
        assert "Nullable" in str(e)

        with pytest.raises(Exception) as e:

            class GetWeatherTool2(Tool):
                name = "get_weather"
                description = "Get weather in the next days at given location."
                inputs = {
                    "location": {"type": "string", "description": "the location"},
                    "celsius": {
                        "type": "string",
                        "description": "the temperature type",
                    },
                }
                output_type = "string"

                def forward(self, location: str, celsius: bool = False) -> str:
                    return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

            GetWeatherTool2()
        assert "Nullable" in str(e)

        with pytest.raises(Exception) as e:

            class GetWeatherTool3(Tool):
                name = "get_weather"
                description = "Get weather in the next days at given location."
                inputs = {
                    "location": {"type": "string", "description": "the location"},
                    "celsius": {
                        "type": "string",
                        "description": "the temperature type",
                        "nullable": True,
                    },
                }
                output_type = "string"

                def forward(self, location, celsius: str) -> str:
                    return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

            GetWeatherTool3()
        assert "Nullable" in str(e)

    def test_tool_default_parameters_is_nullable(self):
        @tool
        def get_weather(location: str, celsius: bool = False) -> str:
            """
            Get weather in the next days at given location.

            Args:
                location: The location to get the weather for.
                celsius: is the temperature given in celsius?
            """
            return "The weather is UNGODLY with torrential rains and temperatures below -10Â°C"

        assert get_weather.inputs["celsius"]["nullable"]

    def test_tool_supports_any_none(self, tmp_path):
        @tool
        def get_weather(location: Any) -> None:
            """
            Get weather in the next days at given location.

            Args:
                location: The location to get the weather for.
            """
            return

        get_weather.save(tmp_path)
        assert get_weather.inputs["location"]["type"] == "any"
        assert get_weather.output_type == "null"

    def test_tool_supports_array(self):
        @tool
        def get_weather(locations: list[str], months: tuple[str, str] | None = None) -> dict[str, float]:
            """
            Get weather in the next days at given locations.

            Args:
                locations: The locations to get the weather for.
                months: The months to get the weather for
            """
            return

        assert get_weather.inputs["locations"]["type"] == "array"
        assert get_weather.inputs["months"]["type"] == "array"

    def test_tool_supports_string_literal(self):
        @tool
        def get_weather(unit: Literal["celsius", "fahrenheit"] = "celsius") -> None:
            """
            Get weather in the next days at given location.

            Args:
                unit: The unit of temperature
            """
            return

        assert get_weather.inputs["unit"]["type"] == "string"
        assert get_weather.inputs["unit"]["enum"] == ["celsius", "fahrenheit"]

    def test_tool_supports_numeric_literal(self):
        @tool
        def get_choice(choice: Literal[1, 2, 3]) -> None:
            """
            Get choice based on the provided numeric literal.

            Args:
                choice: The numeric choice to be made.
            """
            return

        assert get_choice.inputs["choice"]["type"] == "integer"
        assert get_choice.inputs["choice"]["enum"] == [1, 2, 3]

    def test_tool_supports_nullable_literal(self):
        @tool
        def get_choice(choice: Literal[1, 2, 3, None]) -> None:
            """
            Get choice based on the provided value.

            Args:
                choice: The numeric choice to be made.
            """
            return

        assert get_choice.inputs["choice"]["type"] == "integer"
        assert get_choice.inputs["choice"]["nullable"] is True
        assert get_choice.inputs["choice"]["enum"] == [1, 2, 3]

    def test_saving_tool_produces_valid_pyhon_code_with_multiline_description(self, tmp_path):
        @tool
        def get_weather(location: Any) -> None:
            """
            Get weather in the next days at given location.
            And works pretty well.

            Args:
                location: The location to get the weather for.
            """
            return

        get_weather.save(tmp_path)
        with open(os.path.join(tmp_path, "tool.py"), "r", encoding="utf-8") as f:
            source_code = f.read()
            compile(source_code, f.name, "exec")

    @pytest.mark.parametrize("fixture_name", ["boolean_default_tool_class", "boolean_default_tool_function"])
    def test_to_dict_boolean_default_input(self, fixture_name, request):
        """Test that boolean input parameter with default value is correctly represented in to_dict output"""
        tool = request.getfixturevalue(fixture_name)
        result = tool.to_dict()
        # Check that the boolean default annotation is preserved
        assert "flag: bool = False" in result["code"]
        # Check nullable attribute is set for the parameter with default value
        assert "'nullable': True" in result["code"]

    @pytest.mark.parametrize("fixture_name", ["optional_input_tool_class", "optional_input_tool_function"])
    def test_to_dict_optional_input(self, fixture_name, request):
        """Test that Optional/nullable input parameter is correctly represented in to_dict output"""
        tool = request.getfixturevalue(fixture_name)
        result = tool.to_dict()
        # Check the Optional type annotation is preserved
        assert "optional_text: str | None = None" in result["code"]
        # Check that the input is marked as nullable in the code
        assert "'nullable': True" in result["code"]

    def test_from_dict_roundtrip(self, example_tool):
        # Convert to dict
        tool_dict = example_tool.to_dict()
        # Create from dict
        recreated_tool = Tool.from_dict(tool_dict)
        # Verify properties
        assert recreated_tool.name == example_tool.name
        assert recreated_tool.description == example_tool.description
        assert recreated_tool.inputs == example_tool.inputs
        assert recreated_tool.output_type == example_tool.output_type
        # Verify functionality
        test_input = "Hello, world!"
        assert recreated_tool(test_input) == test_input.upper()

    def test_tool_from_dict_invalid(self):
        # Missing code key
        with pytest.raises(ValueError) as e:
            Tool.from_dict({"name": "invalid_tool"})
        assert "must contain 'code' key" in str(e)

    def test_tool_decorator_preserves_original_function(self):
        # Define a test function with type hints and docstring
        def test_function(items: list[str]) -> str:
            """Join a list of strings.
            Args:
                items: A list of strings to join
            Returns:
                The joined string
            """
            return ", ".join(items)

        # Store original function signature, name, and source
        original_signature = inspect.signature(test_function)
        original_name = test_function.__name__
        original_docstring = test_function.__doc__

        # Create a tool from the function
        test_tool = tool(test_function)

        # Check that the original function is unchanged
        assert original_signature == inspect.signature(test_function)
        assert original_name == test_function.__name__
        assert original_docstring == test_function.__doc__

        # Verify that the tool's forward method has a different signature (it has 'self')
        tool_forward_sig = inspect.signature(test_tool.forward)
        assert list(tool_forward_sig.parameters.keys())[0] == "self"

        # Original function should not have 'self' parameter
        assert "self" not in original_signature.parameters

    def test_tool_with_union_type_return(self):
        @tool
        def union_type_return_tool_function(param: int) -> str | bool:
            """
            Tool with output union type.

            Args:
                param: Input parameter.
            """
            return str(param) if param > 0 else False

        assert isinstance(union_type_return_tool_function, Tool)
        assert union_type_return_tool_function.output_type == "any"


class TestToolDecorator:
    def test_tool_decorator_source_extraction_with_multiple_decorators(self):
        """Test that @tool correctly extracts source code with multiple decorators."""

        def dummy_decorator(func):
            return func

        with pytest.warns(UserWarning, match="has decorators other than @tool"):

            @tool
            @dummy_decorator
            def multi_decorator_tool(text: str) -> str:
                """Tool with multiple decorators.

                Args:
                    text: Input text
                """
                return text.upper()

        # Verify the tool works
        assert isinstance(multi_decorator_tool, Tool)
        assert multi_decorator_tool.name == "multi_decorator_tool"
        assert multi_decorator_tool("hello") == "HELLO"

        # Verify the source code extraction is correct
        forward_source = multi_decorator_tool.forward.__source__
        assert "def forward(self, text: str) -> str:" in forward_source
        assert "return text.upper()" in forward_source
        # Should not contain decorator lines
        assert "@tool" not in forward_source
        assert "@dummy_decorator" not in forward_source
        # Should not contain definition line
        assert "def multi_decorator_tool" not in forward_source

    def test_tool_decorator_source_extraction_with_multiline_signature(self):
        """Test that @tool correctly extracts source code with multiline function signatures."""

        with warnings.catch_warnings():
            warnings.simplefilter("error")

            @tool
            def multiline_signature_tool(
                text: str,
                count: int = 1,
                uppercase: bool = False,
                multiline_parameter_1: int = 1_000,
                multiline_parameter_2: int = 2_000,
            ) -> str:
                """Tool with multiline signature.

                Args:
                    text: Input text
                    count: Number of repetitions
                    uppercase: Whether to convert to uppercase
                    multiline_parameter_1: Dummy parameter
                    multiline_parameter_2: Dummy parameter
                """
                result = text * count
                return result.upper() if uppercase else result

        # Verify the tool works
        assert isinstance(multiline_signature_tool, Tool)
        assert multiline_signature_tool.name == "multiline_signature_tool"
        assert multiline_signature_tool("hello", 2, True) == "HELLOHELLO"

        # Verify the source code extraction is correct
        forward_source = multiline_signature_tool.forward.__source__
        assert (
            "def forward(self, text: str, count: int=1, uppercase: bool=False, multiline_parameter_1: int=1000, multiline_parameter_2: int=2000) -> str:"
            in forward_source
            or "def forward(self, text: str, count: int = 1, uppercase: bool = False, multiline_parameter_1: int = 1000, multiline_parameter_2: int = 2000) -> str:"
            in forward_source
        )
        assert "result = text * count" in forward_source
        assert "return result.upper() if uppercase else result" in forward_source
        # Should not contain the original multiline function definition
        assert "def multiline_signature_tool(" not in forward_source
        # Should not contain leftover lines from the original multiline function definition
        assert "            count: int = 1," not in forward_source
        assert "            count: int=1," not in forward_source

    def test_tool_decorator_source_extraction_with_multiple_decorators_and_multiline(self):
        """Test that @tool works with both multiple decorators and multiline signatures."""

        def dummy_decorator_1(func):
            return func

        def dummy_decorator_2(func):
            return func

        with pytest.warns(UserWarning, match="has decorators other than @tool"):

            @tool
            @dummy_decorator_1
            @dummy_decorator_2
            def complex_tool(
                text: str,
                multiplier: int = 2,
                separator: str = " ",
                multiline_parameter_1: int = 1_000,
                multiline_parameter_2: int = 2_000,
            ) -> str:
                """Complex tool with multiple decorators and multiline signature.

                Args:
                    text: Input text
                    multiplier: How many times to repeat
                    separator: What to use between repetitions
                    multiline_parameter_1: Dummy parameter
                    multiline_parameter_2: Dummy parameter
                """
                parts = [text] * multiplier
                return separator.join(parts)

        # Verify the tool works
        assert isinstance(complex_tool, Tool)
        assert complex_tool.name == "complex_tool"
        assert complex_tool("hello", 3, "-") == "hello-hello-hello"

        # Verify the source code extraction is correct
        forward_source = complex_tool.forward.__source__
        assert (
            "def forward(self, text: str, multiplier: int=2, separator: str=' ', multiline_parameter_1: int=1000, multiline_parameter_2: int=2000) -> str:"
            in forward_source
            or "def forward(self, text: str, multiplier: int = 2, separator: str = ' ', multiline_parameter_1: int = 1000, multiline_parameter_2: int = 2000) -> str:"
            in forward_source
        )
        assert "parts = [text] * multiplier" in forward_source
        assert "return separator.join(parts)" in forward_source
        # Should not contain any decorator lines
        assert "@tool" not in forward_source
        assert "@dummy_decorator_1" not in forward_source
        assert "@dummy_decorator_2" not in forward_source
        # Should not contain leftover lines from the original multiline function definition
        assert "            multiplier: int = 2," not in forward_source
        assert "            multiplier: int=2," not in forward_source


@pytest.fixture
def mock_server_parameters():
    return MagicMock()


@pytest.fixture
def mock_mcp_adapt():
    with patch("mcpadapt.core.MCPAdapt") as mock:
        mock.return_value.__enter__.return_value = ["tool1", "tool2"]
        mock.return_value.__exit__.return_value = None
        yield mock


@pytest.fixture
def mock_smolagents_adapter():
    with patch("mcpadapt.smolagents_adapter.SmolAgentsAdapter") as mock:
        yield mock


# Ignore FutureWarning about structured_output default value change: this test intentionally uses default behavior
@pytest.mark.filterwarnings("ignore:.*structured_output:FutureWarning")
class TestToolCollection:
    def test_from_mcp(self, mock_server_parameters, mock_mcp_adapt, mock_smolagents_adapter):
        with ToolCollection.from_mcp(mock_server_parameters, trust_remote_code=True) as tool_collection:
            assert isinstance(tool_collection, ToolCollection)
            assert len(tool_collection.tools) == 2
            assert "tool1" in tool_collection.tools
            assert "tool2" in tool_collection.tools

    @require_run_all
    def test_integration_from_mcp(self):
        # define the most simple mcp server with one tool that echoes the input text
        mcp_server_script = dedent("""\
            from mcp.server.fastmcp import FastMCP

            mcp = FastMCP("Echo Server")

            @mcp.tool()
            def echo_tool(text: str) -> str:
                return text

            mcp.run()
        """).strip()

        mcp_server_params = mcp.StdioServerParameters(
            command="python",
            args=["-c", mcp_server_script],
        )

        with ToolCollection.from_mcp(mcp_server_params, trust_remote_code=True) as tool_collection:
            assert len(tool_collection.tools) == 1, "Expected 1 tool"
            assert tool_collection.tools[0].name == "echo_tool", "Expected tool name to be 'echo_tool'"
            assert tool_collection.tools[0](text="Hello") == "Hello", "Expected tool to echo the input text"

    def test_integration_from_mcp_with_streamable_http(self):
        import subprocess
        import time

        # define the most simple mcp server with one tool that echoes the input text
        mcp_server_script = dedent("""\
            from mcp.server.fastmcp import FastMCP

            mcp = FastMCP("Echo Server", host="127.0.0.1", port=8000)

            @mcp.tool()
            def echo_tool(text: str) -> str:
                return text

            mcp.run(transport="streamable-http")
        """).strip()

        # start the SSE mcp server in a subprocess
        server_process = subprocess.Popen(
            ["python", "-c", mcp_server_script],
        )

        # wait for the server to start
        time.sleep(1)

        try:
            with ToolCollection.from_mcp(
                {"url": "http://127.0.0.1:8000/mcp", "transport": "streamable-http"}, trust_remote_code=True
            ) as tool_collection:
                assert len(tool_collection.tools) == 1, "Expected 1 tool"
                assert tool_collection.tools[0].name == "echo_tool", "Expected tool name to be 'echo_tool'"
                assert tool_collection.tools[0](text="Hello") == "Hello", "Expected tool to echo the input text"
        finally:
            # clean up the process when test is done
            server_process.kill()
            server_process.wait()

    def test_integration_from_mcp_with_sse(self):
        import subprocess
        import time

        # define the most simple mcp server with one tool that echoes the input text
        mcp_server_script = dedent("""\
            from mcp.server.fastmcp import FastMCP

            mcp = FastMCP("Echo Server", host="127.0.0.1", port=8000)

            @mcp.tool()
            def echo_tool(text: str) -> str:
                return text

            mcp.run("sse")
        """).strip()

        # start the SSE mcp server in a subprocess
        server_process = subprocess.Popen(
            ["python", "-c", mcp_server_script],
        )

        # wait for the server to start
        time.sleep(1)

        try:
            with ToolCollection.from_mcp(
                {"url": "http://127.0.0.1:8000/sse", "transport": "sse"}, trust_remote_code=True
            ) as tool_collection:
                assert len(tool_collection.tools) == 1, "Expected 1 tool"
                assert tool_collection.tools[0].name == "echo_tool", "Expected tool name to be 'echo_tool'"
                assert tool_collection.tools[0](text="Hello") == "Hello", "Expected tool to echo the input text"
        finally:
            # clean up the process when test is done
            server_process.kill()
            server_process.wait()


@pytest.mark.parametrize("tool_fixture_name", ["boolean_default_tool_class"])
def test_launch_gradio_demo_does_not_raise(tool_fixture_name, request):
    tool = request.getfixturevalue(tool_fixture_name)
    with patch("gradio.Interface.launch") as mock_launch:
        launch_gradio_demo(tool)
    assert mock_launch.call_count == 1


@pytest.mark.parametrize(
    "tool_input_type, expected_input, expects_error",
    [
        (bool, True, False),
        (str, "b", False),
        (int, 1, False),
        (float, 1, False),
        (list, ["a", "b"], False),
        (list[str], ["a", "b"], False),
        (dict[str, str], {"a": "b"}, False),
        (dict[str, str], "b", True),
        (bool, "b", True),
        (str | int, "a", False),
        (str | int, 1, False),
        (str | int, None, True),
        (str | int, True, True),
    ],
)
def test_validate_tool_arguments(tool_input_type, expected_input, expects_error):
    @tool
    def test_tool(argument_a: tool_input_type) -> str:
        """Fake tool

        Args:
            argument_a: The input
        """
        return argument_a

    if expects_error:
        with pytest.raises((ValueError, TypeError)):
            validate_tool_arguments(test_tool, {"argument_a": expected_input})

    else:
        # Should not raise any exception
        validate_tool_arguments(test_tool, {"argument_a": expected_input})


@pytest.mark.parametrize(
    "scenario, type_hint, default, input_value, expected_error_message",
    [
        # Required parameters (no default)
        # - Valid input
        ("required_unsupported_none", str, ..., "text", None),
        # - None not allowed
        ("required_unsupported_none", str, ..., None, "Argument param has type 'null' but should be 'string'"),
        # - Missing required parameter is not allowed
        ("required_unsupported_none", str, ..., ..., "Argument param is required"),
        #
        # Required parameters but supports None
        # - Valid input
        ("required_supported_none", str | None, ..., "text", None),
        # - None allowed
        ("required_supported_none", str | None, ..., None, None),
        # - Missing required parameter is not allowed
        # TODO: Fix this test case: property is marked as nullable because it can be None, but it can't be missing because it is required
        # ("required_supported_none", str | None, ..., ..., "Argument param is required"),
        pytest.param(
            "required_supported_none",
            str | None,
            ...,
            ...,
            "Argument param is required",
            marks=pytest.mark.skip(reason="TODO: Fix this test case"),
        ),
        #
        # Optional parameters (has default, doesn't support None)
        # - Valid input
        ("optional_unsupported_none", str, "default", "text", None),
        # - None not allowed
        # TODO: Fix this test case: property is marked as nullable because it has a default value, but it can't be None
        # ("optional_unsupported_none", str, "default", None, "Argument param has type 'null' but should be 'string'"),
        pytest.param(
            "optional_unsupported_none",
            str,
            "default",
            None,
            "Argument param has type 'null' but should be 'string'",
            marks=pytest.mark.skip(reason="TODO: Fix this test case"),
        ),
        # - Missing optional parameter is allowed
        ("optional_unsupported_none", str, "default", ..., None),
        #
        # Optional and supports None parameters with string default
        # - Valid input
        ("optional_supported_none_str_default", str | None, "default", "text", None),
        # - None allowed
        ("optional_supported_none_str_default", str | None, "default", None, None),
        # - Missing optional parameter is allowed
        ("optional_supported_none_str_default", str | None, "default", ..., None),
        #
        # Optional and supports None parameters with None default
        # - Valid input
        ("optional_supported_none_none_default", str | None, None, "text", None),
        # - None allowed
        ("optional_supported_none_none_default", str | None, None, None, None),
        # - Missing optional parameter is allowed
        ("optional_supported_none_none_default", str | None, None, ..., None),
    ],
)
def test_validate_tool_arguments_nullable(scenario, type_hint, default, input_value, expected_error_message):
    """Test validation of tool arguments with focus on nullable properties: optional (with default value) and supporting None value."""

    # Create a tool with the appropriate signature
    if default is ...:  # Using Ellipsis to indicate no default value

        @tool
        def test_tool(param: type_hint) -> str:
            """Test tool.

            Args:
                param: Input param
            """
            return str(param) if param is not None else "NULL"
    else:

        @tool
        def test_tool(param: type_hint = default) -> str:
            """Test tool.

            Args:
                param: Input param.
            """
            return str(param) if param is not None else "NULL"

    # Test with the input dictionary
    input_dict = {"param": input_value} if input_value is not ... else {}

    if expected_error_message:
        with pytest.raises((ValueError, TypeError), match=expected_error_message):
            validate_tool_arguments(test_tool, input_dict)
    else:
        # Should not raise any exception
        validate_tool_arguments(test_tool, input_dict)



================================================
FILE: tests/test_types.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import tempfile
import unittest
import uuid

import PIL.Image

from smolagents.agent_types import AgentAudio, AgentImage, AgentText

from .utils.markers import require_soundfile, require_torch


def get_new_path(suffix="") -> str:
    directory = tempfile.mkdtemp()
    return os.path.join(directory, str(uuid.uuid4()) + suffix)


@require_soundfile
@require_torch
class AgentAudioTests(unittest.TestCase):
    def test_from_tensor(self):
        import soundfile as sf
        import torch

        tensor = torch.rand(12, dtype=torch.float64) - 0.5
        agent_type = AgentAudio(tensor)
        path = str(agent_type.to_string())

        # Ensure that the tensor and the agent_type's tensor are the same
        self.assertTrue(torch.allclose(tensor, agent_type.to_raw(), atol=1e-4))

        del agent_type

        # Ensure the path remains even after the object deletion
        self.assertTrue(os.path.exists(path))

        # Ensure that the file contains the same value as the original tensor
        new_tensor, _ = sf.read(path)
        self.assertTrue(torch.allclose(tensor, torch.tensor(new_tensor), atol=1e-4))

    def test_from_string(self):
        import soundfile as sf
        import torch

        tensor = torch.rand(12, dtype=torch.float64) - 0.5
        path = get_new_path(suffix=".wav")
        sf.write(path, tensor, 16000)

        agent_type = AgentAudio(path)

        self.assertTrue(torch.allclose(tensor, agent_type.to_raw(), atol=1e-4))
        self.assertEqual(agent_type.to_string(), path)


@require_torch
class TestAgentImage:
    def test_from_tensor(self):
        import torch

        tensor = torch.randint(0, 256, (64, 64, 3))
        agent_type = AgentImage(tensor)
        path = str(agent_type.to_string())

        # Ensure that the tensor and the agent_type's tensor are the same
        assert torch.allclose(tensor, agent_type._tensor, atol=1e-4)

        assert isinstance(agent_type.to_raw(), PIL.Image.Image)

        # Ensure the path remains even after the object deletion
        del agent_type
        assert os.path.exists(path)

    def test_from_string(self, shared_datadir):
        path = shared_datadir / "000000039769.png"
        image = PIL.Image.open(path)
        agent_type = AgentImage(path)

        assert path.samefile(agent_type.to_string())
        assert image == agent_type.to_raw()

        # Ensure the path remains even after the object deletion
        del agent_type
        assert os.path.exists(path)

    def test_from_image(self, shared_datadir):
        path = shared_datadir / "000000039769.png"
        image = PIL.Image.open(path)
        agent_type = AgentImage(image)

        assert not path.samefile(agent_type.to_string())
        assert image == agent_type.to_raw()

        # Ensure the path remains even after the object deletion
        del agent_type
        assert os.path.exists(path)


class AgentTextTests(unittest.TestCase):
    def test_from_string(self):
        string = "Hey!"
        agent_type = AgentText(string)

        self.assertEqual(string, agent_type.to_string())
        self.assertEqual(string, agent_type.to_raw())



================================================
FILE: tests/test_utils.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import inspect
import os
import textwrap
import unittest

import pytest
from IPython.core.interactiveshell import InteractiveShell

from smolagents import Tool
from smolagents.tools import tool
from smolagents.utils import (
    create_agent_gradio_app_template,
    get_source,
    instance_to_source,
    is_valid_name,
    parse_code_blobs,
    parse_json_blob,
)


class ValidTool(Tool):
    name = "valid_tool"
    description = "A valid tool"
    inputs = {"input": {"type": "string", "description": "input"}}
    output_type = "string"
    simple_attr = "string"
    dict_attr = {"key": "value"}

    def __init__(self, optional_param="default"):
        super().__init__()
        self.param = optional_param

    def forward(self, input: str) -> str:
        return input.upper()


@tool
def valid_tool_function(input: str) -> str:
    """A valid tool function.

    Args:
        input (str): Input string.
    """
    return input.upper()


VALID_TOOL_SOURCE = """\
from smolagents.tools import Tool

class ValidTool(Tool):
    name = "valid_tool"
    description = "A valid tool"
    inputs = {'input': {'type': 'string', 'description': 'input'}}
    output_type = "string"
    simple_attr = "string"
    dict_attr = {'key': 'value'}

    def __init__(self, optional_param="default"):
        super().__init__()
        self.param = optional_param

    def forward(self, input: str) -> str:
        return input.upper()
"""

VALID_TOOL_FUNCTION_SOURCE = '''\
from smolagents.tools import Tool

class SimpleTool(Tool):
    name = "valid_tool_function"
    description = "A valid tool function."
    inputs = {'input': {'type': 'string', 'description': 'Input string.'}}
    output_type = "string"

    def __init__(self):
        self.is_initialized = True

    def forward(self, input: str) -> str:
        """A valid tool function.

        Args:
            input (str): Input string.
        """
        return input.upper()
'''


class AgentTextTests(unittest.TestCase):
    def test_parse_code_blobs(self):
        with pytest.raises(ValueError):
            parse_code_blobs("Wrong blob!", ("<code>", "</code>"))

        # Parsing mardkwon with code blobs should work
        output = parse_code_blobs(
            """
Here is how to solve the problem:
<code>
import numpy as np
</code>
""",
            ("<code>", "</code>"),
        )
        assert output == "import numpy as np"

        # Parsing pure python code blobs should work
        code_blob = "import numpy as np"
        output = parse_code_blobs(code_blob, ("```python", "```"))
        assert output == code_blob

        # Allow whitespaces after header
        output = parse_code_blobs("<code>    \ncode_a\n</code>", ("<code>", "</code>"))
        assert output == "code_a"

        # Parsing markdown with code blobs should work
        output = parse_code_blobs(
            """
Here is how to solve the problem:
```python
import numpy as np
```
""",
            ("<code>", "</code>"),
        )
        assert output == "import numpy as np"

    def test_multiple_code_blobs(self):
        test_input = "<code>\nFoo\n</code>\n\n<code>\ncode_a\n</code>\n\n<code>\ncode_b\n</code>"
        result = parse_code_blobs(test_input, ("<code>", "</code>"))
        assert result == "Foo\n\ncode_a\n\ncode_b"


@pytest.fixture(scope="function")
def ipython_shell():
    """Reset IPython shell before and after each test."""
    shell = InteractiveShell.instance()
    shell.reset()  # Clean before test
    yield shell
    shell.reset()  # Clean after test


@pytest.mark.parametrize(
    "obj_name, code_blob",
    [
        ("test_func", "def test_func():\n    return 42"),
        ("TestClass", "class TestClass:\n    ..."),
    ],
)
def test_get_source_ipython(ipython_shell, obj_name, code_blob):
    ipython_shell.run_cell(code_blob, store_history=True)
    obj = ipython_shell.user_ns[obj_name]
    assert get_source(obj) == code_blob


def test_get_source_standard_class():
    class TestClass: ...

    source = get_source(TestClass)
    assert source == "class TestClass: ..."
    assert source == textwrap.dedent(inspect.getsource(TestClass)).strip()


def test_get_source_standard_function():
    def test_func(): ...

    source = get_source(test_func)
    assert source == "def test_func(): ..."
    assert source == textwrap.dedent(inspect.getsource(test_func)).strip()


def test_get_source_ipython_errors_empty_cells(ipython_shell):
    test_code = textwrap.dedent("""class TestClass:\n    ...""").strip()
    ipython_shell.user_ns["In"] = [""]
    ipython_shell.run_cell(test_code, store_history=True)
    with pytest.raises(ValueError, match="No code cells found in IPython session"):
        get_source(ipython_shell.user_ns["TestClass"])


def test_get_source_ipython_errors_definition_not_found(ipython_shell):
    test_code = textwrap.dedent("""class TestClass:\n    ...""").strip()
    ipython_shell.user_ns["In"] = ["", "print('No class definition here')"]
    ipython_shell.run_cell(test_code, store_history=True)
    with pytest.raises(ValueError, match="Could not find source code for TestClass in IPython history"):
        get_source(ipython_shell.user_ns["TestClass"])


def test_get_source_ipython_errors_type_error():
    with pytest.raises(TypeError, match="Expected class or callable"):
        get_source(None)


@pytest.mark.parametrize(
    "tool, expected_tool_source", [(ValidTool(), VALID_TOOL_SOURCE), (valid_tool_function, VALID_TOOL_FUNCTION_SOURCE)]
)
def test_instance_to_source(tool, expected_tool_source):
    tool_source = instance_to_source(tool, base_cls=Tool)
    assert tool_source == expected_tool_source


def test_e2e_class_tool_save(tmp_path):
    class TestTool(Tool):
        name = "test_tool"
        description = "Test tool description"
        inputs = {
            "task": {
                "type": "string",
                "description": "tool input",
            }
        }
        output_type = "string"

        def forward(self, task: str):
            import IPython  # noqa: F401

            return task

    test_tool = TestTool()
    test_tool.save(tmp_path, make_gradio_app=True)
    assert set(os.listdir(tmp_path)) == {"requirements.txt", "app.py", "tool.py"}
    assert (tmp_path / "tool.py").read_text() == textwrap.dedent(
        """\
        from typing import Any, Optional
        from smolagents.tools import Tool
        import IPython

        class TestTool(Tool):
            name = "test_tool"
            description = "Test tool description"
            inputs = {'task': {'type': 'string', 'description': 'tool input'}}
            output_type = "string"

            def forward(self, task: str):
                import IPython  # noqa: F401

                return task

            def __init__(self, *args, **kwargs):
                self.is_initialized = False
        """
    )
    requirements = set((tmp_path / "requirements.txt").read_text().split())
    assert requirements == {"IPython", "smolagents"}
    assert (tmp_path / "app.py").read_text() == textwrap.dedent(
        """\
        from smolagents import launch_gradio_demo
        from tool import TestTool

        tool = TestTool()
        launch_gradio_demo(tool)
        """
    )


def test_e2e_ipython_class_tool_save(tmp_path):
    shell = InteractiveShell.instance()
    code_blob = textwrap.dedent(
        f"""\
        from smolagents.tools import Tool
        class TestTool(Tool):
            name = "test_tool"
            description = "Test tool description"
            inputs = {{"task": {{"type": "string",
                    "description": "tool input",
                }}
            }}
            output_type = "string"

            def forward(self, task: str):
                import IPython  # noqa: F401

                return task
        TestTool().save("{tmp_path}", make_gradio_app=True)
        """
    )
    assert shell.run_cell(code_blob, store_history=True).success
    assert set(os.listdir(tmp_path)) == {"requirements.txt", "app.py", "tool.py"}
    assert (tmp_path / "tool.py").read_text() == textwrap.dedent(
        """\
        from typing import Any, Optional
        from smolagents.tools import Tool
        import IPython

        class TestTool(Tool):
            name = "test_tool"
            description = "Test tool description"
            inputs = {'task': {'type': 'string', 'description': 'tool input'}}
            output_type = "string"

            def forward(self, task: str):
                import IPython  # noqa: F401

                return task

            def __init__(self, *args, **kwargs):
                self.is_initialized = False
        """
    )
    requirements = set((tmp_path / "requirements.txt").read_text().split())
    assert requirements == {"IPython", "smolagents"}
    assert (tmp_path / "app.py").read_text() == textwrap.dedent(
        """\
        from smolagents import launch_gradio_demo
        from tool import TestTool

        tool = TestTool()
        launch_gradio_demo(tool)
        """
    )


def test_e2e_function_tool_save(tmp_path):
    @tool
    def test_tool(task: str) -> str:
        """
        Test tool description

        Args:
            task: tool input
        """
        import IPython  # noqa: F401

        return task

    test_tool.save(tmp_path, make_gradio_app=True)
    assert set(os.listdir(tmp_path)) == {"requirements.txt", "app.py", "tool.py"}
    assert (tmp_path / "tool.py").read_text() == textwrap.dedent(
        """\
        from smolagents import Tool
        from typing import Any, Optional

        class SimpleTool(Tool):
            name = "test_tool"
            description = "Test tool description"
            inputs = {'task': {'type': 'string', 'description': 'tool input'}}
            output_type = "string"

            def forward(self, task: str) -> str:
                \"""
                Test tool description

                Args:
                    task: tool input
                \"""
                import IPython  # noqa: F401

                return task"""
    )
    requirements = set((tmp_path / "requirements.txt").read_text().split())
    assert requirements == {"smolagents"}  # FIXME: IPython should be in the requirements
    assert (tmp_path / "app.py").read_text() == textwrap.dedent(
        """\
        from smolagents import launch_gradio_demo
        from tool import SimpleTool

        tool = SimpleTool()
        launch_gradio_demo(tool)
        """
    )


def test_e2e_ipython_function_tool_save(tmp_path):
    shell = InteractiveShell.instance()
    code_blob = textwrap.dedent(
        f"""
        from smolagents import tool

        @tool
        def test_tool(task: str) -> str:
            \"""
            Test tool description

            Args:
                task: tool input
            \"""
            import IPython  # noqa: F401

            return task

        test_tool.save("{tmp_path}", make_gradio_app=True)
        """
    )
    assert shell.run_cell(code_blob, store_history=True).success
    assert set(os.listdir(tmp_path)) == {"requirements.txt", "app.py", "tool.py"}
    assert (tmp_path / "tool.py").read_text() == textwrap.dedent(
        """\
        from smolagents import Tool
        from typing import Any, Optional

        class SimpleTool(Tool):
            name = "test_tool"
            description = "Test tool description"
            inputs = {'task': {'type': 'string', 'description': 'tool input'}}
            output_type = "string"

            def forward(self, task: str) -> str:
                \"""
                Test tool description

                Args:
                    task: tool input
                \"""
                import IPython  # noqa: F401

                return task"""
    )
    requirements = set((tmp_path / "requirements.txt").read_text().split())
    assert requirements == {"smolagents"}  # FIXME: IPython should be in the requirements
    assert (tmp_path / "app.py").read_text() == textwrap.dedent(
        """\
        from smolagents import launch_gradio_demo
        from tool import SimpleTool

        tool = SimpleTool()
        launch_gradio_demo(tool)
        """
    )


@pytest.mark.parametrize(
    "raw_json, expected_data, expected_blob",
    [
        (
            """{}""",
            {},
            "",
        ),
        (
            """Text{}""",
            {},
            "Text",
        ),
        (
            """{"simple": "json"}""",
            {"simple": "json"},
            "",
        ),
        (
            """With text here{"simple": "json"}""",
            {"simple": "json"},
            "With text here",
        ),
        (
            """{"simple": "json"}With text after""",
            {"simple": "json"},
            "",
        ),
        (
            """With text before{"simple": "json"}And text after""",
            {"simple": "json"},
            "With text before",
        ),
    ],
)
def test_parse_json_blob_with_valid_json(raw_json, expected_data, expected_blob):
    data, blob = parse_json_blob(raw_json)

    assert data == expected_data
    assert blob == expected_blob


@pytest.mark.parametrize(
    "raw_json",
    [
        """simple": "json"}""",
        """With text here"simple": "json"}""",
        """{"simple": ""json"}With text after""",
        """{"simple": "json"With text after""",
        "}}",
    ],
)
def test_parse_json_blob_with_invalid_json(raw_json):
    with pytest.raises(Exception):
        parse_json_blob(raw_json)


@pytest.mark.parametrize(
    "name,expected",
    [
        # Valid identifiers
        ("valid_name", True),
        ("ValidName", True),
        ("valid123", True),
        ("_private", True),
        # Invalid identifiers
        ("", False),
        ("123invalid", False),
        ("invalid-name", False),
        ("invalid name", False),
        ("invalid.name", False),
        # Python keywords
        ("if", False),
        ("for", False),
        ("class", False),
        ("return", False),
        # Non-string inputs
        (123, False),
        (None, False),
        ([], False),
        ({}, False),
    ],
)
def test_is_valid_name(name, expected):
    """Test the is_valid_name function with various inputs."""
    assert is_valid_name(name) is expected


def test_agent_gradio_app_template_excludes_class_keyword():
    """Test that the AGENT_GRADIO_APP_TEMPLATE excludes 'class' from agent kwargs."""

    # Mock agent_dict with 'class' key that should be excluded
    agent_dict = {
        "model": {"class": "CodeAgent", "data": {}},
        "class": "CodeAgent",  # This should be excluded to prevent SyntaxError
        "some_valid_attr": "value",
        "tools": [],
        "managed_agents": {},
        "requirements": [],
        "prompt_templates": {},
    }

    template = create_agent_gradio_app_template()
    result = template.render(
        agent_name="test_agent",
        class_name="CodeAgent",
        agent_dict=agent_dict,
        tools={},
        managed_agents={},
        managed_agent_relative_path="",
    )

    # Should contain valid attribute but not 'class='  as a keyword argument
    assert "some_valid_attr='value'," in result
    assert "class=" not in result

    # Verify the generated code is syntactically valid Python
    import ast

    try:
        ast.parse(result)
    except SyntaxError as e:
        pytest.fail(f"Generated app.py contains syntax error: {e}")



================================================
FILE: tests/test_vision_web_browser.py
================================================
"""Test XPath injection vulnerability fix in vision_web_browser.py"""

from unittest.mock import Mock, patch

import pytest

from smolagents.vision_web_browser import _escape_xpath_string, search_item_ctrl_f


@pytest.fixture
def mock_driver():
    """Mock Selenium WebDriver"""
    driver = Mock()
    driver.find_elements.return_value = [Mock()]  # Mock found elements
    driver.execute_script.return_value = None
    return driver


class TestXPathEscaping:
    """Test XPath string escaping functionality"""

    @pytest.mark.parametrize(
        "input_text,expected_pattern",
        [
            ("normal text", "'normal text'"),
            ("text with 'quote'", "\"text with 'quote'\""),
            ('text with "quote"', "'text with \"quote\"'"),
            ("text with one single'quote", '"text with one single\'quote"'),
            ('text with one double"quote', "'text with one double\"quote'"),
            (
                "text with both 'single' and \"double\" quotes",
                "concat('text with both ', \"'\", 'single', \"'\", ' and \"double\" quotes')",
            ),
            ("", "''"),
            ("'", '"\'"'),
            ('"', "'\"'"),
        ],
    )
    def test_escape_xpath_string_basic(self, input_text, expected_pattern):
        """Test basic XPath escaping cases"""
        result = _escape_xpath_string(input_text)
        assert result == expected_pattern

    @pytest.mark.parametrize(
        "input_text",
        [
            "text with both 'single' and \"double\" quotes",
            'it\'s a "test" case',
            "'mixed\" quotes'",
        ],
    )
    def test_escape_xpath_string_mixed_quotes(self, input_text):
        """Test XPath escaping with mixed quotes uses concat()"""
        result = _escape_xpath_string(input_text)
        assert result.startswith("concat(")
        assert result.endswith(")")

    @pytest.mark.parametrize(
        "malicious_input",
        [
            "')] | //script[@src='evil.js'] | foo[contains(text(), '",
            "') or 1=1 or ('",
            "')] | //user[contains(@role,'admin')] | foo[contains(text(), '",
            "') and substring(//user[1]/password,1,1)='a",
        ],
    )
    def test_escape_prevents_injection(self, malicious_input):
        """Test that malicious XPath injection attempts are safely escaped"""
        result = _escape_xpath_string(malicious_input)
        # Should either be wrapped in quotes or use concat()
        assert (
            (result.startswith("'") and result.endswith("'"))
            or (result.startswith('"') and result.endswith('"'))
            or result.startswith("concat(")
        )


class TestSearchItemCtrlF:
    """Test the search_item_ctrl_f function with XPath injection protection"""

    @pytest.mark.parametrize(
        "search_text",
        [
            "normal search",
            "search with 'quotes'",
            'search with "quotes"',
            "')] | //script[@src='evil.js'] | foo[contains(text(), '",
            "') or 1=1 or ('",
        ],
    )
    def test_search_item_prevents_injection(self, search_text, mock_driver):
        """Test that search_item_ctrl_f prevents XPath injection"""
        with patch("smolagents.vision_web_browser.driver", mock_driver, create=True):
            # Call the function
            result = search_item_ctrl_f(search_text)

            # Verify driver.find_elements was called
            mock_driver.find_elements.assert_called_once()

            # Get the actual XPath query that was generated
            call_args = mock_driver.find_elements.call_args
            xpath_query = call_args[0][1]  # Second positional argument

            # Verify the query doesn't contain unescaped injection
            if "')] | //" in search_text:
                # For injection attempts, verify they're properly escaped
                # The query should either use concat() or be properly quoted
                is_concat = "concat(" in xpath_query
                is_properly_quoted = xpath_query.count('"') >= 2 or xpath_query.count("'") >= 2
                assert is_concat or is_properly_quoted, f"XPath injection not prevented: {xpath_query}"

            # Verify we got a result
            assert "Found" in result

    def test_search_item_nth_result(self, mock_driver):
        """Test nth_result parameter works correctly"""
        mock_driver.find_elements.return_value = [Mock(), Mock(), Mock()]  # 3 elements

        with patch("smolagents.vision_web_browser.driver", mock_driver, create=True):
            result = search_item_ctrl_f("test", nth_result=2)

            # Should find 3 matches and focus on element 2
            assert "Found 3 matches" in result
            assert "Focused on element 2 of 3" in result

    def test_search_item_not_found(self, mock_driver):
        """Test exception when nth_result exceeds available matches"""
        mock_driver.find_elements.return_value = [Mock()]  # Only 1 element

        with patch("smolagents.vision_web_browser.driver", mock_driver, create=True):
            with pytest.raises(Exception, match="Match nÂ°3 not found"):
                search_item_ctrl_f("test", nth_result=3)



================================================
FILE: tests/fixtures/agents.py
================================================
import pytest


AGENT_DICTS = {
    "v1.9": {
        "tools": [],
        "model": {
            "class": "InferenceClientModel",
            "data": {
                "last_input_token_count": None,
                "last_output_token_count": None,
                "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
                "provider": None,
            },
        },
        "managed_agents": {},
        "prompt_templates": {
            "system_prompt": "dummy system prompt",
            "planning": {
                "initial_facts": "dummy planning initial facts",
                "initial_plan": "dummy planning initial plan",
                "update_facts_pre_messages": "dummy planning update facts pre messages",
                "update_facts_post_messages": "dummy planning update facts post messages",
                "update_plan_pre_messages": "dummy planning update plan pre messages",
                "update_plan_post_messages": "dummy planning update plan post messages",
            },
            "managed_agent": {
                "task": "dummy managed agent task",
                "report": "dummy managed agent report",
            },
            "final_answer": {
                "pre_messages": "dummy final answer pre messages",
                "post_messages": "dummy final answer post messages",
            },
        },
        "max_steps": 10,
        "verbosity_level": 2,
        "grammar": None,
        "planning_interval": 2,
        "name": "test_agent",
        "description": "dummy description",
        "requirements": ["smolagents"],
        "authorized_imports": ["pandas"],
    },
    # Added: executor_type, executor_kwargs, max_print_outputs_length
    "v1.10": {
        "tools": [],
        "model": {
            "class": "InferenceClientModel",
            "data": {
                "last_input_token_count": None,
                "last_output_token_count": None,
                "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
                "provider": None,
            },
        },
        "managed_agents": {},
        "prompt_templates": {
            "system_prompt": "dummy system prompt",
            "planning": {
                "initial_facts": "dummy planning initial facts",
                "initial_plan": "dummy planning initial plan",
                "update_facts_pre_messages": "dummy planning update facts pre messages",
                "update_facts_post_messages": "dummy planning update facts post messages",
                "update_plan_pre_messages": "dummy planning update plan pre messages",
                "update_plan_post_messages": "dummy planning update plan post messages",
            },
            "managed_agent": {
                "task": "dummy managed agent task",
                "report": "dummy managed agent report",
            },
            "final_answer": {
                "pre_messages": "dummy final answer pre messages",
                "post_messages": "dummy final answer post messages",
            },
        },
        "max_steps": 10,
        "verbosity_level": 2,
        "grammar": None,
        "planning_interval": 2,
        "name": "test_agent",
        "description": "dummy description",
        "requirements": ["smolagents"],
        "authorized_imports": ["pandas"],
        "executor_type": "local",
        "executor_kwargs": {},
        "max_print_outputs_length": None,
    },
    # Removed: grammar, last_input_token_count, last_output_token_count
    "v1.20": {
        "tools": [],
        "model": {
            "class": "InferenceClientModel",
            "data": {
                "model_id": "Qwen/Qwen2.5-Coder-32B-Instruct",
                "provider": None,
            },
        },
        "managed_agents": {},
        "prompt_templates": {
            "system_prompt": "dummy system prompt",
            "planning": {
                "initial_facts": "dummy planning initial facts",
                "initial_plan": "dummy planning initial plan",
                "update_facts_pre_messages": "dummy planning update facts pre messages",
                "update_facts_post_messages": "dummy planning update facts post messages",
                "update_plan_pre_messages": "dummy planning update plan pre messages",
                "update_plan_post_messages": "dummy planning update plan post messages",
            },
            "managed_agent": {
                "task": "dummy managed agent task",
                "report": "dummy managed agent report",
            },
            "final_answer": {
                "pre_messages": "dummy final answer pre messages",
                "post_messages": "dummy final answer post messages",
            },
        },
        "max_steps": 10,
        "verbosity_level": 2,
        "planning_interval": 2,
        "name": "test_agent",
        "description": "dummy description",
        "requirements": ["smolagents"],
        "authorized_imports": ["pandas"],
        "executor_type": "local",
        "executor_kwargs": {},
        "max_print_outputs_length": None,
    },
}


@pytest.fixture
def get_agent_dict():
    def _get_agent_dict(agent_dict_key):
        return AGENT_DICTS[agent_dict_key]

    return _get_agent_dict



================================================
FILE: tests/fixtures/tools.py
================================================
import pytest

from smolagents.tools import Tool, tool


@pytest.fixture
def test_tool():
    class TestTool(Tool):
        name = "test_tool"
        description = "A test tool"
        inputs = {"input": {"type": "string", "description": "Input value"}}
        output_type = "string"

        def forward(self, input):
            if input == "error":
                raise ValueError("Tool execution error")
            return f"Processed: {input}"

    return TestTool()


@pytest.fixture
def no_input_tool():
    class NoInputTool(Tool):
        name = "no_input_tool"
        description = "Tool with no inputs"
        inputs = {}
        output_type = "string"

        def forward(self):
            return "test"

    return NoInputTool()


@pytest.fixture
def single_input_tool():
    class SingleInputTool(Tool):
        name = "single_input_tool"
        description = "Tool with one input"
        inputs = {"text": {"type": "string", "description": "Input text"}}
        output_type = "string"

        def forward(self, text):
            return "test"

    return SingleInputTool()


@pytest.fixture
def multi_input_tool():
    class MultiInputTool(Tool):
        name = "multi_input_tool"
        description = "Tool with multiple inputs"
        inputs = {
            "text": {"type": "string", "description": "Text input"},
            "count": {"type": "integer", "description": "Number count"},
        }
        output_type = "object"

        def forward(self, text, count):
            return "test"

    return MultiInputTool()


@pytest.fixture
def multiline_description_tool():
    class MultilineDescriptionTool(Tool):
        name = "multiline_description_tool"
        description = "This is a tool with\nmultiple lines\nin the description"
        inputs = {"input": {"type": "string", "description": "Some input"}}
        output_type = "string"

        def forward(self, input):
            return "test"

    return MultilineDescriptionTool()


@pytest.fixture
def example_tool():
    @tool
    def valid_tool_function(input: str) -> str:
        """A valid tool function.

        Args:
            input (str): Input string.
        """
        return input.upper()

    return valid_tool_function


@pytest.fixture
def boolean_default_tool_class():
    class BooleanDefaultTool(Tool):
        name = "boolean_default_tool"
        description = "A tool with a boolean default parameter"
        inputs = {
            "text": {"type": "string", "description": "Input text"},
            "flag": {"type": "boolean", "description": "Boolean flag with default value", "nullable": True},
        }
        output_type = "string"

        def forward(self, text: str, flag: bool = False) -> str:
            return f"Text: {text}, Flag: {flag}"

    return BooleanDefaultTool()


@pytest.fixture
def boolean_default_tool_function():
    @tool
    def boolean_default_tool(text: str, flag: bool = False) -> str:
        """
        A tool with a boolean default parameter.

        Args:
            text: Input text
            flag: Boolean flag with default value
        """
        return f"Text: {text}, Flag: {flag}"

    return boolean_default_tool


@pytest.fixture
def optional_input_tool_class():
    class OptionalInputTool(Tool):
        name = "optional_input_tool"
        description = "A tool with an optional input parameter"
        inputs = {
            "required_text": {"type": "string", "description": "Required input text"},
            "optional_text": {"type": "string", "description": "Optional input text", "nullable": True},
        }
        output_type = "string"

        def forward(self, required_text: str, optional_text: str | None = None) -> str:
            if optional_text:
                return f"{required_text} + {optional_text}"
            return required_text

    return OptionalInputTool()


@pytest.fixture
def optional_input_tool_function():
    @tool
    def optional_input_tool(required_text: str, optional_text: str | None = None) -> str:
        """
        A tool with an optional input parameter.

        Args:
            required_text: Required input text
            optional_text: Optional input text
        """
        if optional_text:
            return f"{required_text} + {optional_text}"
        return required_text

    return optional_input_tool



================================================
FILE: tests/utils/markers.py
================================================
# coding=utf-8
# Copyright 2024 HuggingFace Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Markers for tests ."""

import os
from importlib.util import find_spec

import pytest


require_run_all = pytest.mark.skipif(not os.getenv("RUN_ALL"), reason="requires RUN_ALL environment variable")
require_soundfile = pytest.mark.skipif(find_spec("soundfile") is None, reason="requires soundfile")
require_torch = pytest.mark.skipif(find_spec("torch") is None, reason="requires torch")



================================================
FILE: .github/ISSUE_TEMPLATE/bug_report.md
================================================
---
name: Bug report
about: The clearer your bug report, the faster it will be fixed!
title: "BUG: "
labels: bug
assignees: ''

---

<!--
Thank you for reporting a bug!
Please provide as much detail as possible to help us identify and fix the issue.
-->

**Problem**
A clear and concise description of what the bug is.

**Steps to reproduce**
Please provide a minimal, self-contained, and reproducible example of the bug.
1.  ...
2.  ...
3.  ...

```python
# Your code to reproduce the bug here
```

**Actual behavior and error logs**
A clear and concise description of what actually happened. Please include the full traceback if an exception was raised.
```shell

```

**Expected behavior**
A clear and concise description of what you expected to happen.

**Environment:**
Please complete the following information:
- OS: [e.g., Linux, macOS, Windows]
- Python version: [e.g., 3.10]
- Package version: (output of `pip show smolagents`)
```

```

**Additional context (optional)**
Add any other context, screenshots, or links about the bug here.

---

### Checklist
- [ ] I have searched the existing issues and have not found a similar bug report.
- [ ] I have provided a minimal, reproducible example.
- [ ] I have provided the full traceback of the error.
- [ ] I have provided my environment details.
- [ ] I am willing to work on this issue and submit a pull request. (optional)



================================================
FILE: .github/ISSUE_TEMPLATE/doc_improvement.md
================================================
---
name: Documentation Improvement
about: Report wrong or missing documentation
title: 'DOC: '
labels: documentation
assignees: ''

---

<!--
Thank you for contributing to our documentation!
Please provide as much detail as possible.
-->

**Problem**
A clear and description of what is wrong or missing in the documentation.

**Location of the documentation**
Provide the specific location of the documentation that needs improvement. Select what is applicable:
- Function/Class/Method name:  (if applicable: e.g., `module.ClassName.method_name`)
- URL: (if applicable: e.g. `https://huggingface.co/docs/smolagents/installation`)

**Suggested improvement**
A clear and concise description of the fix and improvement you suggest and why it is better.

**Additional context (optional)**
Add any other context or screenshots about the documentation improvement here.

---

### Checklist
- [ ] I have searched the existing issues and have not found a similar issue.
- [ ] I am willing to work on this issue and submit a pull request. (optional)



================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest an idea for this project
title: 'ENH: '
labels: enhancement
assignees: ''

---

<!--
Thank you for suggesting an idea to improve this project!
Please provide as much detail as possible.
-->

**Problem**
A clear and concise description of what the problem is that this feature would solve. For example, "I'm always frustrated when..."

**Proposed solution**
A clear and concise description of what you want to happen.

**Is this not possible with the current options.**
Make sure to consider if what you're requesting can be done with current abstractions.

**Alternatives considered**
A clear and concise description of any alternative solutions or features you've considered.
Please also describe if what you're requesting can be achieved with the current abstractions, and if so, why a new feature is still needed.

**Additional context (optional)**
Add any other context, screenshots, or links about the feature request here.

---

### Checklist
- [ ] I have searched the existing issues and have not found a similar feature request.
- [ ] I have verified that this feature is not already implemented in the latest version.
- [ ] I am willing to work on this feature and submit a pull request. (optional)



================================================
FILE: .github/workflows/build_documentation.yml
================================================
name: Build documentation

on:
  push:
    branches:
      - main
      - doc-builder*
      - v*-release
      - use_templates
    paths:
      - 'docs/source/**'
      - 'assets/**'
      - '.github/workflows/doc-build.yml'
      - 'pyproject.toml'

jobs:
   build:
    uses: huggingface/doc-builder/.github/workflows/build_main_documentation.yml@main
    with:
      commit_sha: ${{ github.sha }}
      package: smolagents
      languages: en hi ko zh
      notebook_folder: smolagents_doc
      # additional_args: --not_python_module # use this arg if repository is documentation only
    secrets:
      token: ${{ secrets.HUGGINGFACE_PUSH }}
      hf_token: ${{ secrets.HF_DOC_BUILD_PUSH }}


================================================
FILE: .github/workflows/build_pr_documentation.yml
================================================
name: Build PR Documentation

on:
  pull_request:
    paths:
      - 'docs/source/**'
      - 'assets/**'
      - '.github/workflows/doc-pr-build.yml'

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  build:
    uses: huggingface/doc-builder/.github/workflows/build_pr_documentation.yml@main
    with:
      commit_sha: ${{ github.event.pull_request.head.sha }}
      pr_number: ${{ github.event.number }}
      package: smolagents
      languages: en hi ko zh
      # additional_args: --not_python_module # use this arg if repository is documentation only


================================================
FILE: .github/workflows/quality.yml
================================================
name: Quality Check

on: [pull_request]

env:
  UV_SYSTEM_PYTHON: 1

jobs:
  check_code_quality:
    runs-on: ubuntu-latest
    env:
      UV_HTTP_TIMEOUT: 600 # max 10min to install deps

    steps:
      - uses: actions/checkout@v6
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.12"

      # Setup venv
      - name: Setup uv
        run: |
          pip install --upgrade uv

      - name: Install dependencies
        run: uv pip install "smolagents[quality] @ ."

      # Equivalent of "make quality" but step by step
      - run: ruff check examples src tests  # linter
      - run: ruff format --check examples src tests  # formatter



================================================
FILE: .github/workflows/tests.yml
================================================
name: Python tests

on:
  pull_request:
  push:
    branches:
      - ci-*

env:
  UV_SYSTEM_PYTHON: 1

jobs:
  build-ubuntu:
    runs-on: ubuntu-latest
    env:
      UV_HTTP_TIMEOUT: 600 # max 10min to install deps

    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.10", "3.12"]

    steps:
      - uses: actions/checkout@v6
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}

      # Setup venv
      - name: Setup uv
        run: |
          pip install --upgrade uv

      # Install dependencies
      - name: Install dependencies
        run: |
          uv pip install "smolagents[test] @ ."

      # Run tests
      - name: Test with pytest
        run: |
          pytest ./tests/



================================================
FILE: .github/workflows/trufflehog.yml
================================================
on:
  push:

name: Secret Leaks

permissions:
  contents: read

jobs:
  trufflehog:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v6
        with:
          fetch-depth: 0
      - name: Secret Scanning
        uses: trufflesecurity/trufflehog@main


================================================
FILE: .github/workflows/upload_pr_documentation.yml
================================================
name: Upload PR Documentation

on:
  workflow_run:
    workflows: ["Build PR Documentation"]
    types:
      - completed

jobs:
  build:
    uses: huggingface/doc-builder/.github/workflows/upload_pr_documentation.yml@main
    with:
      package_name: smolagents
    secrets:
      hf_token: ${{ secrets.HF_DOC_BUILD_PUSH }}
      comment_bot_token: ${{ secrets.COMMENT_BOT_TOKEN }}

